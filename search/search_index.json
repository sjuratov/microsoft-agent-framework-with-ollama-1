{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Slogan Writer-Reviewer Agent System","text":"<p>Multi-agent CLI application for generating creative slogans through iterative Writer-Reviewer collaboration using Microsoft Agent Framework and Ollama.</p> <ul> <li> <p> Multi-Agent Collaboration</p> <p>Writer and Reviewer agents work together iteratively to create compelling slogans</p> </li> <li> <p> Highly Configurable</p> <p>Customize iterations, models, and output formats to match your needs</p> </li> <li> <p> Fast &amp; Local</p> <p>Runs entirely on your machine using Ollama - no API keys or external services</p> </li> <li> <p> CLI &amp; REST API</p> <p>Use from command line or integrate via FastAPI REST endpoints</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>This tool uses two AI agents to collaboratively create compelling slogans:</p> <ul> <li>Writer Agent: Generates creative slogans based on your input</li> <li>Reviewer Agent: Provides critical feedback or approves with \"SHIP IT!\"</li> </ul> <p>The agents iterate up to 10 times (default 5, configurable) until the reviewer approves or the maximum turns are reached.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udd16 Multi-Agent Collaboration: Writer and Reviewer agents work together iteratively</li> <li>\ud83d\udd04 Configurable Iterations: Set custom iteration limits (1-10 turns, default 5)</li> <li>\ud83d\udc40 Iteration Visibility: Optional verbose mode to see the collaboration process</li> <li>\ud83c\udfa8 Model Selection: Choose different Ollama models for varied creative styles</li> <li>\ud83d\ude80 Fast &amp; Local: Runs entirely on your machine using Ollama</li> <li>\u23f1\ufe0f Performance Timing: Track total duration and per-turn timing in verbose mode</li> <li>\ud83c\udfa8 Color-Coded Output: Clear visual feedback with styled terminal output</li> <li>\ud83d\udcbe Multiple Output Formats: Save results as text or JSON for integration</li> <li>\ud83d\udd27 Model Management: List available models and validate before generation</li> <li>\ud83c\udf10 REST API: Full FastAPI implementation with interactive docs</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<p> Installation Quick Start CLI Guide API Guide</p>"},{"location":"#example-usage","title":"Example Usage","text":""},{"location":"#command-line","title":"Command Line","text":"<pre><code># Basic usage\nslogan-gen \"eco-friendly water bottle\"\n\n# See the iteration process\nslogan-gen \"eco-friendly water bottle\" --verbose\n\n# Use a specific model\nslogan-gen \"tech startup\" --model llama2\n\n# Save output\nslogan-gen \"coffee shop\" --output slogan.txt\n</code></pre>"},{"location":"#rest-api","title":"REST API","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/api/v1/generate\",\n    json={\n        \"description\": \"eco-friendly water bottle\",\n        \"max_iterations\": 5\n    }\n)\nprint(response.json()[\"slogan\"])\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>User Input\n    \u2193\nWriter Agent \u2500\u2500\u2500\u2500\u2500\u2500\u2192 Generates Slogan\n    \u2193\nReviewer Agent \u2500\u2500\u2500\u2500\u2192 Provides Feedback\n    \u2193\n    \u251c\u2500\u2192 \"SHIP IT!\" \u2192 Done \u2713\n    \u2514\u2500\u2192 Feedback \u2192 Writer Agent (iterate)\n</code></pre> <p>The system uses Microsoft Agent Framework for orchestration and Ollama for local LLM inference, ensuring fast, private, and cost-effective slogan generation.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to generate creative slogans? Follow our Installation Guide to get started in minutes!</p>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Installation Guide - Get up and running</li> <li>Quick Start - Your first slogan</li> <li>CLI Usage - Complete command reference</li> <li>API Usage - REST API integration</li> <li>Development Guide - Contributing to the project</li> <li>Architecture - System design and patterns</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+: Required for modern async/await support</li> <li>Ollama: Local LLM runtime (install from ollama.ai)</li> <li>uv: Fast Python package manager (install instructions)</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is open source. See the project repository for license details.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see our Development Guide for details on how to contribute.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their solutions when using the Slogan Writer-Reviewer Agent System.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#ollama-connection-error","title":"Ollama Connection Error","text":"<p>Error Message:</p> <pre><code>\u274c Error: Cannot connect to Ollama at http://localhost:11434\n</code></pre> <p>Cause: Ollama service is not running.</p> <p>Solution:</p> <ol> <li>Start the Ollama service:</li> </ol> <pre><code>ollama serve\n</code></pre> <ol> <li>Verify Ollama is running:</li> </ol> <pre><code>curl http://localhost:11434/api/tags\n</code></pre> <ol> <li>Check if another process is using port 11434:</li> </ol> <pre><code>lsof -i :11434  # macOS/Linux\nnetstat -ano | findstr :11434  # Windows\n</code></pre>"},{"location":"troubleshooting/#model-not-found","title":"Model Not Found","text":"<p>Error Message:</p> <pre><code>\u274c Error: Model 'mistral' not found in Ollama\n</code></pre> <p>Cause: The specified model hasn't been downloaded.</p> <p>Solution:</p> <ol> <li>List installed models:</li> </ol> <pre><code>ollama list\n</code></pre> <ol> <li>Pull the missing model:</li> </ol> <pre><code>ollama pull mistral\n</code></pre> <ol> <li>Verify installation:</li> </ol> <pre><code>ollama list | grep mistral\n</code></pre>"},{"location":"troubleshooting/#command-not-found-slogan-gen","title":"Command Not Found: slogan-gen","text":"<p>Error Message:</p> <pre><code>zsh: command not found: slogan-gen\n</code></pre> <p>Cause: Package not installed or virtual environment not activated.</p> <p>Solution:</p> <ol> <li>Activate virtual environment:</li> </ol> <pre><code>source .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Reinstall package:</li> </ol> <pre><code>uv pip install -e .\n</code></pre> <ol> <li>Verify installation:</li> </ol> <pre><code>which slogan-gen  # Should show path in .venv\n</code></pre>"},{"location":"troubleshooting/#generation-issues","title":"Generation Issues","text":""},{"location":"troubleshooting/#model-validation-errors-string-too-long","title":"Model Validation Errors (String Too Long)","text":"<p>Error Message:</p> <pre><code>\u274c Workflow Error: 2 validation errors for Turn\nslogan: String should have at most 500 characters\nfeedback: String should have at most 1000 characters\n</code></pre> <p>Cause: Smaller models (1B-2B parameters) like <code>gemma2:2b</code> or <code>gemma3:1b</code> may not follow instructions well and generate verbose output instead of concise slogans.</p> <p>Why This Happens:</p> <ul> <li>Smaller models sometimes generate explanations instead of just the slogan</li> <li>They may provide overly detailed feedback with examples</li> <li>They don't consistently follow \"concise output\" instructions</li> </ul> <p>Solution:</p> <p>Use a larger, more instruction-following model:</p> <pre><code># Recommended: Use mistral (7B parameters)\nslogan-gen generate \"coffee shop\" --model mistral:latest\n\n# Or use phi3:mini (3.8B parameters)\nslogan-gen generate \"coffee shop\" --model phi3:mini\n</code></pre> <p>Model Recommendations:</p> Model Size Instruction Following Best For <code>gemma2:2b</code> 2B Fair Quick testing only <code>phi3:mini</code> 3.8B Good Development <code>mistral:latest</code> 7B Excellent Production (default) <code>llama3.2:latest</code> 8B Excellent High quality output <p>Working as Designed</p> <p>The validation limits (500 chars for slogans, 1000 chars for feedback) are intentionally strict to enforce quality output. Use appropriately-sized models.</p>"},{"location":"troubleshooting/#slow-generation","title":"Slow Generation","text":"<p>Issue: Slogan generation takes too long.</p> <p>Causes:</p> <ul> <li>Large model on CPU-only system</li> <li>Too many iterations</li> <li>System resource constraints</li> </ul> <p>Solutions:</p>"},{"location":"troubleshooting/#1-use-a-smaller-model","title":"1. Use a Smaller Model","text":"<pre><code># Fast: 2B model (5-10s for 2 turns)\nslogan-gen generate \"test\" --model gemma2:2b\n\n# Balanced: 7B model (15-30s for 2 turns)\nslogan-gen generate \"test\" --model mistral\n</code></pre> <p>Model Performance Comparison:</p> Model Size Typical Time (2 turns) Quality <code>gemma2:2b</code> 2B ~5-10s Good <code>phi3:mini</code> 3.8B ~10-15s Very Good <code>mistral:latest</code> 7B ~15-30s Excellent <code>llama3:8b</code> 8B ~60-120s Excellent"},{"location":"troubleshooting/#2-reduce-iterations","title":"2. Reduce Iterations","text":"<pre><code># Limit to 3 turns instead of 5\nslogan-gen generate \"test\" --max-turns 3\n</code></pre>"},{"location":"troubleshooting/#3-check-system-resources","title":"3. Check System Resources","text":"<pre><code># Check CPU usage\ntop  # macOS/Linux\ntaskmgr  # Windows\n\n# Close other applications\n# Ensure adequate RAM available\n</code></pre>"},{"location":"troubleshooting/#4-use-gpu-acceleration-if-available","title":"4. Use GPU Acceleration (If Available)","text":"<p>If you have an NVIDIA/AMD GPU, ensure Ollama is using it:</p> <pre><code># Check if GPU is detected\nollama list\n\n# Look for \"Using GPU\" in output\nollama serve\n</code></pre>"},{"location":"troubleshooting/#poor-quality-slogans","title":"Poor Quality Slogans","text":"<p>Issue: Generated slogans are generic or low quality.</p> <p>Solutions:</p>"},{"location":"troubleshooting/#1-use-a-larger-model","title":"1. Use a Larger Model","text":"<pre><code>slogan-gen generate \"your input\" --model llama3.2\n</code></pre>"},{"location":"troubleshooting/#2-provide-more-specific-input","title":"2. Provide More Specific Input","text":"<p>\u274c Vague: <pre><code>slogan-gen generate \"business\"\n</code></pre></p> <p>\u2705 Specific: <pre><code>slogan-gen generate \"eco-friendly cleaning products for environmentally conscious homeowners\"\n</code></pre></p>"},{"location":"troubleshooting/#3-increase-iterations","title":"3. Increase Iterations","text":"<pre><code>slogan-gen generate \"your input\" --max-turns 7\n</code></pre>"},{"location":"troubleshooting/#4-adjust-temperature","title":"4. Adjust Temperature","text":"<pre><code># More creative\nOLLAMA_TEMPERATURE=0.9 slogan-gen generate \"your input\"\n\n# More focused\nOLLAMA_TEMPERATURE=0.5 slogan-gen generate \"your input\"\n</code></pre>"},{"location":"troubleshooting/#runtime-errors","title":"Runtime Errors","text":""},{"location":"troubleshooting/#timeout-errors","title":"Timeout Errors","text":"<p>Error Message:</p> <pre><code>\u274c Error: Request timeout after 30 seconds\n</code></pre> <p>Cause: Generation took longer than configured timeout.</p> <p>Solution:</p> <p>Increase timeout:</p> <pre><code>export OLLAMA_TIMEOUT=60\nslogan-gen generate \"your input\"\n</code></pre> <p>Or use a faster model:</p> <pre><code>slogan-gen generate \"your input\" --model gemma2:2b\n</code></pre>"},{"location":"troubleshooting/#max-turns-reached-without-approval","title":"Max Turns Reached Without Approval","text":"<p>Message:</p> <pre><code>\u26a0\ufe0f  Workflow completed without reviewer approval\n\ud83d\udcca Reached maximum turns (5/5)\n</code></pre> <p>Cause: Reviewer didn't approve any slogan within iteration limit.</p> <p>This is not an error! The system returns the best slogan from the iterations.</p> <p>To address:</p> <ol> <li>Increase max turns:</li> </ol> <pre><code>slogan-gen generate \"your input\" --max-turns 7\n</code></pre> <ol> <li>Use a better model:</li> </ol> <pre><code>slogan-gen generate \"your input\" --model mistral\n</code></pre> <ol> <li>Provide clearer input:</li> </ol> <pre><code># More specific input helps\nslogan-gen generate \"premium organic coffee roastery targeting health-conscious millennials\"\n</code></pre>"},{"location":"troubleshooting/#json-decode-errors","title":"JSON Decode Errors","text":"<p>Error Message:</p> <pre><code>\u274c Error: Invalid JSON response from model\n</code></pre> <p>Cause: Model returned malformed JSON or non-JSON text.</p> <p>Solution:</p> <ol> <li>Use a more capable model:</li> </ol> <pre><code>slogan-gen generate \"your input\" --model mistral\n</code></pre> <ol> <li>Retry the generation:</li> </ol> <pre><code># Sometimes works on second attempt\nslogan-gen generate \"your input\"\n</code></pre>"},{"location":"troubleshooting/#api-specific-issues","title":"API-Specific Issues","text":""},{"location":"troubleshooting/#cors-errors","title":"CORS Errors","text":"<p>Error in Browser:</p> <pre><code>Access to fetch at 'http://localhost:8000' has been blocked by CORS policy\n</code></pre> <p>Solution:</p> <p>Configure CORS origins:</p> <pre><code>export API_CORS_ORIGINS=\"http://localhost:3000,https://yourdomain.com\"\nuvicorn src.api.main:app\n</code></pre>"},{"location":"troubleshooting/#422-validation-error","title":"422 Validation Error","text":"<p>Error Response:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"input\"],\n      \"msg\": \"String should have at least 3 characters\"\n    }\n  ]\n}\n</code></pre> <p>Cause: Request body doesn't meet validation requirements.</p> <p>Solution:</p> <p>Check API requirements:</p> <ul> <li><code>input</code>: 3-200 characters</li> <li><code>max_turns</code>: 1-10</li> <li><code>model</code>: Must be installed in Ollama</li> </ul> <p>Valid Request:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"coffee shop\",\n    \"max_turns\": 5,\n    \"model\": \"mistral:latest\"\n  }'\n</code></pre>"},{"location":"troubleshooting/#api-timeout","title":"API Timeout","text":"<p>Error:</p> <pre><code>504 Gateway Timeout\n</code></pre> <p>Cause: Generation took longer than API timeout.</p> <p>Solution:</p> <p>Increase timeout:</p> <pre><code>export API_GENERATION_TIMEOUT=900\nexport API_REQUEST_TIMEOUT=930\nuvicorn src.api.main:app\n</code></pre> <p>Or use shorter timeout in client:</p> <pre><code>import httpx\n\nresponse = httpx.post(\n    url,\n    json=data,\n    timeout=900.0  # 15 minutes\n)\n</code></pre>"},{"location":"troubleshooting/#development-issues","title":"Development Issues","text":""},{"location":"troubleshooting/#import-errors","title":"Import Errors","text":"<p>Error Message:</p> <pre><code>ModuleNotFoundError: No module named 'agents'\n</code></pre> <p>Cause: Package not installed in editable mode.</p> <p>Solution:</p> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"troubleshooting/#mypy-type-errors","title":"Mypy Type Errors","text":"<p>Error:</p> <pre><code>error: Skipping analyzing \"agents\": module is installed, but missing library stubs\n</code></pre> <p>Cause: Missing <code>py.typed</code> marker files.</p> <p>Solution:</p> <p>Ensure these files exist:</p> <pre><code>touch src/py.typed\ntouch src/agents/py.typed\ntouch src/cli/py.typed\ntouch src/config/py.typed\ntouch src/orchestration/py.typed\n</code></pre>"},{"location":"troubleshooting/#test-failures","title":"Test Failures","text":"<p>Issue: Tests fail with connection errors.</p> <p>Solution:</p> <p>Ensure Ollama is running:</p> <pre><code>ollama serve\n</code></pre> <p>Pull required test model:</p> <pre><code>ollama pull mistral:latest\n</code></pre>"},{"location":"troubleshooting/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"troubleshooting/#macos-permission-denied","title":"macOS: Permission Denied","text":"<p>Error:</p> <pre><code>Permission denied: '/usr/local/bin/slogan-gen'\n</code></pre> <p>Solution:</p> <pre><code>sudo chmod +x /usr/local/bin/slogan-gen\n</code></pre> <p>Or install in user directory:</p> <pre><code>uv pip install --user -e .\n</code></pre>"},{"location":"troubleshooting/#windows-script-execution-policy","title":"Windows: Script Execution Policy","text":"<p>Error:</p> <pre><code>cannot be loaded because running scripts is disabled\n</code></pre> <p>Solution:</p> <pre><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n</code></pre>"},{"location":"troubleshooting/#linux-glibc-version-error","title":"Linux: GLIBC Version Error","text":"<p>Error:</p> <pre><code>version 'GLIBC_2.XX' not found\n</code></pre> <p>Cause: System GLIBC too old for Ollama.</p> <p>Solution:</p> <p>Upgrade system or use Docker:</p> <pre><code>docker run -d -p 11434:11434 ollama/ollama\n</code></pre>"},{"location":"troubleshooting/#getting-more-help","title":"Getting More Help","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># CLI\nslogan-gen generate \"test\" --verbose\n\n# API\nexport API_LOG_LEVEL=DEBUG\nuvicorn src.api.main:app\n</code></pre>"},{"location":"troubleshooting/#check-system-status","title":"Check System Status","text":"<pre><code># Check Ollama\nollama list\ncurl http://localhost:11434/api/tags\n\n# Check configuration\nslogan-gen config show\n\n# Check models\nslogan-gen models\n</code></pre>"},{"location":"troubleshooting/#report-an-issue","title":"Report an Issue","text":"<p>If you've tried everything and still have issues:</p> <ol> <li>Check existing issues on GitHub</li> <li>Create a new issue with:</li> <li>Error message (full output)</li> <li>Command you ran</li> <li>System info (OS, Python version)</li> <li>Ollama version: <code>ollama --version</code></li> <li>Configuration: <code>slogan-gen config show</code></li> </ol>"},{"location":"troubleshooting/#additional-resources","title":"Additional Resources","text":"<ul> <li>Installation Guide</li> <li>Configuration Guide</li> <li>CLI Usage Guide</li> <li>API Usage Guide</li> <li>Development Guide</li> </ul>"},{"location":"api-reference/agents/","title":"Agents API Reference","text":"<p>This page documents the agent implementations used in the Slogan Writer-Reviewer system.</p>"},{"location":"api-reference/agents/#overview","title":"Overview","text":"<p>The system uses two specialized AI agents that collaborate to generate high-quality slogans:</p> <ul> <li>Writer Agent: Generates creative slogans based on user input and incorporates feedback</li> <li>Reviewer Agent: Evaluates slogans and provides constructive feedback or approval</li> </ul> <p>Both agents are implemented using the Microsoft Agent Framework and communicate with Ollama for LLM inference.</p>"},{"location":"api-reference/agents/#architecture","title":"Architecture","text":"<pre><code>User Input\n    \u2193\nWriter Agent (generates slogan)\n    \u2193\nReviewer Agent (evaluates)\n    \u2193\nFeedback Loop \u2190\u2192 Writer Agent (revises)\n    \u2193\nFinal Approved Slogan\n</code></pre> <p>The agents follow a collaborative pattern where:</p> <ol> <li>Writer creates an initial slogan</li> <li>Reviewer evaluates and provides feedback</li> <li>Writer incorporates feedback and revises</li> <li>Loop continues until approval or max turns reached</li> </ol>"},{"location":"api-reference/agents/#writer-agent","title":"Writer Agent","text":""},{"location":"api-reference/agents/#src.agents.writer.create_writer_agent","title":"create_writer_agent","text":"<pre><code>create_writer_agent(config)\n</code></pre> <p>Create a Writer agent configured with Ollama.</p> <p>The Writer generates creative slogans based on user input and reviewer feedback.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OllamaConfig</code> <p>Ollama configuration settings</p> required <p>Returns:</p> Type Description <code>ChatAgent</code> <p>Configured ChatAgent instance for writing slogans</p> Source code in <code>src/agents/writer.py</code> <pre><code>def create_writer_agent(config: OllamaConfig) -&gt; ChatAgent:\n    \"\"\"Create a Writer agent configured with Ollama.\n\n    The Writer generates creative slogans based on user input and reviewer feedback.\n\n    Args:\n        config: Ollama configuration settings\n\n    Returns:\n        Configured ChatAgent instance for writing slogans\n    \"\"\"\n    client = OpenAIChatClient(\n        base_url=config.base_url,\n        api_key=\"ollama\",  # Ollama doesn't require real API key\n        model_id=config.model_name,\n    )\n\n    system_prompt = \"\"\"You are a creative slogan writer for marketing campaigns.\n\nYour role:\n- Generate catchy, memorable slogans based on the user's product/service description\n- Incorporate any feedback from the reviewer to improve your slogans\n- Be creative, concise, and impactful\n- Keep slogans under 100 characters when possible\n- Focus on emotional appeal and memorability\n\nWhen you receive feedback, carefully revise your slogan to address the reviewer's concerns\nwhile maintaining creativity.\n\nOutput only the slogan text, nothing else.\"\"\"\n\n    agent = ChatAgent(\n        chat_client=client,\n        instructions=system_prompt,\n    )\n\n    return agent\n</code></pre>"},{"location":"api-reference/agents/#writer-system-prompt","title":"Writer System Prompt","text":"<p>The Writer agent is configured with a system prompt that:</p> <ul> <li>Emphasizes creativity and memorability</li> <li>Encourages concise output (under 100 characters)</li> <li>Focuses on emotional appeal</li> <li>Instructs to incorporate reviewer feedback</li> <li>Outputs only the slogan text</li> </ul> <p>Key Characteristics:</p> <ul> <li>Goal: Generate catchy, impactful slogans</li> <li>Style: Creative, concise, emotionally engaging</li> <li>Behavior: Responds to feedback by revising while maintaining creativity</li> <li>Output: Pure slogan text only (no explanations)</li> </ul>"},{"location":"api-reference/agents/#usage-example","title":"Usage Example","text":"<pre><code>from src.agents.writer import create_writer_agent\nfrom src.config import get_ollama_config\n\n# Create writer agent\nconfig = get_ollama_config()\nwriter = create_writer_agent(config)\n\n# Generate initial slogan\nresponse = await writer.send_message(\"Create a slogan for: eco-friendly water bottle\")\nslogan = response.message.content\n\nprint(f\"Writer: {slogan}\")\n# Output: \"\ud83d\udca7 Pure Hydration, Zero Waste!\"\n</code></pre>"},{"location":"api-reference/agents/#reviewer-agent","title":"Reviewer Agent","text":""},{"location":"api-reference/agents/#src.agents.reviewer.create_reviewer_agent","title":"create_reviewer_agent","text":"<pre><code>create_reviewer_agent(config)\n</code></pre> <p>Create a Reviewer agent configured with Ollama.</p> <p>The Reviewer evaluates slogans and provides feedback or approval.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OllamaConfig</code> <p>Ollama configuration settings</p> required <p>Returns:</p> Type Description <code>ChatAgent</code> <p>Configured ChatAgent instance for reviewing slogans</p> Source code in <code>src/agents/reviewer.py</code> <pre><code>def create_reviewer_agent(config: OllamaConfig) -&gt; ChatAgent:\n    \"\"\"Create a Reviewer agent configured with Ollama.\n\n    The Reviewer evaluates slogans and provides feedback or approval.\n\n    Args:\n        config: Ollama configuration settings\n\n    Returns:\n        Configured ChatAgent instance for reviewing slogans\n    \"\"\"\n    client = OpenAIChatClient(\n        base_url=config.base_url,\n        api_key=\"ollama\",  # Ollama doesn't require real API key\n        model_id=config.model_name,\n    )\n\n    system_prompt = \"\"\"You are a marketing slogan reviewer with high standards.\n\nYour role:\n- Evaluate slogans for creativity, clarity, and marketing effectiveness\n- Provide specific, constructive feedback to help improve slogans\n- Approve excellent slogans ONLY when they meet all criteria\n- Be critical but fair - only approve truly great slogans\n\nEvaluation criteria:\n- Is it memorable and catchy?\n- Does it clearly relate to the product/service?\n- Is it concise and impactful?\n- Does it have emotional appeal?\n- Is it unique and creative?\n\nCRITICAL RESPONSE RULES:\n1. If the slogan needs ANY improvement: Provide ONLY feedback. Do NOT include \"SHIP IT!\" anywhere.\n2. If the slogan is truly excellent and meets ALL criteria: Respond with ONLY \"SHIP IT!\"\n   (nothing else).\n3. NEVER mix feedback with approval - choose one or the other.\n\nExamples:\n\u274c BAD: \"This is good but could be better... SHIP IT!\"\n\u274c BAD: \"Try making it more catchy. Otherwise SHIP IT!\"\n\u2705 GOOD (needs work): \"Make it more specific. 'Cloud power' is vague - what kind of power?\"\n\u2705 GOOD (approved): \"SHIP IT!\"\n\nBe thorough in your review. Don't approve mediocre slogans.\"\"\"\n\n    agent = ChatAgent(\n        chat_client=client,\n        instructions=system_prompt,\n    )\n\n    return agent\n</code></pre>"},{"location":"api-reference/agents/#reviewer-system-prompt","title":"Reviewer System Prompt","text":"<p>The Reviewer agent is configured with a system prompt that:</p> <ul> <li>Evaluates slogans against specific criteria</li> <li>Provides constructive, actionable feedback</li> <li>Approves only excellent slogans</li> <li>Uses \"SHIP IT!\" for approval</li> <li>Never mixes feedback with approval</li> </ul> <p>Evaluation Criteria:</p> <ol> <li>Memorability: Is it catchy and easy to remember?</li> <li>Clarity: Does it clearly relate to the product/service?</li> <li>Conciseness: Is it impactful and to the point?</li> <li>Emotional Appeal: Does it resonate emotionally?</li> <li>Uniqueness: Is it creative and original?</li> </ol> <p>Response Patterns:</p> <ul> <li>Needs Improvement: Provides specific, actionable feedback only</li> <li>Excellent: Responds with \"SHIP IT!\" only (no additional text)</li> <li>Critical Rule: Never mixes feedback with approval</li> </ul>"},{"location":"api-reference/agents/#usage-example_1","title":"Usage Example","text":"<pre><code>from src.agents.reviewer import create_reviewer_agent\nfrom src.config import get_ollama_config\n\n# Create reviewer agent\nconfig = get_ollama_config()\nreviewer = create_reviewer_agent(config)\n\n# Evaluate slogan\nslogan = \"Cloud Storage Solutions\"\nresponse = await reviewer.send_message(\n    f\"Review this slogan for a cloud storage company: {slogan}\"\n)\nfeedback = response.message.content\n\nprint(f\"Reviewer: {feedback}\")\n# Output: \"Too generic. What makes this storage different? Add unique value.\"\n\n# After revision\nimproved_slogan = \"\u2601\ufe0f Your Files, Anywhere, Instantly\"\nresponse = await reviewer.send_message(\n    f\"Review this slogan: {improved_slogan}\"\n)\nfeedback = response.message.content\n\nprint(f\"Reviewer: {feedback}\")\n# Output: \"SHIP IT!\"\n</code></pre>"},{"location":"api-reference/agents/#agent-configuration","title":"Agent Configuration","text":"<p>Both agents are configured using <code>OllamaConfig</code> which includes:</p> Parameter Type Description <code>base_url</code> str Ollama API endpoint (default: http://localhost:11434/v1) <code>model_name</code> str Model identifier (default: mistral:latest) <code>temperature</code> float Sampling temperature (default: 0.7) <code>max_tokens</code> int Maximum response length (default: 500) <code>timeout</code> int Request timeout in seconds (default: 30)"},{"location":"api-reference/agents/#configuration-example","title":"Configuration Example","text":"<pre><code>from src.config import get_ollama_config\n\n# Get default configuration\nconfig = get_ollama_config()\n\n# Or create custom configuration\nfrom src.config.settings import OllamaConfig\n\ncustom_config = OllamaConfig(\n    base_url=\"http://localhost:11434/v1\",\n    model_name=\"llama3.2:latest\",\n    temperature=0.8,\n    max_tokens=300\n)\n\n# Use with agents\nwriter = create_writer_agent(custom_config)\nreviewer = create_reviewer_agent(custom_config)\n</code></pre>"},{"location":"api-reference/agents/#agent-communication","title":"Agent Communication","text":"<p>Agents communicate using the Microsoft Agent Framework's message protocol:</p> <pre><code># Send message to agent\nresponse = await agent.send_message(prompt)\n\n# Access response content\ncontent = response.message.content\n\n# Response includes metadata\nprint(f\"Role: {response.message.role}\")  # 'assistant'\nprint(f\"Content: {response.message.content}\")  # The slogan or feedback\n</code></pre>"},{"location":"api-reference/agents/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/agents/#writer-agent_1","title":"Writer Agent","text":"<p>\u2705 Do: - Provide specific, detailed product/service descriptions - Include target audience when relevant - Give context about brand personality - Let the writer be creative</p> <p>\u274c Don't: - Give vague, generic inputs like \"business\" or \"app\" - Over-constrain the creative process - Expect exact character counts (aim for &lt;100 chars)</p>"},{"location":"api-reference/agents/#reviewer-agent_1","title":"Reviewer Agent","text":"<p>\u2705 Do: - Trust the reviewer's critical evaluation - Use feedback to improve slogans iteratively - Expect high standards (only great slogans get \"SHIP IT!\")</p> <p>\u274c Don't: - Expect approval on first try - Ignore specific feedback points - Assume feedback means rejection (it's guidance)</p>"},{"location":"api-reference/agents/#model-selection","title":"Model Selection","text":"<p>Choose models based on your needs:</p> Model Size Writer Performance Reviewer Performance Best For <code>gemma2:2b</code> 2B Good Fair Quick testing <code>phi3:mini</code> 3.8B Very Good Good Development <code>mistral:latest</code> 7B Excellent Excellent Production <code>llama3.2:latest</code> 8B Excellent Excellent High quality <p>Note: Smaller models (1B-2B) may struggle with the strict output format required by the reviewer and can produce validation errors.</p>"},{"location":"api-reference/agents/#error-handling","title":"Error Handling","text":"<p>Common agent-related errors:</p>"},{"location":"api-reference/agents/#connection-errors","title":"Connection Errors","text":"<pre><code>try:\n    writer = create_writer_agent(config)\n    response = await writer.send_message(prompt)\nexcept Exception as e:\n    if \"connection\" in str(e).lower():\n        print(\"Ollama is not running. Start it with: ollama serve\")\n    raise\n</code></pre>"},{"location":"api-reference/agents/#timeout-errors","title":"Timeout Errors","text":"<pre><code>from src.config.settings import OllamaConfig\n\n# Increase timeout for slower models\nconfig = OllamaConfig(timeout=60)  # 60 seconds\nwriter = create_writer_agent(config)\n</code></pre>"},{"location":"api-reference/agents/#model-not-found","title":"Model Not Found","text":"<pre><code># Ensure model is pulled\nollama list\nollama pull mistral:latest\n</code></pre>"},{"location":"api-reference/agents/#see-also","title":"See Also","text":"<ul> <li>Orchestration API - Workflow coordination between agents</li> <li>Configuration API - Configuration management</li> <li>CLI Usage Guide - Using agents via CLI</li> <li>Architecture Overview - System design</li> </ul>"},{"location":"api-reference/cli/","title":"CLI API Reference","text":""},{"location":"api-reference/cli/#cli-api-reference_1","title":"CLI API Reference","text":"<p>This page documents the CLI implementation for the Slogan Writer-Reviewer system.</p>"},{"location":"api-reference/cli/#overview","title":"Overview","text":"<p>The CLI provides a user-friendly command-line interface built with Click. It includes commands for generating slogans, managing models, and configuring settings.</p>"},{"location":"api-reference/cli/#main-entry-point","title":"Main Entry Point","text":"<p>The main CLI group that serves as the entry point for all commands.</p>"},{"location":"api-reference/cli/#src.cli.main.cli","title":"cli","text":"<pre><code>cli()\n</code></pre> <p>AI-powered slogan generation using Writer-Reviewer collaboration.</p> Source code in <code>src/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=\"0.1.0\", prog_name=\"slogan-gen\")\ndef cli() -&gt; None:\n    \"\"\"AI-powered slogan generation using Writer-Reviewer collaboration.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cli/#commands","title":"Commands","text":""},{"location":"api-reference/cli/#generate","title":"generate","text":"<p>Purpose: Generate creative slogans through Writer-Reviewer collaboration.</p> <p>Usage:</p> <pre><code>slogan-gen generate \"eco-friendly water bottle\"\nslogan-gen generate \"AI assistant\" --model mistral:latest --max-turns 7 --verbose\nslogan-gen generate \"coffee shop\" --output result.json\n</code></pre> <p>Options:</p> Option Type Default Description <code>--model</code> string <code>mistral:latest</code> Ollama model to use <code>--max-turns</code> integer 5 Maximum iteration turns (1-10) <code>--verbose</code> / <code>-v</code> flag false Show detailed iteration history <code>--output</code> / <code>-o</code> path Save results to file (.txt or .json)"},{"location":"api-reference/cli/#src.cli.main.generate","title":"generate","text":"<pre><code>generate(input, model, max_turns, verbose, output)\n</code></pre> <p>Generate a creative slogan for INPUT.</p> <p>INPUT: Product or service description (e.g., \"eco-friendly water bottle\")</p> <p>Examples:</p> <pre><code>slogan-gen generate \"eco-friendly water bottle\"\n\nslogan-gen generate \"AI coding assistant\" --model mistral:latest\n\nslogan-gen generate \"cloud platform\" --max-turns 10 --verbose\n</code></pre> Source code in <code>src/cli/main.py</code> <pre><code>@cli.command()\n@click.argument(\"input\", required=True)\n@click.option(\n    \"--model\",\n    default=\"mistral:latest\",\n    help=\"Ollama model to use (default: mistral:latest)\",\n)\n@click.option(\n    \"--max-turns\",\n    type=click.IntRange(1, 10),\n    default=None,\n    help=\"Maximum iteration turns (default: 5, range: 1-10)\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed iteration history\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(writable=True),\n    help=\"Save results to file (supports .txt or .json)\",\n)\ndef generate(\n    input: str, model: str, max_turns: int | None, verbose: bool, output: str | None\n) -&gt; None:\n    \"\"\"Generate a creative slogan for INPUT.\n\n    INPUT: Product or service description (e.g., \"eco-friendly water bottle\")\n\n    Examples:\n\n        slogan-gen generate \"eco-friendly water bottle\"\n\n        slogan-gen generate \"AI coding assistant\" --model mistral:latest\n\n        slogan-gen generate \"cloud platform\" --max-turns 10 --verbose\n    \"\"\"\n    # Validate input\n    if not input.strip():\n        click.echo(\"\u274c Error: Input cannot be empty\", err=True)\n        sys.exit(1)\n\n    try:\n        # Validate model exists (optional but recommended)\n        try:\n            available_models = get_available_models(timeout=5)\n            if model not in available_models:\n                click.echo(\n                    f\"\u26a0\ufe0f  Warning: Model '{model}' not found in available models.\\n\",\n                    err=True,\n                )\n                click.echo(f\"Available models: {', '.join(sorted(available_models))}\\n\", err=True)\n\n                if not click.confirm(\"Continue anyway?\", default=False):\n                    click.echo(\"\\n\ud83d\udca1 To install the model, run: ollama pull \" + model)\n                    sys.exit(1)\n        except (ConnectionError, RuntimeError):\n            # If we can't fetch models, just warn and continue\n            click.echo(\"\u26a0\ufe0f  Warning: Could not verify model availability\\n\", err=True)\n\n        # Run the slogan generation workflow\n        click.echo(f\"\ud83d\ude80 Generating slogan for: {input}\")\n        click.echo(f\"   Using model: {model}\")\n        if max_turns:\n            click.echo(f\"   Max iterations: {max_turns}\")\n        click.echo()\n\n        # Execute async workflow\n        session = asyncio.run(\n            run_slogan_generation(\n                user_input=input,\n                model_name=model,\n                max_turns=max_turns,\n            )\n        )\n\n        # Display results\n        output_text = format_session_output(session, verbose=verbose)\n        click.echo(output_text)\n\n        # Save to file if requested\n        if output:\n            try:\n                import json\n                from pathlib import Path\n\n                output_path = Path(output)\n\n                # Determine format based on extension\n                if output_path.suffix.lower() == \".json\":\n                    # Save as JSON\n                    with open(output_path, \"w\") as f:\n                        json.dump(session.model_dump(mode=\"json\"), f, indent=2, default=str)\n                    click.echo(f\"\\n\ud83d\udcbe Session saved to {output_path} (JSON format)\")\n                else:\n                    # Save as text (default)\n                    with open(output_path, \"w\") as f:\n                        f.write(output_text)\n                    click.echo(f\"\\n\ud83d\udcbe Results saved to {output_path}\")\n\n            except Exception as e:\n                click.echo(f\"\\n\u26a0\ufe0f  Warning: Could not save to file: {e}\", err=True)\n\n    except ValueError as e:\n        click.echo(f\"\u274c Validation Error: {e}\", err=True)\n        sys.exit(1)\n    except ConnectionError as e:\n        click.echo(\n            f\"\u274c Connection Error: {e}\\n\\n\"\n            f\"\ud83d\udca1 Tips:\\n\"\n            f\"   \u2022 Ensure Ollama is running: ollama serve\\n\"\n            f\"   \u2022 Check if the model is available: ollama list\\n\"\n            f\"   \u2022 Pull the model if needed: ollama pull {model}\",\n            err=True,\n        )\n        sys.exit(1)\n    except RuntimeError as e:\n        click.echo(f\"\u274c Workflow Error: {e}\", err=True)\n        sys.exit(1)\n    except Exception as e:\n        click.echo(f\"\u274c Unexpected Error: {e}\", err=True)\n        if verbose:\n            import traceback\n\n            traceback.print_exc()\n        sys.exit(1)\n</code></pre>"},{"location":"api-reference/cli/#models","title":"models","text":"<p>Purpose: List available Ollama models.</p> <p>Usage:</p> <pre><code>slogan-gen models\nslogan-gen models --refresh\n</code></pre> <p>Options:</p> Option Description <code>--refresh</code> / <code>-r</code> Force refresh the model list from Ollama"},{"location":"api-reference/cli/#src.cli.main.models","title":"models","text":"<pre><code>models(refresh)\n</code></pre> <p>List available Ollama models.</p> <p>Queries the Ollama API to display all installed models.</p> <p>Examples:</p> <pre><code>slogan-gen models\n\nslogan-gen models --refresh\n</code></pre> Source code in <code>src/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--refresh\",\n    \"-r\",\n    is_flag=True,\n    help=\"Force refresh the model list from Ollama\",\n)\ndef models(refresh: bool) -&gt; None:\n    \"\"\"List available Ollama models.\n\n    Queries the Ollama API to display all installed models.\n\n    Examples:\n\n        slogan-gen models\n\n        slogan-gen models --refresh\n    \"\"\"\n    try:\n        config = get_ollama_config()\n\n        if refresh:\n            click.echo(\"\ud83d\udd04 Refreshing model list...\\n\")\n\n        available_models = get_available_models()\n\n        if not available_models:\n            click.echo(\"\u26a0\ufe0f  No models found.\")\n            click.echo(\"\\n\ud83d\udca1 To install a model, run: ollama pull &lt;model-name&gt;\")\n            click.echo(\"   Example: ollama pull llama3.2:latest\")\n            return\n\n        click.echo(click.style(\"\ud83d\udce6 Available Ollama Models:\", bold=True, fg=\"cyan\"))\n        click.echo()\n\n        for i, model in enumerate(sorted(available_models), 1):\n            # Highlight the default model\n            if model == config.model_name:\n                click.echo(f\"  {i}. {click.style(model, fg='green', bold=True)} (default)\")\n            else:\n                click.echo(f\"  {i}. {model}\")\n\n        click.echo(f\"\\n\u2713 Total: {len(available_models)} models\")\n        click.echo('\\n\ud83d\udca1 Use with: slogan-gen generate \"your input\" --model &lt;model-name&gt;')\n\n    except ConnectionError as e:\n        click.echo(f\"\u274c {e}\", err=True)\n        click.echo(\"\\n\ud83d\udca1 Make sure Ollama is running: ollama serve\", err=True)\n        sys.exit(1)\n    except Exception as e:\n        click.echo(f\"\u274c Error fetching models: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api-reference/cli/#config-group","title":"config (group)","text":"<p>Purpose: Manage configuration settings.</p> <p>Parent command for configuration subcommands.</p>"},{"location":"api-reference/cli/#src.cli.main.config","title":"config","text":"<pre><code>config()\n</code></pre> <p>Manage configuration settings.</p> <p>View and modify slogan generation configuration.</p> Source code in <code>src/cli/main.py</code> <pre><code>@cli.group()\ndef config() -&gt; None:\n    \"\"\"Manage configuration settings.\n\n    View and modify slogan generation configuration.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cli/#config-show","title":"config show","text":"<p>Purpose: Display current configuration settings.</p> <p>Usage:</p> <pre><code>slogan-gen config show\n</code></pre> <p>Output:</p> <pre><code>\u2699\ufe0f  Current Configuration:\n============================================================\nOllama Base URL.............. http://localhost:11434\nDefault Model................ mistral:latest\nTemperature.................. 0.7 (range: 0.0-2.0)\nMax Tokens................... 500 (range: 1-4096)\nTimeout...................... 30s (range: 1-300)\nMax Turns.................... 5 (range: 1-10)\n============================================================\n</code></pre>"},{"location":"api-reference/cli/#src.cli.main.config_show","title":"config_show","text":"<pre><code>config_show()\n</code></pre> <p>Display current configuration settings.</p> <p>Shows all configuration values including defaults and environment overrides.</p> <p>Example:</p> <pre><code>slogan-gen config show\n</code></pre> Source code in <code>src/cli/main.py</code> <pre><code>@config.command(name=\"show\")\ndef config_show() -&gt; None:\n    \"\"\"Display current configuration settings.\n\n    Shows all configuration values including defaults and environment overrides.\n\n    Example:\n\n        slogan-gen config show\n    \"\"\"\n    try:\n        cfg = get_ollama_config()\n\n        click.echo(click.style(\"\\n\u2699\ufe0f  Current Configuration:\", bold=True, fg=\"cyan\"))\n        click.echo(\"=\" * 60)\n\n        # Format each config value\n        settings = [\n            (\"Ollama Base URL\", cfg.base_url),\n            (\"Default Model\", cfg.model_name),\n            (\"Temperature\", f\"{cfg.temperature} (range: 0.0-2.0)\"),\n            (\"Max Tokens\", f\"{cfg.max_tokens} (range: 1-4096)\"),\n            (\"Timeout\", f\"{cfg.timeout}s (range: 1-300)\"),\n            (\"Max Turns\", f\"{cfg.max_turns} (range: 1-10)\"),\n        ]\n\n        for label, value in settings:\n            click.echo(f\"{label:.&lt;25} {click.style(str(value), fg='green')}\")\n\n        click.echo(\"=\" * 60)\n        click.echo(\"\\n\ud83d\udca1 To modify settings, set environment variables:\")\n        click.echo(\"   Example: export OLLAMA_MODEL_NAME=mistral:latest\")\n        click.echo(\"   Or create a .env file in your project directory\")\n\n    except Exception as e:\n        click.echo(f\"\u274c Error loading configuration: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api-reference/cli/#config-set","title":"config set","text":"<p>Purpose: Set configuration value via environment variable (temporary for current session).</p> <p>Usage:</p> <pre><code>slogan-gen config set MODEL_NAME mistral:latest\nslogan-gen config set MAX_TURNS 7\nslogan-gen config set TEMPERATURE 0.9\n</code></pre> <p>Key Mapping:</p> Friendly Key Environment Variable <code>MODEL_NAME</code> / <code>MODEL</code> <code>OLLAMA_MODEL_NAME</code> <code>BASE_URL</code> / <code>URL</code> <code>OLLAMA_BASE_URL</code> <code>TEMPERATURE</code> / <code>TEMP</code> <code>OLLAMA_TEMPERATURE</code> <code>MAX_TOKENS</code> / <code>TOKENS</code> <code>OLLAMA_MAX_TOKENS</code> <code>TIMEOUT</code> <code>OLLAMA_TIMEOUT</code> <code>MAX_TURNS</code> / <code>TURNS</code> <code>OLLAMA_MAX_TURNS</code>"},{"location":"api-reference/cli/#src.cli.main.config_set","title":"config_set","text":"<pre><code>config_set(key, value)\n</code></pre> <p>Set a configuration value via environment variable.</p> <p>KEY: Configuration key (e.g., MODEL_NAME, MAX_TURNS) VALUE: New value to set</p> <p>Note: This sets environment variables for the current session. For persistent changes, add to your .env file or shell profile.</p> <p>Example:</p> <pre><code>slogan-gen config set MODEL_NAME mistral:latest\n\nslogan-gen config set MAX_TURNS 7\n</code></pre> Source code in <code>src/cli/main.py</code> <pre><code>@config.command(name=\"set\")\n@click.argument(\"key\")\n@click.argument(\"value\")\ndef config_set(key: str, value: str) -&gt; None:\n    \"\"\"Set a configuration value via environment variable.\n\n    KEY: Configuration key (e.g., MODEL_NAME, MAX_TURNS)\n    VALUE: New value to set\n\n    Note: This sets environment variables for the current session.\n    For persistent changes, add to your .env file or shell profile.\n\n    Example:\n\n        slogan-gen config set MODEL_NAME mistral:latest\n\n        slogan-gen config set MAX_TURNS 7\n    \"\"\"\n    import os\n\n    # Map friendly names to environment variable names\n    key_mapping = {\n        \"MODEL_NAME\": \"OLLAMA_MODEL_NAME\",\n        \"MODEL\": \"OLLAMA_MODEL_NAME\",\n        \"BASE_URL\": \"OLLAMA_BASE_URL\",\n        \"URL\": \"OLLAMA_BASE_URL\",\n        \"TEMPERATURE\": \"OLLAMA_TEMPERATURE\",\n        \"TEMP\": \"OLLAMA_TEMPERATURE\",\n        \"MAX_TOKENS\": \"OLLAMA_MAX_TOKENS\",\n        \"TOKENS\": \"OLLAMA_MAX_TOKENS\",\n        \"TIMEOUT\": \"OLLAMA_TIMEOUT\",\n        \"MAX_TURNS\": \"OLLAMA_MAX_TURNS\",\n        \"TURNS\": \"OLLAMA_MAX_TURNS\",\n    }\n\n    # Normalize key\n    key_upper = key.upper()\n    env_key = key_mapping.get(key_upper, f\"OLLAMA_{key_upper}\")\n\n    try:\n        # Set environment variable\n        os.environ[env_key] = value\n\n        # Clear cache to reload config\n        get_ollama_config.cache_clear()\n\n        # Validate by loading config\n        get_ollama_config()\n\n        click.echo(f\"\u2713 Set {click.style(env_key, fg='cyan')} = {click.style(value, fg='green')}\")\n        click.echo(\"\\n\ud83d\udca1 This change is temporary for the current session.\")\n        click.echo(\"   For persistent changes, add to your .env file:\")\n        click.echo(f\"   echo '{env_key}={value}' &gt;&gt; .env\")\n\n    except Exception as e:\n        click.echo(f\"\u274c Error setting configuration: {e}\", err=True)\n        sys.exit(1)\n</code></pre>"},{"location":"api-reference/cli/#output-formatting","title":"Output Formatting","text":"<p>Purpose: Format IterationSession for CLI output.</p> <p>Parameters:</p> Parameter Type Default Description <code>session</code> IterationSession required The completed session <code>verbose</code> bool <code>False</code> Include turn-by-turn details <p>Output Modes:</p> <p>Standard Mode (verbose=False): <pre><code>\u2705 Final Slogan:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n   Your Generated Slogan Here!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2713 Approved by Reviewer in 3 turns\n\u23f1\ufe0f  Total duration: 8.2 seconds\n</code></pre></p> <p>Verbose Mode (verbose=True): <pre><code>\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nTurn 1/5\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\ud83d\udcdd Writer's Slogan:\n   Initial Slogan Here\n\n\ud83d\udcad Reviewer's Feedback:\n   Feedback text here\n\nDuration: 2.3s\n\n[... additional turns ...]\n\n\u2705 Final Slogan:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n   Final Approved Slogan\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2713 Approved by Reviewer in 3 turns\n\u23f1\ufe0f  Total duration: 8.2 seconds (avg 2.7s per turn)\n</code></pre></p> <p>Usage Example:</p> <pre><code>from src.cli.output import format_session_output\nfrom src.orchestration import run_slogan_generation\n\nsession = await run_slogan_generation(\"coffee shop\")\n\n# Standard output\noutput = format_session_output(session, verbose=False)\nprint(output)\n\n# Verbose output\noutput = format_session_output(session, verbose=True)\nprint(output)\n</code></pre>"},{"location":"api-reference/cli/#src.cli.output.format_session_output","title":"format_session_output","text":"<pre><code>format_session_output(session, verbose=False)\n</code></pre> <p>Format session output for display.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>IterationSession</code> <p>The completed iteration session</p> required <code>verbose</code> <code>bool</code> <p>If True, include all iteration details</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted output string</p> Source code in <code>src/cli/output.py</code> <pre><code>def format_session_output(session: IterationSession, verbose: bool = False) -&gt; str:\n    \"\"\"Format session output for display.\n\n    Args:\n        session: The completed iteration session\n        verbose: If True, include all iteration details\n\n    Returns:\n        Formatted output string\n    \"\"\"\n    if not session.completed:\n        return click.style(\"\u26a0\ufe0f  Session not completed\", fg=\"yellow\")\n\n    lines = []\n\n    # Header\n    separator = \"=\" * 60\n    lines.append(\"\\n\" + separator)\n    lines.append(click.style(\"\ud83c\udfaf SLOGAN GENERATION RESULTS\", fg=\"cyan\", bold=True))\n    lines.append(separator)\n\n    # Final slogan\n    if session.final_slogan:\n        slogan_text = f\"\u2728 Final Slogan: {session.final_slogan}\"\n        lines.append(\"\\n\" + click.style(slogan_text, fg=\"green\", bold=True))\n\n    # Completion status\n    completion_emoji = {\n        CompletionReason.APPROVED: \"\u2705\",\n        CompletionReason.MAX_TURNS: \"\u23f1\ufe0f \",\n        CompletionReason.ERROR: \"\u274c\",\n    }\n    completion_colors = {\n        CompletionReason.APPROVED: \"green\",\n        CompletionReason.MAX_TURNS: \"yellow\",\n        CompletionReason.ERROR: \"red\",\n    }\n    emoji = completion_emoji.get(session.completion_reason, \"\") if session.completion_reason else \"\"\n    reason_text = (\n        session.completion_reason.value.replace(\"_\", \" \").title()\n        if session.completion_reason\n        else \"Unknown\"\n    )\n    color = (\n        completion_colors.get(session.completion_reason, \"white\")\n        if session.completion_reason\n        else \"white\"\n    )\n    status_text = f\"{emoji} Status: {reason_text}\"\n    lines.append(\"\\n\" + click.style(status_text, fg=color, bold=True))\n\n    # Stats\n    lines.append(\"\\n\" + click.style(\"\ud83d\udcca Statistics:\", bold=True))\n    lines.append(f\"   \u2022 Total iterations: {len(session.turns)}\")\n    lines.append(f\"   \u2022 Model: {session.model_name}\")\n    lines.append(f\"   \u2022 Input: {session.user_input}\")\n\n    # Timing information\n    if session.completed_at:\n        duration = (session.completed_at - session.started_at).total_seconds()\n        lines.append(click.style(f\"   \u2022 Duration: {duration:.1f} seconds\", dim=True))\n\n        # Average turn time if multiple turns\n        if len(session.turns) &gt; 1:\n            avg_time = duration / len(session.turns)\n            lines.append(click.style(f\"   \u2022 Average per turn: {avg_time:.1f} seconds\", dim=True))\n\n    # Iteration details (if verbose)\n    if verbose and session.turns:\n        lines.append(\"\\n\" + click.style(\"\ud83d\udcdd Iteration Details:\", bold=True))\n        for turn in session.turns:\n            turn_header = click.style(f\"\\n   Turn {turn.turn_number}\", bold=True, dim=True)\n\n            # Calculate per-turn duration\n            if turn.turn_number &gt; 1:\n                prev_turn = session.turns[turn.turn_number - 2]\n                turn_duration = (turn.timestamp - prev_turn.timestamp).total_seconds()\n                turn_header += click.style(f\" ({turn_duration:.1f}s)\", dim=True)\n            elif session.turns:\n                # First turn duration from session start\n                turn_duration = (turn.timestamp - session.started_at).total_seconds()\n                turn_header += click.style(f\" ({turn_duration:.1f}s)\", dim=True)\n\n            lines.append(turn_header + \":\")\n\n            lines.append(f\"   Slogan: {turn.slogan}\")\n\n            if turn.feedback:\n                feedback_label = click.style(\"Feedback:\", fg=\"yellow\")\n                lines.append(f\"   {feedback_label} {turn.feedback}\")\n\n            if turn.approved:\n                approved_text = click.style(\"\u2705 Yes\", fg=\"green\", bold=True)\n            else:\n                approved_text = click.style(\"\u274c No\", fg=\"red\")\n            lines.append(f\"   Approved: {approved_text}\")\n\n    lines.append(\"\\n\" + separator + \"\\n\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api-reference/cli/#error-handling","title":"Error Handling","text":"<p>The CLI provides user-friendly error messages for common issues:</p>"},{"location":"api-reference/cli/#empty-input","title":"Empty Input","text":"<pre><code>$ slogan-gen generate \"\"\n\u274c Error: Input cannot be empty\n</code></pre>"},{"location":"api-reference/cli/#model-not-found","title":"Model Not Found","text":"<pre><code>$ slogan-gen generate \"test\" --model unknown\n\u26a0\ufe0f  Warning: Model 'unknown' not found in available models.\n\nAvailable models: gemma2:2b, mistral:latest, phi3:mini\n\nContinue anyway? [y/N]: n\n\n\ud83d\udca1 To install the model, run: ollama pull unknown\n</code></pre>"},{"location":"api-reference/cli/#connection-error","title":"Connection Error","text":"<pre><code>$ slogan-gen generate \"test\"\n\u274c Connection Error: Cannot connect to Ollama at http://localhost:11434\n\n\ud83d\udca1 Tips:\n   \u2022 Ensure Ollama is running: ollama serve\n   \u2022 Check if the model is available: ollama list\n   \u2022 Pull the model if needed: ollama pull mistral:latest\n</code></pre>"},{"location":"api-reference/cli/#validation-error","title":"Validation Error","text":"<pre><code>$ slogan-gen generate \"test\" --model gemma2:2b\n\u274c Validation Error: 2 validation errors for Turn\nslogan: String should have at most 500 characters\nfeedback: String should have at most 1000 characters\n</code></pre>"},{"location":"api-reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error (validation, connection, runtime, etc.)"},{"location":"api-reference/cli/#color-and-styling","title":"Color and Styling","text":"<p>The CLI uses Click's styling features for visual feedback:</p> Element Style Example Success Green <code>\u2713 Approved by Reviewer</code> Error Red <code>\u274c Error: ...</code> Warning Yellow <code>\u26a0\ufe0f  Warning: ...</code> Info Cyan <code>\ud83d\ude80 Generating slogan...</code> Slogan Bold Final slogan display Duration Regular <code>\u23f1\ufe0f  Total duration: 8.2s</code>"},{"location":"api-reference/cli/#integration-examples","title":"Integration Examples","text":""},{"location":"api-reference/cli/#shell-script","title":"Shell Script","text":"<pre><code>#!/bin/bash\n# batch_generate.sh\n\nwhile read -r input; do\n  echo \"Generating for: $input\"\n  slogan-gen generate \"$input\" --output \"results/${input// /_}.json\"\ndone &lt; inputs.txt\n</code></pre>"},{"location":"api-reference/cli/#python-integration","title":"Python Integration","text":"<pre><code>import subprocess\nimport json\n\ndef generate_via_cli(input_text: str) -&gt; dict:\n    \"\"\"Generate slogan using CLI.\"\"\"\n    result = subprocess.run(\n        [\"slogan-gen\", \"generate\", input_text, \"--output\", \"-\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)\n\n# Usage\nslogan_data = generate_via_cli(\"coffee shop\")\nprint(slogan_data[\"final_slogan\"])\n</code></pre>"},{"location":"api-reference/cli/#see-also","title":"See Also","text":"<ul> <li>CLI Usage Guide - Complete CLI user guide</li> <li>Configuration API - Configuration management</li> <li>Orchestration API - Workflow coordination</li> <li>Troubleshooting Guide - Common CLI issues</li> </ul>"},{"location":"api-reference/config/","title":"Configuration API Reference","text":"<p>This page documents the configuration management for the Slogan Writer-Reviewer system.</p>"},{"location":"api-reference/config/#overview","title":"Overview","text":"<p>The configuration system uses Pydantic Settings to manage environment variables and provide type-safe configuration.</p> <p>Key Features:</p> <ul> <li>Environment variable-based configuration</li> <li><code>.env</code> file support</li> <li>Type validation with Pydantic</li> <li>Cached configuration singleton</li> <li>Automatic Ollama model discovery</li> </ul>"},{"location":"api-reference/config/#ollamaconfig","title":"OllamaConfig","text":"<p>Purpose: Main configuration class for Ollama settings.</p>"},{"location":"api-reference/config/#src.config.settings.OllamaConfig","title":"OllamaConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Ollama configuration from environment variables.</p>"},{"location":"api-reference/config/#configuration-fields","title":"Configuration Fields","text":"Field Type Default Range Description <code>base_url</code> str <code>http://localhost:11434/v1</code> Ollama API endpoint <code>model_name</code> str <code>mistral:latest</code> Default model identifier <code>temperature</code> float <code>0.7</code> 0.0-2.0 Sampling temperature <code>max_tokens</code> int <code>500</code> 1-4096 Maximum response length <code>timeout</code> int <code>30</code> 1-300 Request timeout (seconds) <code>max_turns</code> int <code>5</code> 1-10 Maximum iteration turns"},{"location":"api-reference/config/#environment-variables","title":"Environment Variables","text":"<p>All fields can be configured via environment variables with the <code>OLLAMA_</code> prefix:</p> Environment Variable Maps To Example <code>OLLAMA_BASE_URL</code> <code>base_url</code> <code>http://localhost:11434/v1</code> <code>OLLAMA_MODEL_NAME</code> <code>model_name</code> <code>mistral:latest</code> <code>OLLAMA_TEMPERATURE</code> <code>temperature</code> <code>0.7</code> <code>OLLAMA_MAX_TOKENS</code> <code>max_tokens</code> <code>500</code> <code>OLLAMA_TIMEOUT</code> <code>timeout</code> <code>30</code> <code>OLLAMA_MAX_TURNS</code> <code>max_turns</code> <code>5</code>"},{"location":"api-reference/config/#env-file-support","title":".env File Support","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOLLAMA_BASE_URL=http://localhost:11434/v1\nOLLAMA_MODEL_NAME=llama3.2:latest\nOLLAMA_TEMPERATURE=0.8\nOLLAMA_MAX_TOKENS=1000\nOLLAMA_TIMEOUT=60\nOLLAMA_MAX_TURNS=7\n</code></pre> <p>The configuration will automatically load from this file.</p>"},{"location":"api-reference/config/#usage-example","title":"Usage Example","text":"<pre><code>from src.config.settings import OllamaConfig\n\n# Create config (loads from environment/file)\nconfig = OllamaConfig()\n\nprint(f\"Using model: {config.model_name}\")\nprint(f\"Temperature: {config.temperature}\")\nprint(f\"Max turns: {config.max_turns}\")\n\n# Override values\ncustom_config = OllamaConfig(\n    model_name=\"gemma2:2b\",\n    max_turns=3,\n    temperature=0.9\n)\n\n# Validation is automatic\ntry:\n    invalid = OllamaConfig(temperature=3.0)  # Error: must be &lt;= 2.0\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"api-reference/config/#configuration-functions","title":"Configuration Functions","text":""},{"location":"api-reference/config/#get_ollama_config","title":"get_ollama_config","text":"<p>Purpose: Get cached configuration singleton.</p> <p>Returns: <code>OllamaConfig</code> instance</p> <p>Behavior:</p> <ul> <li>First call: Creates and caches configuration</li> <li>Subsequent calls: Returns cached instance</li> <li>Cache can be cleared: <code>get_ollama_config.cache_clear()</code></li> </ul> <p>Usage Example:</p> <pre><code>from src.config import get_ollama_config\n\n# Get config (cached)\nconfig = get_ollama_config()\n\n# Use in agents\nfrom src.agents import create_writer_agent\nwriter = create_writer_agent(config)\n\n# Clear cache to reload config\nimport os\nos.environ[\"OLLAMA_MODEL_NAME\"] = \"llama3.2:latest\"\nget_ollama_config.cache_clear()\nconfig = get_ollama_config()  # Reloads with new environment\n</code></pre>"},{"location":"api-reference/config/#src.config.settings.get_ollama_config","title":"get_ollama_config  <code>cached</code>","text":"<pre><code>get_ollama_config()\n</code></pre> <p>Get cached Ollama configuration instance.</p> Source code in <code>src/config/settings.py</code> <pre><code>@lru_cache\ndef get_ollama_config() -&gt; OllamaConfig:\n    \"\"\"Get cached Ollama configuration instance.\"\"\"\n    return OllamaConfig()\n</code></pre>"},{"location":"api-reference/config/#get_available_models","title":"get_available_models","text":"<p>Purpose: Query Ollama API for installed models.</p> <p>Parameters:</p> Parameter Type Default Description <code>base_url</code> str | None <code>None</code> Ollama base URL (uses config if None) <code>timeout</code> int <code>10</code> Request timeout in seconds <p>Returns: <code>list[str]</code> - List of model names</p> <p>Raises:</p> <ul> <li><code>ConnectionError</code>: Cannot connect to Ollama</li> <li><code>RuntimeError</code>: API request failed</li> </ul> <p>Usage Example:</p> <pre><code>from src.config import get_available_models\n\n# Get models using default config\ntry:\n    models = get_available_models()\n    print(f\"Available models: {models}\")\n    # ['gemma2:2b', 'mistral:latest', 'llama3.2:latest']\n\nexcept ConnectionError as e:\n    print(f\"Ollama not running: {e}\")\nexcept RuntimeError as e:\n    print(f\"API error: {e}\")\n\n# Use custom Ollama instance\nmodels = get_available_models(\n    base_url=\"http://remote-server:11434/v1\",\n    timeout=30\n)\n</code></pre>"},{"location":"api-reference/config/#src.config.settings.get_available_models","title":"get_available_models","text":"<pre><code>get_available_models(base_url=None, timeout=10)\n</code></pre> <p>Query Ollama API for available models.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str | None</code> <p>Ollama base URL (defaults to config)</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>10</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of available model names</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If unable to connect to Ollama</p> <code>RuntimeError</code> <p>If API request fails</p> Source code in <code>src/config/settings.py</code> <pre><code>def get_available_models(base_url: str | None = None, timeout: int = 10) -&gt; list[str]:\n    \"\"\"Query Ollama API for available models.\n\n    Args:\n        base_url: Ollama base URL (defaults to config)\n        timeout: Request timeout in seconds\n\n    Returns:\n        List of available model names\n\n    Raises:\n        ConnectionError: If unable to connect to Ollama\n        RuntimeError: If API request fails\n    \"\"\"\n    if base_url is None:\n        config = get_ollama_config()\n        base_url = config.base_url\n\n    # Convert OpenAI-compatible URL to Ollama API URL\n    # e.g., http://localhost:11434/v1 -&gt; http://localhost:11434\n    ollama_base = base_url.rstrip(\"/\").removesuffix(\"/v1\")\n    tags_url = f\"{ollama_base}/api/tags\"\n\n    try:\n        response = httpx.get(tags_url, timeout=timeout)\n        response.raise_for_status()\n        data = response.json()\n\n        # Extract model names from response\n        models = data.get(\"models\", [])\n        return [model.get(\"name\", \"\") for model in models if model.get(\"name\")]\n\n    except httpx.ConnectError as e:\n        raise ConnectionError(\n            f\"Unable to connect to Ollama at {ollama_base}. Is Ollama running? Try: ollama serve\"\n        ) from e\n    except httpx.TimeoutException as e:\n        raise RuntimeError(f\"Request to Ollama timed out after {timeout}s\") from e\n    except httpx.HTTPStatusError as e:\n        raise RuntimeError(f\"Ollama API error: {e.response.status_code}\") from e\n    except Exception as e:\n        raise RuntimeError(f\"Failed to fetch models: {e}\") from e\n</code></pre>"},{"location":"api-reference/config/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"api-reference/config/#development-configuration","title":"Development Configuration","text":"<pre><code># config/dev.env\nOLLAMA_MODEL_NAME=gemma2:2b\nOLLAMA_MAX_TURNS=3\nOLLAMA_TIMEOUT=15\nOLLAMA_TEMPERATURE=0.7\n</code></pre> <pre><code>from dotenv import load_dotenv\nload_dotenv(\"config/dev.env\")\n\nfrom src.config import get_ollama_config\nconfig = get_ollama_config()\n</code></pre>"},{"location":"api-reference/config/#production-configuration","title":"Production Configuration","text":"<pre><code># config/prod.env\nOLLAMA_MODEL_NAME=mistral:latest\nOLLAMA_MAX_TURNS=5\nOLLAMA_TIMEOUT=30\nOLLAMA_TEMPERATURE=0.7\nOLLAMA_MAX_TOKENS=500\n</code></pre>"},{"location":"api-reference/config/#testing-configuration","title":"Testing Configuration","text":"<pre><code># tests/conftest.py\nimport pytest\nfrom src.config.settings import OllamaConfig\n\n@pytest.fixture\ndef test_config():\n    \"\"\"Test configuration with fast model.\"\"\"\n    return OllamaConfig(\n        model_name=\"gemma2:2b\",\n        max_turns=2,\n        timeout=10\n    )\n</code></pre>"},{"location":"api-reference/config/#runtime-configuration-override","title":"Runtime Configuration Override","text":"<pre><code>import os\nfrom src.config import get_ollama_config\n\n# Change configuration at runtime\nos.environ[\"OLLAMA_MODEL_NAME\"] = \"llama3.2:latest\"\nos.environ[\"OLLAMA_MAX_TURNS\"] = \"7\"\n\n# Clear cache to reload\nget_ollama_config.cache_clear()\n\n# New config takes effect\nconfig = get_ollama_config()\nprint(config.model_name)  # \"llama3.2:latest\"\nprint(config.max_turns)   # 7\n</code></pre>"},{"location":"api-reference/config/#configuration-validation","title":"Configuration Validation","text":"<p>Pydantic automatically validates configuration:</p>"},{"location":"api-reference/config/#type-validation","title":"Type Validation","text":"<pre><code># \u274c Invalid: wrong type\nconfig = OllamaConfig(max_turns=\"five\")  # Error: must be int\n\n# \u2705 Valid\nconfig = OllamaConfig(max_turns=5)\n</code></pre>"},{"location":"api-reference/config/#range-validation","title":"Range Validation","text":"<pre><code># \u274c Invalid: out of range\nconfig = OllamaConfig(temperature=3.0)  # Error: must be &lt;= 2.0\nconfig = OllamaConfig(max_turns=15)     # Error: must be &lt;= 10\n\n# \u2705 Valid\nconfig = OllamaConfig(temperature=0.8)\nconfig = OllamaConfig(max_turns=7)\n</code></pre>"},{"location":"api-reference/config/#url-validation","title":"URL Validation","text":"<pre><code># \u2705 Valid URLs\nconfig = OllamaConfig(base_url=\"http://localhost:11434/v1\")\nconfig = OllamaConfig(base_url=\"http://192.168.1.100:11434/v1\")\nconfig = OllamaConfig(base_url=\"https://ollama.example.com/v1\")\n</code></pre>"},{"location":"api-reference/config/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"api-reference/config/#dockercontainer","title":"Docker/Container","text":"<pre><code># Dockerfile\nENV OLLAMA_BASE_URL=http://ollama:11434/v1\nENV OLLAMA_MODEL_NAME=mistral:latest\nENV OLLAMA_MAX_TURNS=5\n</code></pre>"},{"location":"api-reference/config/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434/v1\n      - OLLAMA_MODEL_NAME=mistral:latest\n      - OLLAMA_TEMPERATURE=0.7\n</code></pre>"},{"location":"api-reference/config/#kubernetes","title":"Kubernetes","text":"<pre><code># deployment.yaml\nenv:\n  - name: OLLAMA_BASE_URL\n    value: \"http://ollama-service:11434/v1\"\n  - name: OLLAMA_MODEL_NAME\n    value: \"mistral:latest\"\n  - name: OLLAMA_MAX_TURNS\n    value: \"5\"\n</code></pre>"},{"location":"api-reference/config/#shellterminal","title":"Shell/Terminal","text":"<pre><code># .bashrc or .zshrc\nexport OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\nexport OLLAMA_MODEL_NAME=\"mistral:latest\"\nexport OLLAMA_TEMPERATURE=0.7\nexport OLLAMA_MAX_TURNS=5\n\n# Or inline\nOLLAMA_MODEL_NAME=gemma2:2b slogan-gen generate \"test\"\n</code></pre>"},{"location":"api-reference/config/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"api-reference/config/#use-env-files","title":"Use .env Files","text":"<p>\u2705 Do: <pre><code># .env\nOLLAMA_MODEL_NAME=mistral:latest\nOLLAMA_MAX_TURNS=5\n</code></pre></p> <p>\u274c Don't: <pre><code># Hardcoded values\nconfig = OllamaConfig(model_name=\"mistral:latest\")  # Inflexible\n</code></pre></p>"},{"location":"api-reference/config/#validate-early","title":"Validate Early","text":"<p>\u2705 Do: <pre><code># Validate at startup\nconfig = get_ollama_config()\nmodels = get_available_models()\nif config.model_name not in models:\n    print(f\"Warning: {config.model_name} not installed\")\n</code></pre></p>"},{"location":"api-reference/config/#cache-wisely","title":"Cache Wisely","text":"<p>\u2705 Do: <pre><code># Clear cache when environment changes\nos.environ[\"OLLAMA_MODEL_NAME\"] = \"new-model\"\nget_ollama_config.cache_clear()\nconfig = get_ollama_config()\n</code></pre></p> <p>\u274c Don't: <pre><code># Forgetting to clear cache\nos.environ[\"OLLAMA_MODEL_NAME\"] = \"new-model\"\nconfig = get_ollama_config()  # Still returns old cached config!\n</code></pre></p>"},{"location":"api-reference/config/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api-reference/config/#configuration-not-loading","title":"Configuration Not Loading","text":"<p>Problem: Changes to .env file not reflected</p> <p>Solution: <pre><code># Clear cache\nget_ollama_config.cache_clear()\n\n# Or restart application\n</code></pre></p>"},{"location":"api-reference/config/#model-validation-errors","title":"Model Validation Errors","text":"<p>Problem: <code>get_available_models()</code> raises ConnectionError</p> <p>Solution: <pre><code># Ensure Ollama is running\nollama serve\n\n# Verify Ollama is accessible\ncurl http://localhost:11434/api/tags\n</code></pre></p>"},{"location":"api-reference/config/#type-errors","title":"Type Errors","text":"<p>Problem: Pydantic validation errors</p> <p>Solution: <pre><code># Check environment variable types\nimport os\nprint(type(os.environ.get(\"OLLAMA_MAX_TURNS\")))  # str\n\n# Pydantic converts automatically, but must be valid\nos.environ[\"OLLAMA_MAX_TURNS\"] = \"5\"     # \u2705 Valid (converts to int)\nos.environ[\"OLLAMA_MAX_TURNS\"] = \"five\"  # \u274c Invalid (can't convert)\n</code></pre></p>"},{"location":"api-reference/config/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - User configuration guide</li> <li>CLI Usage Guide - Using configuration via CLI</li> <li>API Usage Guide - API configuration</li> <li>Development Guide - Development setup</li> </ul>"},{"location":"api-reference/openapi/","title":"OpenAPI Specification","text":"<p>This page provides access to the OpenAPI specification for the Slogan Writer-Reviewer API.</p>"},{"location":"api-reference/openapi/#interactive-documentation","title":"Interactive Documentation","text":"<p>The API provides multiple ways to explore and interact with the OpenAPI specification:</p>"},{"location":"api-reference/openapi/#swagger-ui-recommended","title":"Swagger UI (Recommended)","text":"<p>URL: <code>http://localhost:8000/docs</code></p> <p>Interactive documentation with: - Try-it-out functionality for all endpoints - Request/response examples - Schema definitions - Authentication testing - Download OpenAPI spec</p> <p>Features: - \u2705 Test endpoints directly from browser - \u2705 See request/response in real-time - \u2705 Explore schemas and models - \u2705 Copy cURL commands</p>"},{"location":"api-reference/openapi/#redoc","title":"ReDoc","text":"<p>URL: <code>http://localhost:8000/redoc</code></p> <p>Clean, responsive documentation with: - Three-column layout - Search functionality - Deep linking - Code samples in multiple languages</p> <p>Features: - \u2705 Better for reading/browsing - \u2705 More polished UI - \u2705 Mobile-friendly - \u2705 Printable format</p>"},{"location":"api-reference/openapi/#raw-openapi-spec","title":"Raw OpenAPI Spec","text":"<p>URL: <code>http://localhost:8000/openapi.json</code></p> <p>Download the raw OpenAPI 3.1.0 specification in JSON format for: - Code generation (client libraries) - API testing tools (Postman, Insomnia) - Documentation generators - Contract testing</p>"},{"location":"api-reference/openapi/#api-overview","title":"API Overview","text":"<p>API Name: Slogan Writer-Reviewer API Version: 1.0.0 OpenAPI Version: 3.1.0 Base URL: <code>http://localhost:8000</code></p>"},{"location":"api-reference/openapi/#endpoints","title":"Endpoints","text":"Method Path Description Tags <code>GET</code> <code>/</code> API information and documentation links info <code>GET</code> <code>/api/v1/health</code> Health check for API and dependencies monitoring <code>GET</code> <code>/api/v1/models</code> List available Ollama models models <code>POST</code> <code>/api/v1/slogans/generate</code> Generate slogan via Writer-Reviewer slogans"},{"location":"api-reference/openapi/#requestresponse-formats","title":"Request/Response Formats","text":"<p>Content Type: <code>application/json</code></p> <p>Request Schemas: - <code>GenerateRequest</code>: Slogan generation request</p> <p>Response Schemas: - <code>RootResponse</code>: API information - <code>HealthResponse</code>: Health check status - <code>ModelsResponse</code>: Available models list - <code>GenerateResponse</code>: Generated slogan with metadata - <code>DependencyStatus</code>: Dependency health status - <code>ModelInfo</code>: Model information - <code>TurnDetail</code>: Iteration turn details</p> <p>Error Schemas: - <code>HTTPValidationError</code>: Pydantic validation errors (422) - Standard HTTP error responses (400, 500, 503, 504)</p>"},{"location":"api-reference/openapi/#using-the-openapi-spec","title":"Using the OpenAPI Spec","text":""},{"location":"api-reference/openapi/#swagger-ui-examples","title":"Swagger UI Examples","text":""},{"location":"api-reference/openapi/#1-try-the-health-endpoint","title":"1. Try the Health Endpoint","text":"<ol> <li>Go to <code>http://localhost:8000/docs</code></li> <li>Expand <code>GET /api/v1/health</code></li> <li>Click \"Try it out\"</li> <li>Click \"Execute\"</li> <li>View the response</li> </ol>"},{"location":"api-reference/openapi/#2-generate-a-slogan","title":"2. Generate a Slogan","text":"<ol> <li>Go to <code>http://localhost:8000/docs</code></li> <li>Expand <code>POST /api/v1/slogans/generate</code></li> <li>Click \"Try it out\"</li> <li>Edit the request body:    <pre><code>{\n  \"input\": \"eco-friendly water bottles\",\n  \"verbose\": true\n}\n</code></pre></li> <li>Click \"Execute\"</li> <li>View the generated slogan and turn history</li> </ol>"},{"location":"api-reference/openapi/#3-test-different-models","title":"3. Test Different Models","text":"<ol> <li>First, get available models from <code>GET /api/v1/models</code></li> <li>Note the model names (e.g., <code>gemma2:2b</code>, <code>mistral:latest</code>)</li> <li>Use a model in <code>POST /api/v1/slogans/generate</code>:    <pre><code>{\n  \"input\": \"smart home devices\",\n  \"model\": \"gemma2:2b\",\n  \"max_turns\": 3\n}\n</code></pre></li> </ol>"},{"location":"api-reference/openapi/#redoc-examples","title":"ReDoc Examples","text":""},{"location":"api-reference/openapi/#browse-api-structure","title":"Browse API Structure","text":"<ol> <li>Go to <code>http://localhost:8000/redoc</code></li> <li>Use left sidebar to navigate between endpoints</li> <li>Click any endpoint to see details</li> <li>Expand \"Schema\" sections to see data models</li> <li>Use search (Ctrl/Cmd + K) to find specific endpoints or schemas</li> </ol>"},{"location":"api-reference/openapi/#copy-code-samples","title":"Copy Code Samples","text":"<ol> <li>Navigate to any endpoint in ReDoc</li> <li>Scroll to \"Code samples\" section</li> <li>Switch between languages (cURL, Python, JavaScript, etc.)</li> <li>Click \"Copy\" to copy the code sample</li> <li>Paste into your application</li> </ol>"},{"location":"api-reference/openapi/#download-openapi-spec","title":"Download OpenAPI Spec","text":""},{"location":"api-reference/openapi/#for-code-generation","title":"For Code Generation","text":"<pre><code># Download the spec\ncurl http://localhost:8000/openapi.json &gt; openapi.json\n\n# Generate Python client\nopenapi-generator-cli generate \\\n  -i openapi.json \\\n  -g python \\\n  -o ./client\n\n# Generate TypeScript client\nopenapi-generator-cli generate \\\n  -i openapi.json \\\n  -g typescript-axios \\\n  -o ./client\n</code></pre>"},{"location":"api-reference/openapi/#for-postmaninsomnia","title":"For Postman/Insomnia","text":"<ol> <li>Download spec: <code>curl http://localhost:8000/openapi.json &gt; openapi.json</code></li> <li>Postman:</li> <li>File \u2192 Import</li> <li>Select <code>openapi.json</code></li> <li>Choose \"OpenAPI 3.0\"</li> <li>All endpoints imported as collection</li> <li>Insomnia:</li> <li>Application \u2192 Import/Export \u2192 Import Data</li> <li>Select <code>openapi.json</code></li> <li>All endpoints imported</li> </ol>"},{"location":"api-reference/openapi/#for-api-testing","title":"For API Testing","text":"<pre><code># Using dredd (API contract testing)\ndredd openapi.json http://localhost:8000\n\n# Using schemathesis (property-based testing)\nschemathesis run http://localhost:8000/openapi.json\n</code></pre>"},{"location":"api-reference/openapi/#openapi-schema-details","title":"OpenAPI Schema Details","text":""},{"location":"api-reference/openapi/#security","title":"Security","text":"<p>Current Status: No authentication required (development)</p> <p>Future: Will support API key authentication</p> <pre><code># Planned security scheme\nsecuritySchemes:\n  ApiKeyAuth:\n    type: apiKey\n    in: header\n    name: X-API-Key\n</code></pre>"},{"location":"api-reference/openapi/#tags","title":"Tags","text":"<p>Endpoints are organized by tags:</p> Tag Description Endpoints <code>info</code> API information <code>/</code> <code>monitoring</code> Health checks <code>/api/v1/health</code> <code>models</code> Model management <code>/api/v1/models</code> <code>slogans</code> Slogan generation <code>/api/v1/slogans/generate</code>"},{"location":"api-reference/openapi/#servers","title":"Servers","text":"<p>Development: <pre><code>servers:\n  - url: http://localhost:8000\n    description: Local development server\n</code></pre></p> <p>Production (example): <pre><code>servers:\n  - url: https://api.example.com\n    description: Production server\n  - url: https://staging-api.example.com\n    description: Staging server\n</code></pre></p>"},{"location":"api-reference/openapi/#validating-the-openapi-spec","title":"Validating the OpenAPI Spec","text":""},{"location":"api-reference/openapi/#using-openapi-validator","title":"Using OpenAPI Validator","text":"<pre><code># Install validator\nnpm install -g @apidevtools/swagger-cli\n\n# Validate spec\nswagger-cli validate http://localhost:8000/openapi.json\n</code></pre>"},{"location":"api-reference/openapi/#using-spectral-openapi-linter","title":"Using Spectral (OpenAPI Linter)","text":"<pre><code># Install Spectral\nnpm install -g @stoplight/spectral-cli\n\n# Lint the spec\nspectral lint http://localhost:8000/openapi.json\n</code></pre>"},{"location":"api-reference/openapi/#using-online-validators","title":"Using online validators","text":"<ol> <li>Swagger Editor: https://editor.swagger.io/</li> <li>Paste spec JSON</li> <li>See validation errors in right panel</li> <li> <p>Preview documentation</p> </li> <li> <p>Redocly Editor: https://redocly.com/docs/api-reference-docs/</p> </li> <li>Upload <code>openapi.json</code></li> <li>Get detailed validation report</li> <li>Preview interactive docs</li> </ol>"},{"location":"api-reference/openapi/#extending-the-openapi-spec","title":"Extending the OpenAPI Spec","text":""},{"location":"api-reference/openapi/#adding-examples","title":"Adding Examples","text":"<p>Edit endpoint docstrings in source code:</p> <pre><code>@router.post(\"/slogans/generate\", response_model=GenerateResponse)\nasync def generate_slogan(request_body: GenerateRequest) -&gt; GenerateResponse:\n    \"\"\"\n    Generate a slogan through Writer-Reviewer collaboration.\n\n    Example request:\n    ```json\n    {\n      \"input\": \"eco-friendly water bottles\",\n      \"verbose\": true\n    }\n    ```\n\n    Example response:\n    ```json\n    {\n      \"slogan\": \"Stay Hydrated, Save the Planet\",\n      \"completion_reason\": \"approved\",\n      \"turn_count\": 2\n    }\n    ```\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/openapi/#adding-security-definitions","title":"Adding Security Definitions","text":"<p>In <code>src/api/main.py</code>:</p> <pre><code>from fastapi import FastAPI, Security\nfrom fastapi.security import APIKeyHeader\n\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\napp = FastAPI(\n    title=\"Slogan Writer-Reviewer API\",\n    version=\"1.0.0\",\n    # Add security schemes\n    openapi_tags=[\n        {\"name\": \"slogans\", \"description\": \"Slogan generation operations\"}\n    ]\n)\n\n@app.post(\"/api/v1/slogans/generate\")\nasync def generate_slogan(\n    request_body: GenerateRequest,\n    api_key: str = Security(api_key_header)  # Require API key\n):\n    ...\n</code></pre>"},{"location":"api-reference/openapi/#customizing-openapi-output","title":"Customizing OpenAPI Output","text":"<pre><code>from fastapi.openapi.utils import get_openapi\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n\n    openapi_schema = get_openapi(\n        title=\"Slogan Writer-Reviewer API\",\n        version=\"1.0.0\",\n        description=\"Multi-agent slogan generation\",\n        routes=app.routes,\n    )\n\n    # Add custom fields\n    openapi_schema[\"info\"][\"x-logo\"] = {\n        \"url\": \"https://example.com/logo.png\"\n    }\n\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\napp.openapi = custom_openapi\n</code></pre>"},{"location":"api-reference/openapi/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api-reference/openapi/#swagger-ui-not-loading","title":"Swagger UI Not Loading","text":"<p>Problem: <code>/docs</code> shows blank page or 404</p> <p>Solution: <pre><code># Ensure FastAPI is running\nuvicorn src.api.main:app --reload\n\n# Check if Swagger UI is enabled\n# In src/api/main.py, ensure docs_url is not disabled:\napp = FastAPI(\n    title=\"...\",\n    docs_url=\"/docs\",  # Must not be None\n    redoc_url=\"/redoc\"\n)\n</code></pre></p>"},{"location":"api-reference/openapi/#openapi-spec-invalid","title":"OpenAPI Spec Invalid","text":"<p>Problem: Validation errors in spec</p> <p>Solution: <pre><code># Regenerate spec\ncurl http://localhost:8000/openapi.json &gt; openapi.json\n\n# Validate\nswagger-cli validate openapi.json\n\n# Fix validation errors in source code docstrings\n</code></pre></p>"},{"location":"api-reference/openapi/#cors-errors-in-swagger-ui","title":"CORS Errors in Swagger UI","text":"<p>Problem: \"Failed to fetch\" errors when trying endpoints</p> <p>Solution: <pre><code># Set CORS origins to allow Swagger UI\nexport API_CORS_ORIGINS=\"*\"\n\n# Or specific origin\nexport API_CORS_ORIGINS=\"http://localhost:8000\"\n\n# Restart API\nuvicorn src.api.main:app --reload\n</code></pre></p>"},{"location":"api-reference/openapi/#missing-schemas","title":"Missing Schemas","text":"<p>Problem: Schemas not showing in OpenAPI spec</p> <p>Solution: <pre><code># Ensure response_model is specified\n@app.get(\"/endpoint\", response_model=MyResponse)\nasync def endpoint() -&gt; MyResponse:  # Type hint required\n    ...\n\n# Ensure Pydantic models have proper Config\nclass MyResponse(BaseModel):\n    field: str\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\"field\": \"example value\"}\n        }\n</code></pre></p>"},{"location":"api-reference/openapi/#see-also","title":"See Also","text":"<ul> <li>REST API Reference - Detailed REST API documentation</li> <li>API Usage Guide - How to use the API</li> <li>Development Guide - API development setup</li> <li>FastAPI Documentation - FastAPI framework docs</li> <li>OpenAPI Specification - OpenAPI standard</li> </ul>"},{"location":"api-reference/orchestration/","title":"Orchestration API Reference","text":""},{"location":"api-reference/orchestration/#orchestration-api-reference_1","title":"Orchestration API Reference","text":"<p>This page documents the workflow orchestration and data models that coordinate the Writer-Reviewer collaboration.</p>"},{"location":"api-reference/orchestration/#overview","title":"Overview","text":"<p>The orchestration layer manages the iterative collaboration between Writer and Reviewer agents. It handles:</p> <ul> <li>Workflow Coordination: Managing the turn-by-turn iteration loop</li> <li>State Management: Tracking session progress and turn history</li> <li>Completion Logic: Determining when to stop iterating</li> <li>Data Models: Structured representations of turns and sessions</li> </ul>"},{"location":"api-reference/orchestration/#workflow","title":"Workflow","text":""},{"location":"api-reference/orchestration/#src.orchestration.workflow.run_slogan_generation","title":"run_slogan_generation  <code>async</code>","text":"<pre><code>run_slogan_generation(user_input, model_name=None, max_turns=None)\n</code></pre> <p>Run the slogan generation workflow.</p> <p>Orchestrates the Writer-Reviewer collaboration to generate an approved slogan.</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>Product/service description from user</p> required <code>model_name</code> <code>str | None</code> <p>Override default Ollama model</p> <code>None</code> <code>max_turns</code> <code>int | None</code> <p>Override default max iteration turns</p> <code>None</code> <p>Returns:</p> Type Description <code>IterationSession</code> <p>Completed IterationSession with all turns and final result</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If user_input is empty</p> <code>RuntimeError</code> <p>If workflow fails to complete</p> Source code in <code>src/orchestration/workflow.py</code> <pre><code>async def run_slogan_generation(\n    user_input: str,\n    model_name: str | None = None,\n    max_turns: int | None = None,\n) -&gt; IterationSession:\n    \"\"\"Run the slogan generation workflow.\n\n    Orchestrates the Writer-Reviewer collaboration to generate an approved slogan.\n\n    Args:\n        user_input: Product/service description from user\n        model_name: Override default Ollama model\n        max_turns: Override default max iteration turns\n\n    Returns:\n        Completed IterationSession with all turns and final result\n\n    Raises:\n        ValueError: If user_input is empty\n        RuntimeError: If workflow fails to complete\n    \"\"\"\n    if not user_input or not user_input.strip():\n        raise ValueError(\"User input cannot be empty\")\n\n    # Load configuration\n    config = get_ollama_config()\n    if model_name:\n        config.model_name = model_name\n    if max_turns is not None:\n        config.max_turns = max_turns\n\n    # Initialize session\n    session = IterationSession(\n        user_input=user_input.strip(),\n        model_name=config.model_name,\n    )\n\n    # Create agents\n    writer = create_writer_agent(config)\n    reviewer = create_reviewer_agent(config)\n\n    try:\n        # Main iteration loop\n        while should_continue_iteration(session, config):\n            turn_number = len(session.turns) + 1\n\n            # Build writer prompt\n            if turn_number == 1:\n                writer_prompt = f\"Create a slogan for: {user_input}\"\n            else:\n                previous_feedback = session.turns[-1].feedback\n                previous_slogan = session.turns[-1].slogan\n                writer_prompt = (\n                    f\"Previous slogan: {previous_slogan}\\n\"\n                    f\"Feedback: {previous_feedback}\\n\\n\"\n                    f\"Create an improved slogan for: {user_input}\"\n                )\n\n            # Get slogan from writer\n            writer_response = await writer.run(writer_prompt)\n            slogan = writer_response.text.strip()\n\n            # Get feedback from reviewer\n            reviewer_prompt = (\n                f\"Review this slogan for '{user_input}':\\n\\n\"\n                f\"Slogan: {slogan}\\n\\n\"\n                f\"Provide feedback or approve with 'SHIP IT!'\"\n            )\n            reviewer_response = await reviewer.run(reviewer_prompt)\n            feedback = reviewer_response.text.strip()\n\n            # Check approval\n            approved = is_approved(feedback)\n\n            # Add turn to session\n            session.add_turn(\n                slogan=slogan,\n                feedback=feedback,\n                approved=approved,\n            )\n\n            # Stop if approved\n            if approved:\n                session.complete(CompletionReason.APPROVED)\n                break\n\n        # If not approved and loop ended, we hit max turns\n        if not session.completed:\n            session.complete(CompletionReason.MAX_TURNS)\n\n        return session\n\n    except Exception as e:\n        session.complete(CompletionReason.ERROR)\n        raise RuntimeError(f\"Workflow failed: {e}\") from e\n</code></pre>"},{"location":"api-reference/orchestration/#workflow-process","title":"Workflow Process","text":"<p>The slogan generation workflow follows this pattern:</p> <pre><code>1. Initialize Session\n   \u2193\n2. Turn Loop:\n   \u251c\u2500 Writer generates slogan\n   \u251c\u2500 Reviewer evaluates\n   \u251c\u2500 Check approval\n   \u2514\u2500 Continue or stop\n   \u2193\n3. Complete Session\n</code></pre> <p>Loop Continuation Logic:</p> <ul> <li>Continue if: Not approved AND turns &lt; max_turns</li> <li>Stop if: Approved OR reached max_turns</li> </ul>"},{"location":"api-reference/orchestration/#usage-example","title":"Usage Example","text":"<pre><code>from src.orchestration import run_slogan_generation\n\n# Basic usage\nsession = await run_slogan_generation(\n    user_input=\"eco-friendly water bottle\"\n)\n\nprint(f\"Final Slogan: {session.final_slogan}\")\nprint(f\"Reason: {session.completion_reason}\")\nprint(f\"Turns: {len(session.turns)}\")\n\n# With custom configuration\nsession = await run_slogan_generation(\n    user_input=\"AI coding assistant\",\n    model_name=\"llama3.2:latest\",\n    max_turns=7\n)\n\n# Access turn history\nfor turn in session.turns:\n    print(f\"Turn {turn.turn_number}: {turn.slogan}\")\n    print(f\"Feedback: {turn.feedback}\")\n    print(f\"Approved: {turn.approved}\")\n</code></pre>"},{"location":"api-reference/orchestration/#helper-functions","title":"Helper Functions","text":""},{"location":"api-reference/orchestration/#is_approved","title":"is_approved","text":"<p>Purpose: Detect if reviewer has approved the slogan.</p> <p>Detection Strategy:</p> <ol> <li>Checks if response starts with \"ship it\" (most reliable)</li> <li>Checks for \"ship it!\" on its own line</li> <li>Uses regex to match variations with punctuation</li> </ol> <p>Examples:</p> <pre><code>from src.orchestration.workflow import is_approved\n\n# Approved responses\nis_approved(\"SHIP IT!\")                    # True\nis_approved(\"Ship it!\")                    # True\nis_approved(\"ship it\")                     # True\nis_approved(\"SHIP IT! Great work!\")        # True\n\n# Not approved (contains feedback)\nis_approved(\"Good, but ship it later\")     # False\nis_approved(\"Try shipping this idea\")      # False\nis_approved(\"Needs work. Make it catchy\")  # False\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.workflow.is_approved","title":"is_approved","text":"<pre><code>is_approved(reviewer_response)\n</code></pre> <p>Check if reviewer approved the slogan.</p> <p>Looks for \"SHIP IT!\" as a standalone phrase or at the start of response. More strict than just searching anywhere to avoid false positives.</p> <p>Parameters:</p> Name Type Description Default <code>reviewer_response</code> <code>str</code> <p>The reviewer's feedback text</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if approved, False otherwise</p> Source code in <code>src/orchestration/workflow.py</code> <pre><code>def is_approved(reviewer_response: str) -&gt; bool:\n    \"\"\"Check if reviewer approved the slogan.\n\n    Looks for \"SHIP IT!\" as a standalone phrase or at the start of response.\n    More strict than just searching anywhere to avoid false positives.\n\n    Args:\n        reviewer_response: The reviewer's feedback text\n\n    Returns:\n        True if approved, False otherwise\n    \"\"\"\n    if not reviewer_response:\n        return False\n\n    # Normalize response: strip whitespace and convert to lowercase\n    normalized = reviewer_response.strip().lower()\n\n    # Check if response STARTS with \"ship it\" (most reliable indicator)\n    if normalized.startswith(\"ship it\"):\n        return True\n\n    # Check if \"ship it!\" appears on its own line (strong indicator)\n    lines = [line.strip() for line in normalized.split(\"\\n\")]\n    for line in lines:\n        # Match lines that are exactly \"ship it\" or \"ship it!\" with optional punctuation\n        if re.match(r\"^ship\\s+it[!.]*$\", line):\n            return True\n\n    return False\n</code></pre>"},{"location":"api-reference/orchestration/#should_continue_iteration","title":"should_continue_iteration","text":"<p>Purpose: Determine if the iteration loop should continue.</p> <p>Logic:</p> <ul> <li>Returns <code>True</code> if: Not approved AND turns &lt; max_turns</li> <li>Returns <code>False</code> if: Approved OR reached max_turns</li> </ul> <p>Usage:</p> <pre><code>from src.orchestration.workflow import should_continue_iteration\nfrom src.orchestration.models import IterationSession\nfrom src.config import get_ollama_config\n\nsession = IterationSession(user_input=\"test\", model_name=\"mistral:latest\")\nconfig = get_ollama_config()\n\n# First iteration\nshould_continue_iteration(session, config)  # True (no turns yet)\n\n# Add approved turn\nsession.add_turn(\"Great Slogan!\", \"SHIP IT!\", approved=True)\nshould_continue_iteration(session, config)  # False (approved)\n\n# Reached max turns\nfor i in range(config.max_turns):\n    session.add_turn(f\"Slogan {i}\", \"Try again\", approved=False)\nshould_continue_iteration(session, config)  # False (max turns)\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.workflow.should_continue_iteration","title":"should_continue_iteration","text":"<pre><code>should_continue_iteration(session, config)\n</code></pre> <p>Determine if iteration should continue.</p> <p>Iteration continues if: - We haven't reached max turns - The latest slogan hasn't been approved</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>IterationSession</code> <p>Current iteration session</p> required <code>config</code> <code>OllamaConfig</code> <p>Ollama configuration (for max_turns)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if iteration should continue, False to stop</p> Source code in <code>src/orchestration/workflow.py</code> <pre><code>def should_continue_iteration(\n    session: IterationSession,\n    config: OllamaConfig,\n) -&gt; bool:\n    \"\"\"Determine if iteration should continue.\n\n    Iteration continues if:\n    - We haven't reached max turns\n    - The latest slogan hasn't been approved\n\n    Args:\n        session: Current iteration session\n        config: Ollama configuration (for max_turns)\n\n    Returns:\n        True if iteration should continue, False to stop\n    \"\"\"\n    if not session.turns:\n        return True\n\n    latest_turn = session.turns[-1]\n\n    # Stop if approved\n    if latest_turn.approved:\n        return False\n\n    # Stop if reached max turns\n    if len(session.turns) &gt;= config.max_turns:\n        return False\n\n    return True\n</code></pre>"},{"location":"api-reference/orchestration/#data-models","title":"Data Models","text":""},{"location":"api-reference/orchestration/#iterationsession","title":"IterationSession","text":"<p>Purpose: Represents a complete slogan generation session with all turns.</p> <p>Key Fields:</p> Field Type Description <code>user_input</code> str Original product/service description <code>model_name</code> str Ollama model used <code>turns</code> list[Turn] All iteration turns <code>final_slogan</code> str | None Final approved or best slogan <code>completed</code> bool Session completion status <code>completion_reason</code> CompletionReason | None Why session ended <code>started_at</code> datetime Session start timestamp <code>completed_at</code> datetime | None Session end timestamp <p>Methods:</p> <ul> <li><code>add_turn(slogan, feedback, approved)</code>: Add new turn to session</li> <li><code>complete(reason)</code>: Mark session as complete</li> </ul> <p>Usage Example:</p> <pre><code>from src.orchestration.models import IterationSession, CompletionReason\n\n# Create session\nsession = IterationSession(\n    user_input=\"coffee shop\",\n    model_name=\"mistral:latest\"\n)\n\n# Add turns\nsession.add_turn(\n    slogan=\"Coffee Perfection\",\n    feedback=\"Too generic, add emotion\",\n    approved=False\n)\n\nsession.add_turn(\n    slogan=\"\u2615 Brew Happiness Daily\",\n    feedback=\"SHIP IT!\",\n    approved=True\n)\n\n# Complete session\nsession.complete(CompletionReason.APPROVED)\n\n# Access results\nprint(f\"Final: {session.final_slogan}\")          # \"\u2615 Brew Happiness Daily\"\nprint(f\"Turns: {len(session.turns)}\")            # 2\nprint(f\"Duration: {session.completed_at - session.started_at}\")\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.models.IterationSession","title":"IterationSession","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete iteration session with all turns.</p>"},{"location":"api-reference/orchestration/#src.orchestration.models.IterationSession.add_turn","title":"add_turn","text":"<pre><code>add_turn(slogan, feedback=None, approved=False)\n</code></pre> <p>Add a new turn to the session.</p> Source code in <code>src/orchestration/models.py</code> <pre><code>def add_turn(self, slogan: str, feedback: str | None = None, approved: bool = False) -&gt; Turn:\n    \"\"\"Add a new turn to the session.\"\"\"\n    turn = Turn(\n        turn_number=len(self.turns) + 1,\n        slogan=slogan,\n        feedback=feedback,\n        approved=approved,\n    )\n    self.turns.append(turn)\n    return turn\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.models.IterationSession.complete","title":"complete","text":"<pre><code>complete(reason)\n</code></pre> <p>Mark session as complete.</p> Source code in <code>src/orchestration/models.py</code> <pre><code>def complete(self, reason: CompletionReason) -&gt; None:\n    \"\"\"Mark session as complete.\"\"\"\n    self.completed = True\n    self.completion_reason = reason\n    self.completed_at = datetime.now()\n    self.final_slogan = self.turns[-1].slogan if self.turns else None\n</code></pre>"},{"location":"api-reference/orchestration/#turn","title":"Turn","text":"<p>Purpose: Represents one iteration turn in the Writer-Reviewer cycle.</p> <p>Key Fields:</p> Field Type Constraints Description <code>turn_number</code> int 1-10 Turn sequence number <code>slogan</code> str 1-500 chars Generated slogan <code>feedback</code> str | None max 1000 chars Reviewer feedback <code>approved</code> bool Approval status <code>timestamp</code> datetime Turn creation time <p>Validation:</p> <ul> <li><code>slogan</code>: Must be 1-500 characters (enforces conciseness)</li> <li><code>feedback</code>: Maximum 1000 characters</li> <li><code>turn_number</code>: Must be 1-10</li> </ul> <p>Usage Example:</p> <pre><code>from src.orchestration.models import Turn\n\nturn = Turn(\n    turn_number=1,\n    slogan=\"Eco-Smart Hydration\",\n    feedback=\"Good start, but emphasize sustainability more\",\n    approved=False\n)\n\nprint(f\"Turn {turn.turn_number}: {turn.slogan}\")\nprint(f\"Status: {'Approved' if turn.approved else 'Needs revision'}\")\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.models.Turn","title":"Turn","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents one iteration turn in the Writer-Reviewer cycle.</p>"},{"location":"api-reference/orchestration/#completionreason","title":"CompletionReason","text":"<p>Purpose: Enumeration of reasons why a session completed.</p> <p>Values:</p> Value Description <code>APPROVED</code> Reviewer approved with \"SHIP IT!\" <code>MAX_TURNS</code> Reached maximum turn limit <code>ERROR</code> Error occurred during workflow <p>Usage Example:</p> <pre><code>from src.orchestration.models import CompletionReason\n\n# Check completion reason\nif session.completion_reason == CompletionReason.APPROVED:\n    print(\"\u2705 Slogan approved!\")\nelif session.completion_reason == CompletionReason.MAX_TURNS:\n    print(\"\u26a0\ufe0f Max turns reached, using best slogan\")\nelif session.completion_reason == CompletionReason.ERROR:\n    print(\"\u274c Error occurred\")\n</code></pre>"},{"location":"api-reference/orchestration/#src.orchestration.models.CompletionReason","title":"CompletionReason","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Reasons for session completion.</p>"},{"location":"api-reference/orchestration/#agentrole","title":"AgentRole","text":"<p>Purpose: Enumeration of agent roles in the system.</p> <p>Values:</p> <ul> <li><code>WRITER</code>: Writer agent (generates slogans)</li> <li><code>REVIEWER</code>: Reviewer agent (provides feedback)</li> </ul>"},{"location":"api-reference/orchestration/#src.orchestration.models.AgentRole","title":"AgentRole","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Agent roles in the system.</p>"},{"location":"api-reference/orchestration/#workflowmessage","title":"WorkflowMessage","text":"<p>Purpose: Message exchanged between agents (for future extensions).</p> <p>Fields:</p> Field Type Description <code>from_agent</code> AgentRole Sending agent <code>to_agent</code> AgentRole Receiving agent <code>content</code> str Message content <code>turn_number</code> int Associated turn number <code>metadata</code> dict Additional metadata"},{"location":"api-reference/orchestration/#src.orchestration.models.WorkflowMessage","title":"WorkflowMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message exchanged between agents.</p>"},{"location":"api-reference/orchestration/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"api-reference/orchestration/#basic-workflow","title":"Basic Workflow","text":"<pre><code># Simple generation\nsession = await run_slogan_generation(\"coffee shop\")\n\nif session.completion_reason == CompletionReason.APPROVED:\n    print(f\"\u2705 Approved: {session.final_slogan}\")\nelse:\n    print(f\"\u26a0\ufe0f Max turns reached: {session.final_slogan}\")\n</code></pre>"},{"location":"api-reference/orchestration/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Use larger model and more turns for higher quality\nsession = await run_slogan_generation(\n    user_input=\"luxury fashion brand\",\n    model_name=\"llama3.2:latest\",\n    max_turns=10\n)\n</code></pre>"},{"location":"api-reference/orchestration/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    session = await run_slogan_generation(\"test\")\nexcept ValueError as e:\n    print(f\"Invalid input: {e}\")\nexcept RuntimeError as e:\n    print(f\"Workflow error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api-reference/orchestration/#accessing-turn-history","title":"Accessing Turn History","text":"<pre><code>session = await run_slogan_generation(\"AI assistant\")\n\nprint(\"\\n\ud83d\udcca Iteration History:\")\nfor turn in session.turns:\n    status = \"\u2705\" if turn.approved else \"\ud83d\udd04\"\n    print(f\"\\n{status} Turn {turn.turn_number}:\")\n    print(f\"  Slogan: {turn.slogan}\")\n    print(f\"  Feedback: {turn.feedback[:100]}...\")\n    print(f\"  Time: {turn.timestamp.strftime('%H:%M:%S')}\")\n</code></pre>"},{"location":"api-reference/orchestration/#session-metrics","title":"Session Metrics","text":"<pre><code>session = await run_slogan_generation(\"product\")\n\n# Calculate metrics\ntotal_duration = (session.completed_at - session.started_at).total_seconds()\navg_duration_per_turn = total_duration / len(session.turns)\napproval_rate = sum(1 for t in session.turns if t.approved) / len(session.turns)\n\nprint(f\"Total Duration: {total_duration:.1f}s\")\nprint(f\"Avg Per Turn: {avg_duration_per_turn:.1f}s\")\nprint(f\"Turns: {len(session.turns)}/{session.max_turns}\")\nprint(f\"Approval Rate: {approval_rate:.1%}\")\n</code></pre>"},{"location":"api-reference/orchestration/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/orchestration/#input-validation","title":"Input Validation","text":"<p>\u2705 Do: <pre><code>user_input = user_input.strip()\nif not user_input:\n    raise ValueError(\"Input cannot be empty\")\n\nsession = await run_slogan_generation(user_input)\n</code></pre></p> <p>\u274c Don't: <pre><code># Don't pass unvalidated input\nsession = await run_slogan_generation(user_input)  # May raise ValueError\n</code></pre></p>"},{"location":"api-reference/orchestration/#max-turns-configuration","title":"Max Turns Configuration","text":"<p>\u2705 Do: <pre><code># Quick brainstorming\nsession = await run_slogan_generation(input, max_turns=3)\n\n# High quality\nsession = await run_slogan_generation(input, max_turns=7)\n</code></pre></p> <p>\u274c Don't: <pre><code># Don't use 1 turn (no feedback loop)\nsession = await run_slogan_generation(input, max_turns=1)\n\n# Don't use too many turns (slow, diminishing returns)\nsession = await run_slogan_generation(input, max_turns=15)  # Invalid (max 10)\n</code></pre></p>"},{"location":"api-reference/orchestration/#error-handling_1","title":"Error Handling","text":"<p>\u2705 Do: <pre><code>try:\n    session = await run_slogan_generation(input)\n    if session.completion_reason == CompletionReason.ERROR:\n        # Handle gracefully\n        logger.error(\"Workflow failed\")\nexcept RuntimeError as e:\n    # Catch and handle workflow errors\n    logger.exception(\"Workflow exception\")\n</code></pre></p>"},{"location":"api-reference/orchestration/#see-also","title":"See Also","text":"<ul> <li>Agents API - Writer and Reviewer agent implementations</li> <li>Config API - Configuration management</li> <li>CLI Usage Guide - Using orchestration via CLI</li> <li>Architecture: Workflow - Workflow design details</li> </ul>"},{"location":"api-reference/rest-api/","title":"REST API Reference","text":"<p>This page documents the FastAPI implementation of the Slogan Writer-Reviewer REST API.</p>"},{"location":"api-reference/rest-api/#overview","title":"Overview","text":"<p>The REST API provides a FastAPI-based HTTP interface for slogan generation with:</p> <ul> <li>Async/Await Architecture: Non-blocking request handling</li> <li>CORS Support: Configurable cross-origin resource sharing</li> <li>Request Logging: Automatic request/response tracking with unique IDs</li> <li>Error Handling: Comprehensive exception handlers with detailed responses</li> <li>OpenAPI Documentation: Auto-generated interactive docs</li> </ul> <p>Base URL: <code>http://localhost:8000</code> (development)</p>"},{"location":"api-reference/rest-api/#fastapi-application","title":"FastAPI Application","text":"<p>Purpose: Main FastAPI application instance with middleware and routes configured.</p>"},{"location":"api-reference/rest-api/#src.api.main.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = FastAPI(title='Slogan Writer-Reviewer API', version='1.0.0', description='Multi-agent slogan generation via Writer-Reviewer collaboration')\n</code></pre>"},{"location":"api-reference/rest-api/#application-configuration","title":"Application Configuration","text":"Property Value Description Title \"Slogan Writer-Reviewer API\" API name Version \"1.0.0\" Semantic version Description \"Multi-agent slogan generation via Writer-Reviewer collaboration\" API purpose"},{"location":"api-reference/rest-api/#middleware-stack","title":"Middleware Stack","text":"<ol> <li>CORS Middleware: Configurable cross-origin requests</li> <li>RequestLoggingMiddleware: Request/response logging with UUIDs</li> </ol>"},{"location":"api-reference/rest-api/#registered-routes","title":"Registered Routes","text":"<ul> <li><code>/</code> - Root endpoint with API information</li> <li><code>/api/v1/health</code> - Health check endpoint</li> <li><code>/api/v1/models</code> - Available models endpoint</li> <li><code>/api/v1/slogans/generate</code> - Slogan generation endpoint</li> </ul>"},{"location":"api-reference/rest-api/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/rest-api/#root-endpoint","title":"Root Endpoint","text":"<p>Purpose: Provide API information and documentation links.</p> <p>HTTP Method: <code>GET /</code></p> <p>Response: RootResponse with API name, version, description, and documentation URLs</p> <p>Example Request: <pre><code>curl http://localhost:8000/\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"name\": \"Slogan Writer-Reviewer API\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Multi-agent slogan generation via Writer-Reviewer collaboration\",\n  \"documentation\": {\n    \"swagger\": \"/docs\",\n    \"redoc\": \"/redoc\",\n    \"openapi\": \"/openapi.json\"\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#src.api.main.get_root","title":"get_root","text":"<pre><code>get_root()\n</code></pre> <p>Get API information and documentation links.</p> Source code in <code>src/api/main.py</code> <pre><code>@app.get(\"/\", response_model=RootResponse, tags=[\"info\"])\ndef get_root() -&gt; RootResponse:\n    \"\"\"Get API information and documentation links.\"\"\"\n    return RootResponse(\n        name=\"Slogan Writer-Reviewer API\",\n        version=\"1.0.0\",\n        description=\"Multi-agent slogan generation via Writer-Reviewer collaboration\",\n        documentation={\n            \"swagger\": \"/docs\",\n            \"redoc\": \"/redoc\",\n            \"openapi\": \"/openapi.json\",\n        },\n    )\n</code></pre>"},{"location":"api-reference/rest-api/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>Purpose: Check API and Ollama dependency health.</p> <p>HTTP Method: <code>GET /api/v1/health</code></p> <p>Response Codes: - <code>200 OK</code>: API and dependencies healthy - <code>503 Service Unavailable</code>: Ollama not connected</p> <p>Response: HealthResponse with status, version, timestamp, and dependency details</p> <p>Example Request: <pre><code>curl http://localhost:8000/api/v1/health\n</code></pre></p> <p>Example Response (Healthy): <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"1.0.0\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"dependencies\": {\n    \"ollama\": {\n      \"connected\": true,\n      \"url\": \"http://localhost:11434\",\n      \"response_time_ms\": 45,\n      \"error\": null\n    }\n  }\n}\n</code></pre></p> <p>Example Response (Degraded): <pre><code>{\n  \"status\": \"degraded\",\n  \"version\": \"1.0.0\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"dependencies\": {\n    \"ollama\": {\n      \"connected\": false,\n      \"url\": \"http://localhost:11434\",\n      \"response_time_ms\": null,\n      \"error\": \"Connection refused\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#src.api.routes.health.get_health","title":"get_health  <code>async</code>","text":"<pre><code>get_health()\n</code></pre> <p>Check API and dependency health status.</p> Source code in <code>src/api/routes/health.py</code> <pre><code>@router.get(\"/health\")\nasync def get_health() -&gt; JSONResponse:\n    \"\"\"Check API and dependency health status.\"\"\"\n    # Check Ollama connectivity\n    ollama_url = \"http://localhost:11434\"\n    ollama_status = await check_ollama_health(ollama_url)\n\n    health_data = HealthResponse(\n        status=\"healthy\" if ollama_status.connected else \"degraded\",\n        version=\"1.0.0\",\n        timestamp=datetime.now(UTC),\n        dependencies={\"ollama\": ollama_status},\n    )\n\n    status_code = (\n        status.HTTP_200_OK\n        if health_data.status == \"healthy\"\n        else status.HTTP_503_SERVICE_UNAVAILABLE\n    )\n\n    return JSONResponse(\n        status_code=status_code,\n        content=health_data.model_dump(mode=\"json\"),\n    )\n</code></pre>"},{"location":"api-reference/rest-api/#models-endpoint","title":"Models Endpoint","text":"<p>Purpose: List available Ollama models.</p> <p>HTTP Method: <code>GET /api/v1/models</code></p> <p>Response: ModelsResponse with list of models, default model, and count</p> <p>Error Responses: - <code>503 Service Unavailable</code>: Ollama not accessible - <code>500 Internal Server Error</code>: Other errors</p> <p>Example Request: <pre><code>curl http://localhost:8000/api/v1/models\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"models\": [\n    {\"name\": \"gemma2:2b\", \"display_name\": \"Gemma2 2B\"},\n    {\"name\": \"mistral:latest\", \"display_name\": \"Mistral Latest\"},\n    {\"name\": \"llama3.2:latest\", \"display_name\": \"Llama3.2 Latest\"}\n  ],\n  \"default_model\": \"mistral:latest\",\n  \"count\": 3\n}\n</code></pre></p> <p>Example Error (Service Unavailable): <pre><code>{\n  \"detail\": {\n    \"error\": \"service_unavailable\",\n    \"message\": \"Failed to connect to Ollama at http://localhost:11434/v1\",\n    \"suggestion\": \"Ensure Ollama is running with 'ollama serve'\"\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#src.api.routes.models.get_models","title":"get_models  <code>async</code>","text":"<pre><code>get_models(config=Depends(get_config))\n</code></pre> <p>Get list of available Ollama models.</p> <p>Returns information about all models available on the Ollama instance, including the default model configured for the application.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OllamaConfig</code> <p>Injected Ollama configuration</p> <code>Depends(get_config)</code> <p>Returns:</p> Type Description <code>ModelsResponse</code> <p>ModelsResponse with list of models, default model, and count</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>503 if Ollama is unavailable</p> <code>HTTPException</code> <p>500 for other errors</p> Source code in <code>src/api/routes/models.py</code> <pre><code>@router.get(\"/models\", response_model=ModelsResponse)\nasync def get_models(config: OllamaConfig = Depends(get_config)) -&gt; ModelsResponse:\n    \"\"\"\n    Get list of available Ollama models.\n\n    Returns information about all models available on the Ollama instance,\n    including the default model configured for the application.\n\n    Args:\n        config: Injected Ollama configuration\n\n    Returns:\n        ModelsResponse with list of models, default model, and count\n\n    Raises:\n        HTTPException: 503 if Ollama is unavailable\n        HTTPException: 500 for other errors\n    \"\"\"\n    try:\n        # Get available models from Ollama API\n        model_names = get_available_models(base_url=config.base_url, timeout=10)\n\n        # Convert to ModelInfo objects with display names\n        models = [\n            ModelInfo(\n                name=name,\n                display_name=name.replace(\":\", \" \").title()\n            )\n            for name in model_names\n        ]\n\n        return ModelsResponse(\n            models=models,\n            default_model=config.model_name,\n            count=len(models)\n        )\n\n    except ConnectionError as e:\n        raise HTTPException(\n            status_code=503,\n            detail={\n                \"error\": \"service_unavailable\",\n                \"message\": str(e),\n                \"suggestion\": \"Ensure Ollama is running with 'ollama serve'\"\n            }\n        ) from e\n\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail={\n                \"error\": \"internal_error\",\n                \"message\": f\"Failed to retrieve models: {str(e)}\"\n            }\n        ) from e\n</code></pre>"},{"location":"api-reference/rest-api/#generate-slogan-endpoint","title":"Generate Slogan Endpoint","text":"<p>Purpose: Generate a slogan via Writer-Reviewer collaboration.</p> <p>HTTP Method: <code>POST /api/v1/slogans/generate</code></p> <p>Request Body: GenerateRequest with input, optional model, max_turns, verbose</p> <p>Response: GenerateResponse with final slogan and metadata</p> <p>Timeout: 600 seconds (10 minutes)</p> <p>Error Responses: - <code>400 Bad Request</code>: Invalid model specified - <code>422 Unprocessable Entity</code>: Validation error - <code>500 Internal Server Error</code>: Generation error - <code>504 Gateway Timeout</code>: Generation timeout (&gt;600s)</p> <p>Example Request (Basic): <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"eco-friendly water bottles\"\n  }'\n</code></pre></p> <p>Example Response (Basic): <pre><code>{\n  \"slogan\": \"Stay Hydrated, Save the Planet\",\n  \"input\": \"eco-friendly water bottles\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 3,\n  \"model_name\": \"mistral:latest\",\n  \"total_duration_seconds\": 12.45,\n  \"average_duration_per_turn\": 4.15,\n  \"turns\": null,\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"request_id\": \"a1b2c3d4-e5f6-7890-ab12-cd34ef567890\"\n}\n</code></pre></p> <p>Example Request (Verbose with Options): <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"AI-powered productivity tools\",\n    \"model\": \"llama3.2:latest\",\n    \"max_turns\": 5,\n    \"verbose\": true\n  }'\n</code></pre></p> <p>Example Response (Verbose): <pre><code>{\n  \"slogan\": \"Work Smarter, Not Harder with AI\",\n  \"input\": \"AI-powered productivity tools\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 2,\n  \"model_name\": \"llama3.2:latest\",\n  \"total_duration_seconds\": 8.73,\n  \"average_duration_per_turn\": 4.37,\n  \"turns\": [\n    {\n      \"turn_number\": 1,\n      \"slogan\": \"Supercharge Your Workflow with AI\",\n      \"feedback\": \"Good start but 'supercharge' is overused...\",\n      \"approved\": false,\n      \"timestamp\": \"2024-01-15T10:30:05Z\"\n    },\n    {\n      \"turn_number\": 2,\n      \"slogan\": \"Work Smarter, Not Harder with AI\",\n      \"feedback\": null,\n      \"approved\": true,\n      \"timestamp\": \"2024-01-15T10:30:09Z\"\n    }\n  ],\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"request_id\": \"b2c3d4e5-f6g7-8901-bc23-de45fg678901\"\n}\n</code></pre></p> <p>Example Error (Validation): <pre><code>{\n  \"detail\": [\n    {\n      \"type\": \"string_too_short\",\n      \"loc\": [\"body\", \"input\"],\n      \"msg\": \"String should have at least 1 character\",\n      \"input\": \"\",\n      \"ctx\": {\"min_length\": 1}\n    }\n  ]\n}\n</code></pre></p> <p>Example Error (Invalid Model): <pre><code>{\n  \"detail\": {\n    \"error\": \"invalid_model\",\n    \"message\": \"Model 'nonexistent:latest' not found\",\n    \"available_models\": [\"gemma2:2b\", \"mistral:latest\", \"llama3.2:latest\"]\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#src.api.routes.generate.generate_slogan","title":"generate_slogan  <code>async</code>","text":"<pre><code>generate_slogan(request_body, request, config=Depends(get_config))\n</code></pre> <p>Generate a slogan through Writer-Reviewer collaboration.</p> <p>This endpoint orchestrates the multi-agent slogan generation workflow, where a Writer agent creates slogans and a Reviewer agent provides feedback until an acceptable slogan is approved or max iterations reached.</p> <p>Parameters:</p> Name Type Description Default <code>request_body</code> <code>GenerateRequest</code> <p>Generation request with input, optional model, max_turns, verbose</p> required <code>request</code> <code>Request</code> <p>FastAPI request object (for accessing request_id from middleware)</p> required <code>config</code> <code>OllamaConfig</code> <p>Injected Ollama configuration</p> <code>Depends(get_config)</code> <p>Returns:</p> Type Description <code>GenerateResponse</code> <p>GenerateResponse with final slogan and metadata</p> <p>Raises:</p> Type Description <code>HTTPException 400</code> <p>Invalid model specified</p> <code>HTTPException 422</code> <p>Validation error (handled by FastAPI)</p> <code>HTTPException 500</code> <p>Generation error</p> <code>HTTPException 504</code> <p>Generation timeout (&gt;600s)</p> Source code in <code>src/api/routes/generate.py</code> <pre><code>@router.post(\"/slogans/generate\", response_model=GenerateResponse)\nasync def generate_slogan(\n    request_body: GenerateRequest,\n    request: Request,\n    config: OllamaConfig = Depends(get_config),\n) -&gt; GenerateResponse:\n    \"\"\"\n    Generate a slogan through Writer-Reviewer collaboration.\n\n    This endpoint orchestrates the multi-agent slogan generation workflow,\n    where a Writer agent creates slogans and a Reviewer agent provides feedback\n    until an acceptable slogan is approved or max iterations reached.\n\n    Args:\n        request_body: Generation request with input, optional model, max_turns, verbose\n        request: FastAPI request object (for accessing request_id from middleware)\n        config: Injected Ollama configuration\n\n    Returns:\n        GenerateResponse with final slogan and metadata\n\n    Raises:\n        HTTPException 400: Invalid model specified\n        HTTPException 422: Validation error (handled by FastAPI)\n        HTTPException 500: Generation error\n        HTTPException 504: Generation timeout (&gt;600s)\n    \"\"\"\n    # Get request ID from middleware (or generate if middleware not active)\n    request_id = getattr(request.state, \"request_id\", str(uuid4()))\n    started_at = datetime.now()\n\n    try:\n        # Validate model if specified\n        if request_body.model:\n            try:\n                available_models = get_available_models(base_url=config.base_url, timeout=10)\n                if request_body.model not in available_models:\n                    models_list = ', '.join(available_models)\n                    error_msg = (\n                        f\"Model '{request_body.model}' not found. \"\n                        f\"Available models: {models_list}\"\n                    )\n                    raise HTTPException(status_code=400, detail=error_msg)\n            except HTTPException:\n                # Re-raise HTTPException (our validation error)\n                raise\n            except (ConnectionError, RuntimeError):\n                # If we can't fetch models, let the generation proceed with the requested model\n                # The generation itself will fail if the model truly doesn't exist\n                pass\n\n        # Run generation with timeout\n        session = await run_generation_async(\n            user_input=request_body.input,\n            model_name=request_body.model,\n            max_turns=request_body.max_turns,\n        )\n\n        # Convert to API response\n        response = convert_session_to_response(\n            session=session,\n            original_input=request_body.input,\n            verbose=request_body.verbose,\n            request_id=request_id,\n            started_at=started_at,\n        )\n\n        return response\n\n    except HTTPException:\n        # Re-raise HTTPException from validation or other explicit raises\n        raise\n\n    except TimeoutError as e:\n        raise HTTPException(\n            status_code=504,\n            detail={\n                \"error\": \"generation_timeout\",\n                \"message\": str(e),\n                \"request_id\": request_id,\n            }\n        ) from e\n\n    except ConnectionError as e:\n        raise HTTPException(\n            status_code=503,\n            detail={\n                \"error\": \"service_unavailable\",\n                \"message\": str(e),\n                \"request_id\": request_id,\n            }\n        ) from e\n\n    except ValueError as e:\n        raise HTTPException(\n            status_code=400,\n            detail={\n                \"error\": \"invalid_request\",\n                \"message\": str(e),\n                \"request_id\": request_id,\n            }\n        ) from e\n\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail={\n                \"error\": \"internal_error\",\n                \"message\": f\"Slogan generation failed: {str(e)}\",\n                \"request_id\": request_id,\n            }\n        ) from e\n</code></pre>"},{"location":"api-reference/rest-api/#requestresponse-schemas","title":"Request/Response Schemas","text":""},{"location":"api-reference/rest-api/#request-models","title":"Request Models","text":""},{"location":"api-reference/rest-api/#generaterequest","title":"GenerateRequest","text":"<p>Fields:</p> Field Type Required Constraints Description <code>input</code> str \u2705 1-500 chars Product/topic description <code>model</code> str | None \u274c Ollama model (uses default if omitted) <code>max_turns</code> int | None \u274c 1-10 Max iterations (uses config default if omitted) <code>verbose</code> bool \u274c Include turn details (default: false)"},{"location":"api-reference/rest-api/#src.api.schemas.requests.GenerateRequest","title":"GenerateRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for slogan generation endpoint.</p>"},{"location":"api-reference/rest-api/#response-models","title":"Response Models","text":""},{"location":"api-reference/rest-api/#rootresponse","title":"RootResponse","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.RootResponse","title":"RootResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for root endpoint.</p>"},{"location":"api-reference/rest-api/#healthresponse","title":"HealthResponse","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.HealthResponse","title":"HealthResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for health check.</p>"},{"location":"api-reference/rest-api/#dependencystatus","title":"DependencyStatus","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.DependencyStatus","title":"DependencyStatus","text":"<p>               Bases: <code>BaseModel</code></p> <p>Status of a single dependency.</p>"},{"location":"api-reference/rest-api/#modelsresponse","title":"ModelsResponse","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.ModelsResponse","title":"ModelsResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for models endpoint.</p>"},{"location":"api-reference/rest-api/#modelinfo","title":"ModelInfo","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.ModelInfo","title":"ModelInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about an available model.</p>"},{"location":"api-reference/rest-api/#generateresponse","title":"GenerateResponse","text":"<p>Fields:</p> Field Type Description <code>slogan</code> str Final approved slogan <code>input</code> str Original user input <code>completion_reason</code> \"approved\" | \"max_turns\" | \"error\" Why generation stopped <code>turn_count</code> int Number of iterations <code>model_name</code> str Model used <code>total_duration_seconds</code> float Total time <code>average_duration_per_turn</code> float Average time per turn <code>turns</code> list[TurnDetail] | None Turn history (verbose only) <code>created_at</code> datetime Request timestamp <code>request_id</code> UUID | None Request identifier"},{"location":"api-reference/rest-api/#src.api.schemas.responses.GenerateResponse","title":"GenerateResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for slogan generation endpoint.</p>"},{"location":"api-reference/rest-api/#turndetail","title":"TurnDetail","text":""},{"location":"api-reference/rest-api/#src.api.schemas.responses.TurnDetail","title":"TurnDetail","text":"<p>               Bases: <code>BaseModel</code></p> <p>Details of a single iteration turn (for verbose mode).</p>"},{"location":"api-reference/rest-api/#middleware","title":"Middleware","text":""},{"location":"api-reference/rest-api/#requestloggingmiddleware","title":"RequestLoggingMiddleware","text":"<p>Purpose: Log all HTTP requests/responses with unique tracking IDs.</p> <p>Features:</p> <ul> <li>Generates <code>X-Request-ID</code> header (UUID v4)</li> <li>Logs request method, path, query params, client IP</li> <li>Logs response status code, duration</li> <li>Adds request ID to response headers</li> <li>Stores request ID in <code>request.state</code> for endpoint access</li> </ul> <p>Log Example: <pre><code>INFO Request started: POST /api/v1/slogans/generate\n  request_id=a1b2c3d4-e5f6-7890-ab12-cd34ef567890\n  method=POST\n  path=/api/v1/slogans/generate\n  query_params=\n  client_host=127.0.0.1\n\nINFO Request completed: POST /api/v1/slogans/generate - 200\n  request_id=a1b2c3d4-e5f6-7890-ab12-cd34ef567890\n  method=POST\n  path=/api/v1/slogans/generate\n  status_code=200\n  duration_ms=12450.67\n</code></pre></p>"},{"location":"api-reference/rest-api/#src.api.middleware.RequestLoggingMiddleware","title":"RequestLoggingMiddleware","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Middleware for logging HTTP requests and responses.</p> <p>Generates a unique X-Request-ID for each request and logs: - Request method, path, and query parameters - Response status code and duration - Request ID for tracking and debugging</p>"},{"location":"api-reference/rest-api/#src.api.middleware.RequestLoggingMiddleware.dispatch","title":"dispatch  <code>async</code>","text":"<pre><code>dispatch(request, call_next)\n</code></pre> <p>Process request and add logging.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Incoming HTTP request</p> required <code>call_next</code> <code>Callable[[Request], Awaitable[Response]]</code> <p>Next middleware/endpoint handler</p> required <p>Returns:</p> Type Description <code>Response</code> <p>Response with X-Request-ID header</p> Source code in <code>src/api/middleware.py</code> <pre><code>async def dispatch(\n    self, request: Request, call_next: Callable[[Request], Awaitable[Response]]\n) -&gt; Response:\n    \"\"\"\n    Process request and add logging.\n\n    Args:\n        request: Incoming HTTP request\n        call_next: Next middleware/endpoint handler\n\n    Returns:\n        Response with X-Request-ID header\n    \"\"\"\n    # Generate or extract request ID\n    request_id = request.headers.get(\"X-Request-ID\") or str(uuid4())\n\n    # Store request ID in request state for access in endpoints\n    request.state.request_id = request_id\n\n    # Log request\n    start_time = time.time()\n    logger.info(\n        f\"Request started: {request.method} {request.url.path}\",\n        extra={\n            \"request_id\": request_id,\n            \"method\": request.method,\n            \"path\": request.url.path,\n            \"query_params\": str(request.query_params),\n            \"client_host\": request.client.host if request.client else None,\n        }\n    )\n\n    # Process request\n    try:\n        response = await call_next(request)\n\n        # Calculate duration\n        duration_ms = (time.time() - start_time) * 1000\n\n        # Log response\n        logger.info(\n            f\"Request completed: {request.method} {request.url.path} - {response.status_code}\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"status_code\": response.status_code,\n                \"duration_ms\": round(duration_ms, 2),\n            }\n        )\n\n        # Add request ID to response headers\n        response.headers[\"X-Request-ID\"] = request_id\n\n        return response\n\n    except Exception as e:\n        # Log error\n        duration_ms = (time.time() - start_time) * 1000\n        logger.error(\n            f\"Request failed: {request.method} {request.url.path} - {str(e)}\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"error\": str(e),\n                \"duration_ms\": round(duration_ms, 2),\n            },\n            exc_info=True\n        )\n        raise\n</code></pre>"},{"location":"api-reference/rest-api/#cors-configuration","title":"CORS Configuration","text":"<p>The API supports CORS (Cross-Origin Resource Sharing) configuration via environment variable:</p> <p>Environment Variable: <code>API_CORS_ORIGINS</code></p> <p>Format: Comma-separated list of allowed origins</p> <p>Default: <code>*</code> (all origins, development only)</p> <p>Examples:</p> <pre><code># Allow all origins (development)\nAPI_CORS_ORIGINS=\"*\"\n\n# Allow specific origins (production)\nAPI_CORS_ORIGINS=\"https://app.example.com,https://admin.example.com\"\n\n# Allow localhost variants (development)\nAPI_CORS_ORIGINS=\"http://localhost:3000,http://127.0.0.1:3000\"\n</code></pre> <p>CORS Settings:</p> Setting Value Description <code>allow_credentials</code> <code>true</code> Allow cookies/auth headers <code>allow_methods</code> <code>[\"*\"]</code> All HTTP methods allowed <code>allow_headers</code> <code>[\"*\"]</code> All headers allowed <p>Implementation:</p> <ul> <li>When <code>API_CORS_ORIGINS=\"*\"</code>: Uses <code>allow_origin_regex=\"https?://.*\"</code> (regex match for all)</li> <li>Otherwise: Uses <code>allow_origins=[...]</code> (specific origin list)</li> </ul>"},{"location":"api-reference/rest-api/#error-handling","title":"Error Handling","text":"<p>The API has comprehensive error handlers for different exception types:</p>"},{"location":"api-reference/rest-api/#validation-errors-422","title":"Validation Errors (422)","text":"<p>Handled by <code>validation_exception_handler</code> for Pydantic validation failures:</p> <pre><code>{\n  \"detail\": [\n    {\n      \"type\": \"string_too_short\",\n      \"loc\": [\"body\", \"input\"],\n      \"msg\": \"String should have at least 1 character\",\n      \"input\": \"\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api-reference/rest-api/#http-exceptions-4xx-5xx","title":"HTTP Exceptions (4xx, 5xx)","text":"<p>Handled by <code>http_exception_handler</code> for FastAPI/Starlette HTTP exceptions:</p> <pre><code>{\n  \"detail\": {\n    \"error\": \"service_unavailable\",\n    \"message\": \"Ollama not accessible\",\n    \"suggestion\": \"Ensure Ollama is running\"\n  }\n}\n</code></pre>"},{"location":"api-reference/rest-api/#unhandled-exceptions-500","title":"Unhandled Exceptions (500)","text":"<p>Handled by <code>unhandled_exception_handler</code> for unexpected errors:</p> <pre><code>{\n  \"detail\": {\n    \"error\": \"internal_server_error\",\n    \"message\": \"An unexpected error occurred\"\n  }\n}\n</code></pre>"},{"location":"api-reference/rest-api/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api-reference/rest-api/#python-client-httpx","title":"Python Client (httpx)","text":"<pre><code>import httpx\n\n# Async client\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        \"http://localhost:8000/api/v1/slogans/generate\",\n        json={\"input\": \"smart home devices\", \"verbose\": True}\n    )\n    data = response.json()\n    print(data[\"slogan\"])\n</code></pre>"},{"location":"api-reference/rest-api/#javascripttypescript-fetch","title":"JavaScript/TypeScript (fetch)","text":"<pre><code>const response = await fetch('http://localhost:8000/api/v1/slogans/generate', {\n  method: 'POST',\n  headers: {'Content-Type': 'application/json'},\n  body: JSON.stringify({\n    input: 'cloud storage solutions',\n    model: 'mistral:latest',\n    max_turns: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.slogan);\n</code></pre>"},{"location":"api-reference/rest-api/#curl","title":"cURL","text":"<pre><code># Health check\ncurl http://localhost:8000/api/v1/health\n\n# List models\ncurl http://localhost:8000/api/v1/models\n\n# Generate slogan\ncurl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"sustainable fashion brand\"}'\n\n# Generate with custom request ID\ncurl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Request-ID: my-custom-id-123\" \\\n  -d '{\"input\": \"coffee subscription service\", \"verbose\": true}'\n</code></pre>"},{"location":"api-reference/rest-api/#running-the-api","title":"Running the API","text":""},{"location":"api-reference/rest-api/#development-server","title":"Development Server","text":"<pre><code># With uv (recommended)\nuv run fastapi dev src/api/main.py\n\n# With uvicorn directly\nuvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"api-reference/rest-api/#production-server","title":"Production Server","text":"<pre><code># With gunicorn + uvicorn workers\ngunicorn src.api.main:app \\\n  --workers 4 \\\n  --worker-class uvicorn.workers.UvicornWorker \\\n  --bind 0.0.0.0:8000\n\n# With uvicorn\nuvicorn src.api.main:app \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --workers 4\n</code></pre>"},{"location":"api-reference/rest-api/#environment-configuration","title":"Environment Configuration","text":"<pre><code># .env\nOLLAMA_BASE_URL=http://localhost:11434/v1\nOLLAMA_MODEL_NAME=mistral:latest\nOLLAMA_MAX_TURNS=5\n\nAPI_CORS_ORIGINS=https://app.example.com\nAPI_LOG_LEVEL=INFO\nAPI_GENERATION_TIMEOUT=600\nAPI_REQUEST_TIMEOUT=30\n</code></pre>"},{"location":"api-reference/rest-api/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/rest-api/#use-verbose-mode-sparingly","title":"Use Verbose Mode Sparingly","text":"<p>\u2705 Do: <pre><code># Only use verbose for debugging/analysis\nresponse = client.post(\"/api/v1/slogans/generate\", \n    json={\"input\": \"test\", \"verbose\": True})\n</code></pre></p> <p>\u274c Don't: <pre><code># Don't use verbose in production for all requests (larger responses)\nfor item in items:\n    response = client.post(..., json={\"input\": item, \"verbose\": True})\n</code></pre></p>"},{"location":"api-reference/rest-api/#handle-timeouts","title":"Handle Timeouts","text":"<p>\u2705 Do: <pre><code>try:\n    response = await client.post(\n        url, \n        json={\"input\": text},\n        timeout=620.0  # Slightly &gt; 600s server timeout\n    )\nexcept httpx.TimeoutException:\n    print(\"Generation took too long\")\n</code></pre></p>"},{"location":"api-reference/rest-api/#check-health-before-operations","title":"Check Health Before Operations","text":"<p>\u2705 Do: <pre><code># Verify API health before batch operations\nhealth = await client.get(\"/api/v1/health\")\nif health.json()[\"status\"] != \"healthy\":\n    print(\"API not ready\")\n    return\n</code></pre></p>"},{"location":"api-reference/rest-api/#use-request-ids-for-tracking","title":"Use Request IDs for Tracking","text":"<p>\u2705 Do: <pre><code># Provide custom request ID for tracking\nrequest_id = str(uuid.uuid4())\nresponse = await client.post(\n    url,\n    json={\"input\": text},\n    headers={\"X-Request-ID\": request_id}\n)\n# Log or store request_id for debugging\n</code></pre></p>"},{"location":"api-reference/rest-api/#see-also","title":"See Also","text":"<ul> <li>API Usage Guide - User guide for REST API</li> <li>Configuration API Reference - Configuration management</li> <li>Orchestration API Reference - Workflow implementation</li> <li>Development Guide - API development setup</li> </ul>"},{"location":"architecture/agents/","title":"Agent Architecture","text":"<p>This document describes the agent design patterns, communication protocols, and implementation details for the Writer-Reviewer multi-agent system.</p>"},{"location":"architecture/agents/#overview","title":"Overview","text":"<p>The system implements a two-agent collaborative pattern where specialized agents work together to generate high-quality slogans through iterative refinement.</p>"},{"location":"architecture/agents/#agent-roles","title":"Agent Roles","text":"Agent Role Input Output Writer Creative generation Product description + feedback Slogan proposal Reviewer Quality evaluation Slogan to review Approval or feedback"},{"location":"architecture/agents/#design-philosophy","title":"Design Philosophy","text":"<ul> <li>Single Responsibility: Each agent has one clear purpose</li> <li>Specialized Prompts: System prompts optimized for specific roles</li> <li>Loose Coupling: Agents don't directly communicate</li> <li>Stateless: Agents don't maintain state between calls</li> <li>Extensible: Easy to add new agent types</li> </ul>"},{"location":"architecture/agents/#agent-communication-protocol","title":"Agent Communication Protocol","text":""},{"location":"architecture/agents/#message-flow","title":"Message Flow","text":"<pre><code>User Input\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Orchestration Layer               \u2502\n\u2502     (Mediates all communication)          \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502                                  \u2502\n    \u2502 1. Generate Request              \u2502\n    \u25bc                                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502  Writer  \u2502                           \u2502\n\u2502  Agent   \u2502                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n     \u2502 2. Slogan                       \u2502\n     \u25bc                                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502 Orchestration  \u2502                     \u2502\n\u2502    (stores)    \u2502                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n     \u2502 3. Review Request               \u2502\n     \u25bc                                 \u2502\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502   Reviewer   \u2502\n                              \u2502    Agent     \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502 4. Feedback/Approval\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Orchestration  \u2502\n\u2502  (decision)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502\n     \u251c\u2500 If \"SHIP IT!\" \u2192 Complete\n     \u2502\n     \u2514\u2500 If feedback \u2192 Go to step 1 (with feedback)\n</code></pre>"},{"location":"architecture/agents/#communication-objects","title":"Communication Objects","text":""},{"location":"architecture/agents/#workflowmessage","title":"WorkflowMessage","text":"<pre><code>@dataclass\nclass WorkflowMessage:\n    \"\"\"Message passed between orchestration and agents.\"\"\"\n    role: AgentRole  # WRITER or REVIEWER\n    content: str     # Slogan or feedback\n    metadata: dict   # Additional context\n</code></pre>"},{"location":"architecture/agents/#turn","title":"Turn","text":"<pre><code>class Turn(BaseModel):\n    \"\"\"Single Writer-Reviewer exchange.\"\"\"\n    turn_number: int\n    slogan: str\n    feedback: str | None\n    approved: bool\n    timestamp: datetime\n</code></pre>"},{"location":"architecture/agents/#writer-agent","title":"Writer Agent","text":""},{"location":"architecture/agents/#purpose","title":"Purpose","text":"<p>Generate creative, memorable slogans based on product descriptions and iterative feedback.</p>"},{"location":"architecture/agents/#system-prompt","title":"System Prompt","text":"<pre><code>WRITER_SYSTEM_PROMPT = \"\"\"\nYou are a creative slogan writer. Your job is to create short, \nmemorable slogans for products or services.\n\nGuidelines:\n- Keep slogans between 2-7 words\n- Focus on emotional appeal and memorability\n- Use active voice and strong verbs\n- Avoid clich\u00e9s and overused phrases\n- Be specific to the product/service\n\nIf you receive feedback, use it to improve your next slogan.\nOnly provide the slogan text, no explanations.\n\"\"\"\n</code></pre>"},{"location":"architecture/agents/#behavior","title":"Behavior","text":""},{"location":"architecture/agents/#initial-generation-turn-1","title":"Initial Generation (Turn 1)","text":"<p>Input: <pre><code>{\n    \"user_input\": \"eco-friendly water bottles\",\n    \"previous_feedback\": None\n}\n</code></pre></p> <p>Processing: 1. Analyze product description 2. Identify key attributes (eco-friendly, water bottles) 3. Generate memorable slogan 4. Focus on emotional appeal</p> <p>Output: <pre><code>\"Hydrate Green, Live Clean\"\n</code></pre></p>"},{"location":"architecture/agents/#refinement-turn-2","title":"Refinement (Turn 2+)","text":"<p>Input: <pre><code>{\n    \"user_input\": \"eco-friendly water bottles\",\n    \"previous_feedback\": \"Good start but 'live clean' is vague. \n                         Be more specific about environmental impact.\"\n}\n</code></pre></p> <p>Processing: 1. Analyze feedback points 2. Identify weaknesses (vagueness) 3. Maintain strengths (rhythm, brevity) 4. Generate improved version</p> <p>Output: <pre><code>\"Hydrate Green, Save Our Seas\"\n</code></pre></p>"},{"location":"architecture/agents/#implementation","title":"Implementation","text":"<pre><code>async def create_writer_agent(config: OllamaConfig) -&gt; Agent:\n    \"\"\"\n    Create Writer agent for slogan generation.\n\n    Args:\n        config: Ollama configuration\n\n    Returns:\n        Configured Writer agent\n    \"\"\"\n    return Agent(\n        name=\"writer\",\n        model=config.model_name,\n        system_message=WRITER_SYSTEM_PROMPT,\n        temperature=config.temperature,\n        max_tokens=config.max_tokens,\n    )\n</code></pre>"},{"location":"architecture/agents/#prompt-engineering","title":"Prompt Engineering","text":"<p>Key Techniques:</p> <ol> <li>Clear Guidelines: Explicit instructions (2-7 words, avoid clich\u00e9s)</li> <li>Examples (implicit): Training data includes good examples</li> <li>Constraints: Word count, tone, style</li> <li>Feedback Integration: Instruction to use reviewer feedback</li> <li>Format Specification: \"Only provide slogan text\"</li> </ol> <p>Temperature Settings: - Low (0.3-0.5): Conservative, focused (good for professional slogans) - Medium (0.7): Balanced creativity (default, recommended) - High (0.9-1.2): More creative, diverse (good for brainstorming)</p>"},{"location":"architecture/agents/#reviewer-agent","title":"Reviewer Agent","text":""},{"location":"architecture/agents/#purpose_1","title":"Purpose","text":"<p>Evaluate slogans against quality criteria and provide constructive feedback or approval.</p>"},{"location":"architecture/agents/#system-prompt_1","title":"System Prompt","text":"<pre><code>REVIEWER_SYSTEM_PROMPT = \"\"\"\nYou are a professional slogan reviewer. Evaluate slogans based on:\n\n1. Memorability - Is it catchy and easy to remember?\n2. Clarity - Is the message clear?\n3. Conciseness - Is it brief (2-7 words)?\n4. Emotional Appeal - Does it evoke the right feelings?\n5. Uniqueness - Is it original and distinctive?\n\nIf the slogan meets all criteria excellently, respond with exactly:\n\"SHIP IT!\"\n\nOtherwise, provide specific, constructive feedback on what needs \nimprovement. Be direct but helpful.\n\"\"\"\n</code></pre>"},{"location":"architecture/agents/#behavior_1","title":"Behavior","text":""},{"location":"architecture/agents/#evaluation-process","title":"Evaluation Process","text":"<p>Input: <pre><code>{\n    \"slogan\": \"Hydrate Green, Live Clean\"\n}\n</code></pre></p> <p>Processing: 1. Memorability Check: Is it catchy? \u2705 Yes (rhyming, rhythm) 2. Clarity Check: Is message clear? \u26a0\ufe0f \"Live clean\" is vague 3. Conciseness Check: Word count? \u2705 4 words 4. Emotional Appeal: Right feelings? \u2705 Positive, eco-conscious 5. Uniqueness Check: Original? \u2705 Not clich\u00e9d</p> <p>Decision: Not all criteria met \u2192 Provide feedback</p> <p>Output: <pre><code>Good start! The rhythm and rhyme work well. However, \n\"live clean\" is vague. Be more specific about the \nenvironmental impact - what exactly are we protecting?\n</code></pre></p>"},{"location":"architecture/agents/#approval","title":"Approval","text":"<p>Input: <pre><code>{\n    \"slogan\": \"Hydrate Green, Save Our Seas\"\n}\n</code></pre></p> <p>Processing: 1. Memorability: \u2705 Excellent (rhyming, clear rhythm) 2. Clarity: \u2705 Clear environmental message 3. Conciseness: \u2705 5 words (optimal) 4. Emotional Appeal: \u2705 Strong (ocean conservation) 5. Uniqueness: \u2705 Original phrasing</p> <p>Decision: All criteria met \u2192 Approve</p> <p>Output: <pre><code>SHIP IT!\n</code></pre></p>"},{"location":"architecture/agents/#implementation_1","title":"Implementation","text":"<pre><code>async def create_reviewer_agent(config: OllamaConfig) -&gt; Agent:\n    \"\"\"\n    Create Reviewer agent for slogan evaluation.\n\n    Args:\n        config: Ollama configuration\n\n    Returns:\n        Configured Reviewer agent\n    \"\"\"\n    return Agent(\n        name=\"reviewer\",\n        model=config.model_name,\n        system_message=REVIEWER_SYSTEM_PROMPT,\n        temperature=0.3,  # Lower temperature for consistency\n        max_tokens=config.max_tokens,\n    )\n</code></pre>"},{"location":"architecture/agents/#approval-detection","title":"Approval Detection","text":"<pre><code>def is_approved(feedback: str) -&gt; bool:\n    \"\"\"\n    Check if reviewer approved the slogan.\n\n    Args:\n        feedback: Reviewer's response\n\n    Returns:\n        True if approved (contains \"SHIP IT!\")\n    \"\"\"\n    normalized = feedback.upper().strip()\n    return bool(re.search(r'\\bSHIP IT!?\\b', normalized))\n</code></pre> <p>Detection Strategy: - Case-insensitive matching - Handles variations: \"SHIP IT\", \"Ship it!\", \"ship it\" - Word boundary check (avoids false positives)</p>"},{"location":"architecture/agents/#agent-coordination","title":"Agent Coordination","text":""},{"location":"architecture/agents/#orchestration-layer-responsibilities","title":"Orchestration Layer Responsibilities","text":"<ol> <li>Agent Lifecycle:</li> <li>Create agents with proper configuration</li> <li>Manage agent instances</li> <li> <p>Clean up resources</p> </li> <li> <p>Message Routing:</p> </li> <li>Route user input to Writer</li> <li>Pass slogan to Reviewer</li> <li> <p>Return feedback to Writer</p> </li> <li> <p>State Management:</p> </li> <li>Track iteration count</li> <li>Store turn history</li> <li> <p>Maintain session state</p> </li> <li> <p>Decision Logic:</p> </li> <li>Check approval status</li> <li>Determine if more iterations needed</li> <li>Complete session</li> </ol>"},{"location":"architecture/agents/#iteration-loop","title":"Iteration Loop","text":"<pre><code>async def run_slogan_generation(\n    user_input: str,\n    model_name: str | None = None,\n    max_turns: int | None = None\n) -&gt; IterationSession:\n    \"\"\"\n    Main orchestration loop.\n\n    Workflow:\n    1. Initialize session\n    2. Create agents\n    3. Loop until approved or max turns:\n       a. Writer generates slogan\n       b. Reviewer evaluates\n       c. Check approval\n       d. If not approved, provide feedback to writer\n    4. Complete session\n    \"\"\"\n    # Initialize\n    config = get_ollama_config()\n    session = IterationSession(user_input=user_input, ...)\n    writer = await create_writer_agent(config)\n    reviewer = await create_reviewer_agent(config)\n\n    feedback = None\n\n    # Iteration loop\n    for turn_num in range(1, max_turns + 1):\n        # Writer generates\n        slogan = await writer.generate(user_input, feedback)\n\n        # Reviewer evaluates\n        review = await reviewer.evaluate(slogan)\n\n        # Check approval\n        approved = is_approved(review)\n\n        # Store turn\n        session.add_turn(Turn(\n            turn_number=turn_num,\n            slogan=slogan,\n            feedback=None if approved else review,\n            approved=approved\n        ))\n\n        # Exit if approved\n        if approved:\n            session.complete(CompletionReason.APPROVED)\n            return session\n\n        # Prepare feedback for next iteration\n        feedback = review\n\n    # Max turns reached\n    session.complete(CompletionReason.MAX_TURNS)\n    return session\n</code></pre>"},{"location":"architecture/agents/#agent-best-practices","title":"Agent Best Practices","text":""},{"location":"architecture/agents/#system-prompt-design","title":"System Prompt Design","text":""},{"location":"architecture/agents/#do","title":"\u2705 Do:","text":"<ul> <li>Be specific: Clear guidelines and constraints</li> <li>Provide criteria: Explicit evaluation metrics</li> <li>Use examples: Show desired format</li> <li>Set expectations: Define success conditions</li> <li>Be concise: Avoid unnecessary verbosity</li> </ul>"},{"location":"architecture/agents/#dont","title":"\u274c Don't:","text":"<ul> <li>Be vague: \"Write a good slogan\" (too broad)</li> <li>Conflict: Contradictory instructions</li> <li>Overload: Too many guidelines (cognitive load)</li> <li>Assume: Context the model might not have</li> </ul>"},{"location":"architecture/agents/#temperature-selection","title":"Temperature Selection","text":"Task Temperature Rationale Writer (Initial) 0.7-0.9 Creative diversity needed Writer (Refinement) 0.5-0.7 Focus on feedback integration Reviewer 0.3-0.5 Consistent evaluation standards"},{"location":"architecture/agents/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    slogan = await writer.generate(input, feedback)\nexcept TimeoutError:\n    # Retry with shorter timeout\n    slogan = await writer.generate(input, feedback, timeout=15)\nexcept ModelError as e:\n    # Log error and use fallback\n    logger.error(f\"Writer failed: {e}\")\n    raise GenerationError(\"Writer agent unavailable\")\n</code></pre>"},{"location":"architecture/agents/#extending-the-agent-system","title":"Extending the Agent System","text":""},{"location":"architecture/agents/#adding-a-new-agent","title":"Adding a New Agent","text":""},{"location":"architecture/agents/#example-editor-agent","title":"Example: Editor Agent","text":"<pre><code># src/agents/editor.py\n\nEDITOR_SYSTEM_PROMPT = \"\"\"\nYou are a professional copy editor. Review slogans for:\n1. Grammar and spelling\n2. Punctuation\n3. Style consistency\n4. Word choice\n\nProvide corrections or respond with \"APPROVED\" if perfect.\n\"\"\"\n\nasync def create_editor_agent(config: OllamaConfig) -&gt; Agent:\n    \"\"\"Create Editor agent for grammar/style review.\"\"\"\n    return Agent(\n        name=\"editor\",\n        model=config.model_name,\n        system_message=EDITOR_SYSTEM_PROMPT,\n        temperature=0.2,  # Very consistent editing\n        max_tokens=config.max_tokens,\n    )\n</code></pre>"},{"location":"architecture/agents/#integration","title":"Integration","text":"<pre><code># src/orchestration/workflow.py\n\nasync def run_slogan_generation_with_editor(\n    user_input: str,\n    model_name: str | None = None,\n    max_turns: int | None = None\n) -&gt; IterationSession:\n    \"\"\"Enhanced workflow with Editor agent.\"\"\"\n\n    writer = await create_writer_agent(config)\n    reviewer = await create_reviewer_agent(config)\n    editor = await create_editor_agent(config)  # New agent\n\n    for turn_num in range(1, max_turns + 1):\n        # Writer generates\n        slogan = await writer.generate(user_input, feedback)\n\n        # Editor reviews grammar (NEW STEP)\n        edited_slogan = await editor.review(slogan)\n\n        # Reviewer evaluates\n        review = await reviewer.evaluate(edited_slogan)\n\n        # ... rest of logic\n</code></pre>"},{"location":"architecture/agents/#multi-agent-patterns","title":"Multi-Agent Patterns","text":""},{"location":"architecture/agents/#sequential-pattern-current","title":"Sequential Pattern (Current)","text":"<pre><code>Writer \u2192 Reviewer \u2192 (loop)\n</code></pre>"},{"location":"architecture/agents/#parallel-pattern","title":"Parallel Pattern","text":"<pre><code>       \u250c\u2192 Reviewer A \u2192 Aggregate\nWriter \u2524\n       \u2514\u2192 Reviewer B \u2192 Aggregate\n</code></pre>"},{"location":"architecture/agents/#hierarchical-pattern","title":"Hierarchical Pattern","text":"<pre><code>Writer \u2192 Editor \u2192 Reviewer \u2192 Manager\n</code></pre>"},{"location":"architecture/agents/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/agents/#agent-call-optimization","title":"Agent Call Optimization","text":"<p>Current: 2 LLM calls per iteration (Writer + Reviewer)</p> <p>Optimization Options:</p> <ol> <li>Batch Processing: Generate multiple slogans, review all</li> <li>Caching: Cache agent responses for similar inputs</li> <li>Streaming: Stream responses for faster perceived speed</li> <li>Parallel Reviewers: Multiple reviewers vote on quality</li> </ol>"},{"location":"architecture/agents/#model-selection","title":"Model Selection","text":"Use Case Recommended Model Rationale Development <code>gemma2:2b</code> Fast iteration, lower quality OK Testing <code>phi3:mini</code> Good balance for tests Production <code>mistral:latest</code> Best quality-to-speed ratio High Quality <code>llama3.2:latest</code> Best results, slower"},{"location":"architecture/agents/#testing-agents","title":"Testing Agents","text":""},{"location":"architecture/agents/#unit-testing","title":"Unit Testing","text":"<pre><code># tests/unit/test_agents.py\n\nasync def test_writer_generates_slogan():\n    \"\"\"Test writer agent produces valid slogan.\"\"\"\n    config = OllamaConfig(model_name=\"gemma2:2b\")\n    writer = await create_writer_agent(config)\n\n    slogan = await writer.generate(\"eco-friendly water bottles\")\n\n    assert slogan\n    assert len(slogan.split()) &lt;= 10  # Reasonable length\n    assert slogan.strip() == slogan  # No extra whitespace\n\nasync def test_reviewer_detects_approval():\n    \"\"\"Test reviewer can approve slogans.\"\"\"\n    config = OllamaConfig(model_name=\"gemma2:2b\")\n    reviewer = await create_reviewer_agent(config)\n\n    # Excellent slogan\n    feedback = await reviewer.evaluate(\"Hydrate Green, Save Our Seas\")\n\n    assert is_approved(feedback)\n</code></pre>"},{"location":"architecture/agents/#integration-testing","title":"Integration Testing","text":"<pre><code># tests/integration/test_agents_integration.py\n\nasync def test_writer_reviewer_collaboration():\n    \"\"\"Test full Writer-Reviewer workflow.\"\"\"\n    session = await run_slogan_generation(\n        \"smart home devices\",\n        max_turns=3\n    )\n\n    assert session.final_slogan\n    assert session.turn_count &gt;= 1\n    assert session.completion_reason in [\n        CompletionReason.APPROVED,\n        CompletionReason.MAX_TURNS\n    ]\n</code></pre>"},{"location":"architecture/agents/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview - High-level system design</li> <li>Workflow Architecture - Orchestration details</li> <li>Agents API Reference - API documentation</li> <li>Development Guide - Developer setup</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This document provides a high-level overview of the Slogan Writer-Reviewer system architecture, design decisions, and technology stack.</p>"},{"location":"architecture/overview/#system-overview","title":"System Overview","text":"<p>The Slogan Writer-Reviewer is a multi-agent collaborative system that generates creative slogans through iterative Writer-Reviewer interactions. The system leverages the Microsoft Agent Framework pattern with Ollama for local LLM execution.</p>"},{"location":"architecture/overview/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Multi-Agent Collaboration: Separate Writer and Reviewer agents with distinct roles</li> <li>Iterative Refinement: Feedback loop until approval or max iterations</li> <li>Local-First: Runs entirely on local infrastructure (no cloud dependencies)</li> <li>Multi-Interface: Supports both CLI and REST API interfaces</li> <li>Type-Safe: Fully typed Python codebase with Pydantic validation</li> <li>Production-Ready: Comprehensive error handling, logging, and testing</li> </ul>"},{"location":"architecture/overview/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        User Interfaces                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         CLI (Click)           \u2502      REST API (FastAPI)         \u2502\n\u2502   \u2022 slogan-gen generate       \u2502   \u2022 POST /slogans/generate      \u2502\n\u2502   \u2022 slogan-gen models         \u2502   \u2022 GET /models                 \u2502\n\u2502   \u2022 slogan-gen config         \u2502   \u2022 GET /health                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502   Orchestration Layer       \u2502\n                \u2502  (src/orchestration/)       \u2502\n                \u2502                             \u2502\n                \u2502  \u2022 Workflow Coordination    \u2502\n                \u2502  \u2022 Iteration Management     \u2502\n                \u2502  \u2022 State Tracking           \u2502\n                \u2502  \u2022 Approval Logic           \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502      Agent Layer            \u2502\n                \u2502    (src/agents/)            \u2502\n                \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                \u2502    Writer    \u2502   Reviewer   \u2502\n                \u2502   Agent      \u2502    Agent     \u2502\n                \u2502              \u2502              \u2502\n                \u2502  Generates   \u2502  Evaluates   \u2502\n                \u2502  slogans     \u2502  &amp; approves  \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502              \u2502\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502    LLM Provider           \u2502\n                \u2502  (Ollama + Models)        \u2502\n                \u2502                           \u2502\n                \u2502  \u2022 mistral:latest         \u2502\n                \u2502  \u2022 gemma2:2b              \u2502\n                \u2502  \u2022 llama3.2:latest        \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/overview/#1-interface-layer","title":"1. Interface Layer","text":""},{"location":"architecture/overview/#cli-interface-srccli","title":"CLI Interface (<code>src/cli/</code>)","text":"<ul> <li>Framework: Click (command-line interface toolkit)</li> <li>Entry Point: <code>slogan-gen</code> command</li> <li>Commands:</li> <li><code>generate</code>: Create slogans with Writer-Reviewer workflow</li> <li><code>models</code>: List available Ollama models</li> <li><code>config show</code>: Display current configuration</li> <li><code>config set</code>: Update configuration values</li> <li>Output: Formatted terminal output with color and styling</li> <li>Error Handling: User-friendly error messages with exit codes</li> </ul>"},{"location":"architecture/overview/#rest-api-interface-srcapi","title":"REST API Interface (<code>src/api/</code>)","text":"<ul> <li>Framework: FastAPI (async ASGI web framework)</li> <li>Endpoints:</li> <li><code>POST /api/v1/slogans/generate</code>: Slogan generation</li> <li><code>GET /api/v1/models</code>: List models</li> <li><code>GET /api/v1/health</code>: Health check</li> <li><code>GET /</code>: API information</li> <li>Features:</li> <li>OpenAPI/Swagger documentation</li> <li>CORS support for cross-origin requests</li> <li>Request logging with unique IDs</li> <li>Comprehensive error handling</li> <li>Async request processing</li> </ul>"},{"location":"architecture/overview/#2-orchestration-layer-srcorchestration","title":"2. Orchestration Layer (<code>src/orchestration/</code>)","text":"<p>Purpose: Coordinates the Writer-Reviewer interaction workflow</p> <p>Key Components:</p> <ul> <li><code>workflow.py</code>: Main workflow orchestration</li> <li><code>run_slogan_generation()</code>: Entry point for generation</li> <li><code>is_approved()</code>: Checks for \"SHIP IT!\" approval</li> <li> <p><code>should_continue_iteration()</code>: Determines if more iterations needed</p> </li> <li> <p><code>models.py</code>: Data models for workflow state</p> </li> <li><code>IterationSession</code>: Tracks entire generation session</li> <li><code>Turn</code>: Represents single Writer-Reviewer exchange</li> <li><code>CompletionReason</code>: Enum for session completion (APPROVED, MAX_TURNS, ERROR)</li> <li><code>AgentRole</code>: Enum for agent identification (WRITER, REVIEWER)</li> <li><code>WorkflowMessage</code>: Communication format between agents</li> </ul> <p>Workflow Logic: 1. Initialize session with user input 2. Loop until approval or max turns:    - Writer generates slogan    - Reviewer evaluates slogan    - If \"SHIP IT!\" \u2192 approved, exit loop    - If not approved \u2192 Writer creates new version with feedback 3. Complete session with final result</p>"},{"location":"architecture/overview/#3-agent-layer-srcagents","title":"3. Agent Layer (<code>src/agents/</code>)","text":"<p>Purpose: Implements specialized agents with distinct roles</p>"},{"location":"architecture/overview/#writer-agent-writerpy","title":"Writer Agent (<code>writer.py</code>)","text":"<ul> <li>Role: Creative slogan generation</li> <li>System Prompt: Optimized for memorable, concise, engaging slogans</li> <li>Behavior: Generates initial slogan or refines based on feedback</li> <li>Input: User product description + optional reviewer feedback</li> <li>Output: New slogan proposal</li> </ul>"},{"location":"architecture/overview/#reviewer-agent-reviewerpy","title":"Reviewer Agent (<code>reviewer.py</code>)","text":"<ul> <li>Role: Quality evaluation and approval</li> <li>System Prompt: Evaluation criteria (memorability, clarity, conciseness, emotional appeal, uniqueness)</li> <li>Behavior: Evaluates slogan against criteria</li> <li>Input: Slogan to review</li> <li>Output: </li> <li>\"SHIP IT!\" (approval) OR</li> <li>Constructive feedback for improvement</li> </ul> <p>Agent Communication: - Agents communicate via structured <code>WorkflowMessage</code> objects - Asynchronous message passing pattern - No direct agent-to-agent coupling - Orchestration layer mediates all interactions</p>"},{"location":"architecture/overview/#4-configuration-layer-srcconfig","title":"4. Configuration Layer (<code>src/config/</code>)","text":"<p>Purpose: Centralized configuration management</p> <p>Key Components: - <code>settings.py</code>: Pydantic-based configuration   - <code>OllamaConfig</code>: All Ollama settings (URL, model, temperature, etc.)   - <code>get_ollama_config()</code>: Cached config singleton   - <code>get_available_models()</code>: Query Ollama for installed models</p> <p>Configuration Sources (priority order): 1. CLI Arguments: Override all (e.g., <code>--model gemma2:2b</code>) 2. Environment Variables: <code>OLLAMA_*</code> prefix 3. <code>.env</code> File: Loaded automatically 4. Defaults: Fallback values</p> <p>Configuration Fields: | Field | Default | Description | |-------|---------|-------------| | <code>base_url</code> | <code>http://localhost:11434/v1</code> | Ollama API endpoint | | <code>model_name</code> | <code>mistral:latest</code> | Default model | | <code>temperature</code> | <code>0.7</code> | Sampling temperature (0.0-2.0) | | <code>max_tokens</code> | <code>500</code> | Maximum response length | | <code>timeout</code> | <code>30</code> | Request timeout (seconds) | | <code>max_turns</code> | <code>5</code> | Maximum iteration turns |</p>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/overview/#core-technologies","title":"Core Technologies","text":"Technology Version Purpose Python 3.11+ Programming language Ollama Latest Local LLM runtime Microsoft Agent Framework - Multi-agent pattern Pydantic 2.x Data validation &amp; settings FastAPI Latest REST API framework Click 8.x CLI framework httpx Latest HTTP client (async) uv Latest Package manager"},{"location":"architecture/overview/#llm-models","title":"LLM Models","text":"Model Size Speed Quality Best For gemma2:2b 2B params \u26a1\u26a1\u26a1 Fast \u2b50\u2b50 Good Development, testing phi3:mini 3.8B params \u26a1\u26a1 Medium \u2b50\u2b50\u2b50 Very Good Balanced performance mistral:latest 7B params \u26a1 Slower \u2b50\u2b50\u2b50\u2b50 Excellent Production (default) llama3.2:latest 3B-70B params \u26a1-\u26a1\u26a1 Varies \u2b50\u2b50\u2b50\u2b50\u2b50 Outstanding High-quality output"},{"location":"architecture/overview/#development-tools","title":"Development Tools","text":"<ul> <li>pytest: Testing framework with fixtures and parametrization</li> <li>Ruff: Fast Python linter and formatter</li> <li>mypy: Static type checking</li> <li>MkDocs: Documentation generation (Material theme)</li> <li>mkdocstrings: API doc auto-generation</li> </ul>"},{"location":"architecture/overview/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/overview/#1-multi-agent-architecture","title":"1. Multi-Agent Architecture","text":"<p>Decision: Separate Writer and Reviewer agents instead of single agent</p> <p>Rationale: - \u2705 Separation of Concerns: Each agent has a single, focused responsibility - \u2705 Better Prompts: Specialized system prompts for each role - \u2705 Improved Quality: Dedicated reviewer ensures quality control - \u2705 Extensibility: Easy to add more agents (e.g., Editor, Marketer) - \u2705 Microsoft Agent Framework: Follows established pattern</p> <p>Trade-offs: - \u274c More LLM calls (2+ per iteration vs 1) - \u274c Increased latency (but acceptable for quality gain)</p>"},{"location":"architecture/overview/#2-iterative-refinement-loop","title":"2. Iterative Refinement Loop","text":"<p>Decision: Allow multiple Writer-Reviewer iterations</p> <p>Rationale: - \u2705 Quality Over Speed: Iterative improvement yields better slogans - \u2705 Feedback Integration: Writer learns from reviewer's critique - \u2705 Convergence: Usually reaches approval within 2-3 turns - \u2705 Configurable: <code>max_turns</code> prevents infinite loops</p> <p>Implementation: - Default: 5 max turns (configurable) - Exit conditions: Approval OR max turns reached - Each turn tracked for transparency (verbose mode)</p>"},{"location":"architecture/overview/#3-local-first-with-ollama","title":"3. Local-First with Ollama","text":"<p>Decision: Use Ollama for local LLM execution</p> <p>Rationale: - \u2705 Privacy: No data sent to external APIs - \u2705 Cost: No per-request charges - \u2705 Speed: Low latency (local network) - \u2705 Offline: Works without internet - \u2705 Control: Full control over models and settings</p> <p>Trade-offs: - \u274c Requires local setup (Ollama + models) - \u274c Hardware requirements (GPU recommended) - \u274c Model quality limited by local resources</p>"},{"location":"architecture/overview/#4-type-safe-python-with-pydantic","title":"4. Type-Safe Python with Pydantic","text":"<p>Decision: Use Pydantic for all data models and configuration</p> <p>Rationale: - \u2705 Validation: Automatic data validation at runtime - \u2705 Type Safety: Full mypy compliance - \u2705 Documentation: Self-documenting schemas - \u2705 IDE Support: Better autocomplete and hints - \u2705 FastAPI Integration: Native Pydantic support</p> <p>Example: <pre><code>class IterationSession(BaseModel):\n    session_id: str\n    user_input: str\n    model_name: str\n    turns: list[Turn] = []\n    completion_reason: CompletionReason | None = None\n</code></pre></p>"},{"location":"architecture/overview/#5-dual-interface-cli-rest-api","title":"5. Dual Interface (CLI + REST API)","text":"<p>Decision: Provide both CLI and REST API interfaces</p> <p>Rationale: - \u2705 Flexibility: CLI for scripts/terminals, API for integrations - \u2705 Different Use Cases: Local development vs remote services - \u2705 Same Core: Both use identical orchestration layer - \u2705 Progressive Enhancement: Start with CLI, add API later</p> <p>Implementation: - Shared orchestration logic in <code>src/orchestration/</code> - Interface-specific code in <code>src/cli/</code> and <code>src/api/</code> - No duplication of business logic</p>"},{"location":"architecture/overview/#6-async-first-api","title":"6. Async-First API","text":"<p>Decision: Use async/await for REST API</p> <p>Rationale: - \u2705 Scalability: Handle multiple concurrent requests - \u2705 Non-Blocking: Don't block on LLM calls - \u2705 FastAPI Native: FastAPI is async-first - \u2705 Future-Proof: Ready for async workflow if needed</p> <p>Implementation: <pre><code>async def generate_slogan(request_body: GenerateRequest) -&gt; GenerateResponse:\n    session = await run_slogan_generation(...)\n    return convert_session_to_response(session)\n</code></pre></p>"},{"location":"architecture/overview/#7-comprehensive-error-handling","title":"7. Comprehensive Error Handling","text":"<p>Decision: Detailed error handling at every layer</p> <p>Rationale: - \u2705 User Experience: Clear error messages - \u2705 Debugging: Actionable error information - \u2705 Production-Ready: Graceful failure handling - \u2705 Monitoring: Structured logging for troubleshooting</p> <p>Error Categories: - Connection Errors: Ollama not running - Validation Errors: Invalid input/configuration - Timeout Errors: Generation exceeds limits - Model Errors: Model not found/failed</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#cli-request-flow","title":"CLI Request Flow","text":"<pre><code>User Command\n    \u2193\nCLI Parser (Click)\n    \u2193\nConfig Loading (Pydantic)\n    \u2193\nWorkflow Orchestration\n    \u2193\nAgent Coordination Loop:\n    Writer Agent \u2192 Slogan\n    Reviewer Agent \u2192 Feedback/Approval\n    (repeat until approved or max turns)\n    \u2193\nSession Completion\n    \u2193\nOutput Formatter\n    \u2193\nTerminal Display\n</code></pre>"},{"location":"architecture/overview/#rest-api-request-flow","title":"REST API Request Flow","text":"<pre><code>HTTP Request\n    \u2193\nFastAPI Router\n    \u2193\nRequest Validation (Pydantic)\n    \u2193\nMiddleware Stack:\n    - Request Logging (UUID generation)\n    - CORS Headers\n    \u2193\nEndpoint Handler\n    \u2193\nWorkflow Orchestration (async)\n    \u2193\nAgent Coordination Loop:\n    Writer Agent \u2192 Slogan\n    Reviewer Agent \u2192 Feedback/Approval\n    (repeat until approved or max turns)\n    \u2193\nSession Completion\n    \u2193\nResponse Serialization (Pydantic)\n    \u2193\nMiddleware Stack:\n    - Response Logging\n    - Request ID Header\n    \u2193\nHTTP Response (JSON)\n</code></pre>"},{"location":"architecture/overview/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"architecture/overview/#current-architecture","title":"Current Architecture","text":"<ul> <li>Single-Threaded CLI: One request at a time</li> <li>Multi-Request API: FastAPI handles concurrent requests</li> <li>Thread Pool: API uses thread pool for sync workflow (10 workers)</li> <li>Timeout Protection: 600-second generation timeout</li> </ul>"},{"location":"architecture/overview/#scaling-options","title":"Scaling Options","text":""},{"location":"architecture/overview/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Load Balancer: Distribute API requests across multiple instances</li> <li>Stateless Design: No shared state between API instances</li> <li>Session Storage: Sessions are ephemeral (no persistence required)</li> </ul>"},{"location":"architecture/overview/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>GPU Acceleration: Use NVIDIA GPU for faster inference</li> <li>Larger Models: Use more powerful models for better quality</li> <li>Memory: More RAM for concurrent requests</li> </ul>"},{"location":"architecture/overview/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Model Caching: Ollama keeps models in memory</li> <li>Connection Pooling: Reuse HTTP connections to Ollama</li> <li>Async Workflow: Convert orchestration to async (future work)</li> </ul>"},{"location":"architecture/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/overview/#current-state-development","title":"Current State (Development)","text":"<ul> <li>\u2705 CORS: Configurable (default: all origins)</li> <li>\u2705 Input Validation: Pydantic validates all inputs</li> <li>\u2705 Request Timeouts: Prevent resource exhaustion</li> <li>\u2705 Error Sanitization: Don't expose internal errors</li> </ul>"},{"location":"architecture/overview/#production-recommendations","title":"Production Recommendations","text":"<ul> <li>\ud83d\udd12 API Authentication: Add API key or OAuth2</li> <li>\ud83d\udd12 Rate Limiting: Prevent abuse</li> <li>\ud83d\udd12 HTTPS: Encrypt transport layer</li> <li>\ud83d\udd12 Input Sanitization: Additional sanitization for prompts</li> <li>\ud83d\udd12 Logging: Audit log for security events</li> </ul>"},{"location":"architecture/overview/#extensibility","title":"Extensibility","text":""},{"location":"architecture/overview/#adding-new-agents","title":"Adding New Agents","text":"<pre><code># src/agents/editor.py\nasync def create_editor_agent(config: OllamaConfig) -&gt; Agent:\n    \"\"\"\n    Editor agent for grammar and style improvements.\n    \"\"\"\n    return Agent(\n        name=\"editor\",\n        model=config.model_name,\n        system_message=\"You are an expert editor...\",\n        # ...\n    )\n</code></pre>"},{"location":"architecture/overview/#adding-new-endpoints","title":"Adding New Endpoints","text":"<pre><code># src/api/routes/slogans.py\n@router.post(\"/slogans/batch\", response_model=BatchResponse)\nasync def generate_batch(request: BatchRequest) -&gt; BatchResponse:\n    \"\"\"Generate multiple slogans in parallel.\"\"\"\n    # Implementation\n</code></pre>"},{"location":"architecture/overview/#adding-new-configuration","title":"Adding New Configuration","text":"<pre><code># src/config/settings.py\nclass OllamaConfig(BaseSettings):\n    # Existing fields...\n\n    # New field\n    retry_attempts: int = Field(\n        default=3,\n        ge=1,\n        le=10,\n        description=\"Number of retry attempts\"\n    )\n</code></pre>"},{"location":"architecture/overview/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/overview/#logging","title":"Logging","text":"<ul> <li>Level: INFO (default), configurable via <code>API_LOG_LEVEL</code></li> <li>Format: Structured JSON logs (for production)</li> <li>Request IDs: UUID for request tracing</li> <li>Components Logged:</li> <li>Request/response details</li> <li>Workflow execution</li> <li>Agent interactions</li> <li>Errors and exceptions</li> </ul>"},{"location":"architecture/overview/#metrics-future","title":"Metrics (Future)","text":"<p>Potential metrics to collect: - Request count by endpoint - Generation duration (p50, p95, p99) - Turn count distribution - Approval rate - Error rate by type - Model usage statistics</p>"},{"location":"architecture/overview/#health-checks","title":"Health Checks","text":"<ul> <li><code>/api/v1/health</code>: API and Ollama status</li> <li>Dependency checks (Ollama connectivity)</li> <li>Response time measurement</li> </ul>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/overview/#unit-tests-testsunit","title":"Unit Tests (<code>tests/unit/</code>)","text":"<ul> <li>Isolated component testing</li> <li>Mock external dependencies (Ollama)</li> <li>Fast execution (&lt; 1 second)</li> </ul>"},{"location":"architecture/overview/#integration-tests-testsintegration","title":"Integration Tests (<code>tests/integration/</code>)","text":"<ul> <li>End-to-end workflows</li> <li>Real Ollama integration (requires local setup)</li> <li>Slower execution (several seconds)</li> </ul>"},{"location":"architecture/overview/#api-tests-testsapi","title":"API Tests (<code>tests/api/</code>)","text":"<ul> <li>FastAPI endpoint testing</li> <li>Request/response validation</li> <li>Error handling scenarios</li> </ul> <p>Coverage Target: &gt; 80%</p>"},{"location":"architecture/overview/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/overview/#development","title":"Development","text":"<pre><code>Developer Machine\n    \u251c\u2500\u2500 Ollama (localhost:11434)\n    \u251c\u2500\u2500 CLI (slogan-gen command)\n    \u2514\u2500\u2500 API (uvicorn localhost:8000)\n</code></pre>"},{"location":"architecture/overview/#production-recommended","title":"Production (Recommended)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Load Balancer / Reverse Proxy    \u2502\n\u2502              (nginx/caddy)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n\u2502 API #1 \u2502       \u2502 API #2 \u2502\n\u2502 (pod)  \u2502       \u2502 (pod)  \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502                 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Ollama Service \u2502\n    \u2502   (GPU node)    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\nCOPY . .\n\nRUN pip install uv &amp;&amp; uv sync\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"src.api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>Agents Architecture - Detailed agent design</li> <li>Workflow Architecture - Orchestration details</li> <li>REST API Reference - API documentation</li> <li>Development Guide - Developer setup</li> </ul>"},{"location":"architecture/workflow/","title":"Workflow Architecture","text":"<p>This document describes the orchestration system that coordinates the Writer-Reviewer agents through an iterative refinement loop.</p>"},{"location":"architecture/workflow/#overview","title":"Overview","text":"<p>The workflow system implements a managed iteration pattern where:</p> <ol> <li>Writer generates a slogan</li> <li>Reviewer evaluates and provides feedback</li> <li>Orchestrator decides whether to continue or complete</li> <li>Process repeats until approval or max turns reached</li> </ol>"},{"location":"architecture/workflow/#core-responsibilities","title":"Core Responsibilities","text":"Component Responsibility Orchestrator Manages agent lifecycle, iteration loop, state tracking Session Manager Maintains iteration history and session state Approval Checker Detects \"SHIP IT!\" approval signal Completion Handler Finalizes session with appropriate reason"},{"location":"architecture/workflow/#workflow-state-machine","title":"Workflow State Machine","text":""},{"location":"architecture/workflow/#states","title":"States","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INITIALIZED  \u2502\n\u2502  (Start)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GENERATING  \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  (Writer)    \u2502           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n       \u2502                   \u2502\n       \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502  REVIEWING   \u2502           \u2502\n\u2502 (Reviewer)   \u2502           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n       \u2502                   \u2502\n       \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502   DECIDING   \u2502           \u2502\n\u2502 (Check OK?)  \u2502           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n       \u2502                   \u2502\n       \u251c\u2500 Approved \u2500\u2500\u2500\u2500\u2500\u25ba\u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502                 \u2502  COMPLETED   \u2502\n       \u2502                 \u2502  (Success)   \u2502\n       \u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500 Not Approved \u2500\u2500\u2518 (feedback to Writer)\n       \u2502    + Turn &lt; Max\n       \u2502\n       \u2514\u2500 Max Turns \u2500\u2500\u2500\u25ba\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  COMPLETED   \u2502\n                        \u2502 (Max Turns)  \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/workflow/#state-transitions","title":"State Transitions","text":"From State Event To State Action INITIALIZED Start GENERATING Create agents GENERATING Writer complete REVIEWING Pass slogan to reviewer REVIEWING Reviewer complete DECIDING Check approval DECIDING Approved COMPLETED Finalize session DECIDING Not approved + turns left GENERATING Pass feedback to writer DECIDING Max turns reached COMPLETED Finalize with max turns reason"},{"location":"architecture/workflow/#main-workflow-function","title":"Main Workflow Function","text":""},{"location":"architecture/workflow/#run_slogan_generation","title":"<code>run_slogan_generation</code>","text":"<p>The primary orchestration function that manages the entire workflow.</p> <pre><code>async def run_slogan_generation(\n    user_input: str,\n    model_name: str | None = None,\n    max_turns: int | None = None,\n) -&gt; IterationSession:\n    \"\"\"\n    Run the slogan generation workflow with iterative refinement.\n\n    This function orchestrates the Writer-Reviewer collaboration:\n    1. Initializes session and agents\n    2. Runs iteration loop until approval or max turns\n    3. Returns complete session with all turns\n\n    Args:\n        user_input: Product/service description\n        model_name: LLM model to use (default from config)\n        max_turns: Max iterations (default from config)\n\n    Returns:\n        IterationSession with final slogan and turn history\n\n    Raises:\n        ConfigurationError: If Ollama config invalid\n        ModelNotFoundError: If specified model not available\n        AgentError: If agent creation or execution fails\n    \"\"\"\n</code></pre>"},{"location":"architecture/workflow/#implementation","title":"Implementation","text":"<pre><code># Phase 1: Initialize\nconfig = get_ollama_config()\nif model_name:\n    config.model_name = model_name\nif max_turns:\n    config.max_turns = max_turns\n\nsession = IterationSession(\n    user_input=user_input,\n    model_name=config.model_name,\n    max_turns=config.max_turns,\n)\n\n# Phase 2: Create Agents\nwriter = await create_writer_agent(config)\nreviewer = await create_reviewer_agent(config)\n\n# Phase 3: Iteration Loop\nfeedback: str | None = None\n\nfor turn_number in range(1, config.max_turns + 1):\n    # Step 1: Writer generates slogan\n    writer_message = await writer.send_message(\n        _create_writer_prompt(user_input, feedback)\n    )\n    slogan = writer_message.content.strip()\n\n    # Step 2: Reviewer evaluates\n    reviewer_message = await reviewer.send_message(\n        _create_reviewer_prompt(slogan)\n    )\n    reviewer_feedback = reviewer_message.content.strip()\n\n    # Step 3: Check approval\n    approved = is_approved(reviewer_feedback)\n\n    # Step 4: Record turn\n    turn = Turn(\n        turn_number=turn_number,\n        slogan=slogan,\n        feedback=None if approved else reviewer_feedback,\n        approved=approved,\n        timestamp=datetime.now(timezone.utc),\n    )\n    session.turns.append(turn)\n\n    # Step 5: Decision\n    if approved:\n        session.final_slogan = slogan\n        session.completion_reason = CompletionReason.APPROVED\n        session.turn_count = turn_number\n        session.completed_at = datetime.now(timezone.utc)\n        return session\n\n    # Prepare for next iteration\n    feedback = reviewer_feedback\n\n# Phase 4: Max turns reached\nsession.final_slogan = session.turns[-1].slogan\nsession.completion_reason = CompletionReason.MAX_TURNS\nsession.turn_count = config.max_turns\nsession.completed_at = datetime.now(timezone.utc)\nreturn session\n</code></pre>"},{"location":"architecture/workflow/#helper-functions","title":"Helper Functions","text":""},{"location":"architecture/workflow/#approval-detection","title":"Approval Detection","text":"<pre><code>def is_approved(feedback: str) -&gt; bool:\n    \"\"\"\n    Detect approval signal in reviewer feedback.\n\n    Looks for the exact phrase \"SHIP IT!\" (case-insensitive).\n    Uses word boundaries to avoid false positives.\n\n    Args:\n        feedback: Reviewer's response\n\n    Returns:\n        True if contains \"SHIP IT!\", False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_approved(\"This is perfect! SHIP IT!\")\n        True\n        &gt;&gt;&gt; is_approved(\"This needs work on shipping logistics\")\n        False\n    \"\"\"\n    normalized = feedback.upper().strip()\n    return bool(re.search(r'\\bSHIP IT!?\\b', normalized))\n</code></pre> <p>Design Considerations:</p> <ul> <li>Explicit Signal: Clear, unambiguous approval phrase</li> <li>Case Insensitive: Handles \"SHIP IT\", \"Ship It!\", \"ship it\"</li> <li>Word Boundaries: <code>\\b</code> prevents false matches (e.g., \"relationship\")</li> <li>Optional Punctuation: Matches \"SHIP IT\" or \"SHIP IT!\"</li> </ul>"},{"location":"architecture/workflow/#continuation-logic","title":"Continuation Logic","text":"<pre><code>def should_continue_iteration(\n    turn_number: int,\n    max_turns: int,\n    approved: bool,\n) -&gt; bool:\n    \"\"\"\n    Determine if workflow should continue to next iteration.\n\n    Args:\n        turn_number: Current iteration number (1-based)\n        max_turns: Maximum allowed iterations\n        approved: Whether current slogan was approved\n\n    Returns:\n        True if should continue, False if should stop\n    \"\"\"\n    if approved:\n        return False  # Stop on approval\n\n    if turn_number &gt;= max_turns:\n        return False  # Stop at max turns\n\n    return True  # Continue otherwise\n</code></pre>"},{"location":"architecture/workflow/#prompt-construction","title":"Prompt Construction","text":"<pre><code>def _create_writer_prompt(\n    user_input: str,\n    feedback: str | None,\n) -&gt; str:\n    \"\"\"\n    Build prompt for Writer agent.\n\n    Args:\n        user_input: Product/service description\n        feedback: Previous reviewer feedback (None for first turn)\n\n    Returns:\n        Formatted prompt string\n    \"\"\"\n    if feedback is None:\n        # First turn: Just the product description\n        return f\"Create a slogan for: {user_input}\"\n    else:\n        # Subsequent turns: Include feedback\n        return (\n            f\"Create a slogan for: {user_input}\\n\\n\"\n            f\"Previous feedback:\\n{feedback}\\n\\n\"\n            f\"Please improve based on the feedback.\"\n        )\n\n\ndef _create_reviewer_prompt(slogan: str) -&gt; str:\n    \"\"\"\n    Build prompt for Reviewer agent.\n\n    Args:\n        slogan: Slogan to evaluate\n\n    Returns:\n        Formatted prompt string\n    \"\"\"\n    return f\"Please review this slogan: {slogan}\"\n</code></pre>"},{"location":"architecture/workflow/#data-models","title":"Data Models","text":""},{"location":"architecture/workflow/#iterationsession","title":"IterationSession","text":"<p>Complete record of a workflow execution.</p> <pre><code>class IterationSession(BaseModel):\n    \"\"\"\n    Complete session record for one workflow execution.\n\n    Tracks all iterations, final result, and completion metadata.\n    \"\"\"\n\n    # Input\n    user_input: str\n    model_name: str\n    max_turns: int\n\n    # Execution\n    turns: list[Turn] = Field(default_factory=list)\n    turn_count: int = 0\n\n    # Result\n    final_slogan: str | None = None\n    completion_reason: CompletionReason | None = None\n\n    # Timestamps\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n    completed_at: datetime | None = None\n\n    @property\n    def duration(self) -&gt; timedelta | None:\n        \"\"\"Calculate session duration.\"\"\"\n        if self.completed_at:\n            return self.completed_at - self.created_at\n        return None\n\n    @property\n    def succeeded(self) -&gt; bool:\n        \"\"\"Check if session completed successfully (approved).\"\"\"\n        return self.completion_reason == CompletionReason.APPROVED\n</code></pre>"},{"location":"architecture/workflow/#turn","title":"Turn","text":"<p>Single iteration in the workflow.</p> <pre><code>class Turn(BaseModel):\n    \"\"\"\n    Single Writer-Reviewer exchange.\n\n    Represents one iteration in the refinement loop.\n    \"\"\"\n    turn_number: int\n    slogan: str\n    feedback: str | None  # None if approved\n    approved: bool\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n</code></pre>"},{"location":"architecture/workflow/#completionreason","title":"CompletionReason","text":"<p>Why the workflow ended.</p> <pre><code>class CompletionReason(str, Enum):\n    \"\"\"Reason for workflow completion.\"\"\"\n\n    APPROVED = \"approved\"        # Reviewer approved (\"SHIP IT!\")\n    MAX_TURNS = \"max_turns\"      # Reached maximum iterations\n    ERROR = \"error\"              # Encountered an error\n    USER_CANCELLED = \"cancelled\" # User cancelled (future)\n</code></pre>"},{"location":"architecture/workflow/#agentrole","title":"AgentRole","text":"<p>Agent type identifier.</p> <pre><code>class AgentRole(str, Enum):\n    \"\"\"Role of an agent in the workflow.\"\"\"\n\n    WRITER = \"writer\"\n    REVIEWER = \"reviewer\"\n</code></pre>"},{"location":"architecture/workflow/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"architecture/workflow/#happy-path-approval","title":"Happy Path (Approval)","text":"<pre><code>Turn 1:\n  User Input: \"eco-friendly water bottles\"\n  Writer \u2192 \"Hydrate Green, Live Clean\"\n  Reviewer \u2192 \"Good rhythm but vague. Be specific about impact.\"\n  Decision: Continue\n\nTurn 2:\n  User Input: \"eco-friendly water bottles\"\n  Feedback: \"Good rhythm but vague...\"\n  Writer \u2192 \"Hydrate Green, Save Our Seas\"\n  Reviewer \u2192 \"SHIP IT!\"\n  Decision: Complete (APPROVED)\n\nResult: \u2705 Success in 2 turns\n</code></pre>"},{"location":"architecture/workflow/#max-turns-path","title":"Max Turns Path","text":"<pre><code>Turn 1-5:\n  (Multiple iterations with feedback)\n\nTurn 5:\n  Writer \u2192 \"Final slogan attempt\"\n  Reviewer \u2192 \"Close but still needs work on X\"\n  Decision: Complete (MAX_TURNS)\n\nResult: \u26a0\ufe0f Reached limit without approval\n</code></pre>"},{"location":"architecture/workflow/#error-path","title":"Error Path","text":"<pre><code>Turn 1:\n  Writer \u2192 ConnectionError (Ollama down)\n  Decision: Complete (ERROR)\n\nResult: \u274c Failed to complete\n</code></pre>"},{"location":"architecture/workflow/#configuration","title":"Configuration","text":""},{"location":"architecture/workflow/#default-values","title":"Default Values","text":"<pre><code># src/config/settings.py\n\nDEFAULT_MODEL_NAME = \"mistral:latest\"\nDEFAULT_MAX_TURNS = 5\nDEFAULT_TEMPERATURE = 0.7\nDEFAULT_MAX_TOKENS = 100\nDEFAULT_OLLAMA_BASE_URL = \"http://localhost:11434\"\nDEFAULT_REQUEST_TIMEOUT = 30\n</code></pre>"},{"location":"architecture/workflow/#environment-variables","title":"Environment Variables","text":"<pre><code># Workflow configuration\nOLLAMA_MAX_TURNS=5           # Max iterations per session\nOLLAMA_MODEL_NAME=mistral:latest  # Model to use\nOLLAMA_TEMPERATURE=0.7       # Generation temperature\n\n# Ollama connection\nOLLAMA_BASE_URL=http://localhost:11434\nOLLAMA_REQUEST_TIMEOUT=30\n</code></pre>"},{"location":"architecture/workflow/#runtime-configuration","title":"Runtime Configuration","text":"<pre><code># Override via function parameters\nsession = await run_slogan_generation(\n    user_input=\"smart watches\",\n    model_name=\"llama3.2:latest\",  # Override default\n    max_turns=3,                   # Override default\n)\n</code></pre>"},{"location":"architecture/workflow/#error-handling","title":"Error Handling","text":""},{"location":"architecture/workflow/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>class WorkflowError(Exception):\n    \"\"\"Base exception for workflow errors.\"\"\"\n    pass\n\nclass AgentError(WorkflowError):\n    \"\"\"Error during agent creation or execution.\"\"\"\n    pass\n\nclass ApprovalTimeoutError(WorkflowError):\n    \"\"\"Max turns reached without approval.\"\"\"\n    pass\n\nclass ConfigurationError(WorkflowError):\n    \"\"\"Invalid configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/workflow/#error-recovery","title":"Error Recovery","text":"<pre><code>async def run_slogan_generation_with_retry(\n    user_input: str,\n    max_retries: int = 3,\n) -&gt; IterationSession:\n    \"\"\"\n    Run workflow with automatic retry on failure.\n\n    Retries on transient errors (network, timeout).\n    Does not retry on validation or configuration errors.\n    \"\"\"\n    for attempt in range(1, max_retries + 1):\n        try:\n            return await run_slogan_generation(user_input)\n        except (ConnectionError, TimeoutError) as e:\n            if attempt == max_retries:\n                raise WorkflowError(f\"Failed after {max_retries} attempts\") from e\n\n            wait_time = 2 ** attempt  # Exponential backoff\n            await asyncio.sleep(wait_time)\n            continue\n        except (ConfigurationError, ValidationError):\n            # Don't retry validation errors\n            raise\n</code></pre>"},{"location":"architecture/workflow/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/workflow/#current-performance","title":"Current Performance","text":"Metric Value Notes Avg Turns 2-3 Usually approved within 3 turns Avg Duration 10-15s Per turn (model dependent) Success Rate ~85% Percentage approved within max_turns"},{"location":"architecture/workflow/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"architecture/workflow/#1-early-stopping","title":"1. Early Stopping","text":"<pre><code>def should_stop_early(session: IterationSession) -&gt; bool:\n    \"\"\"\n    Detect convergence and stop early.\n\n    If last 2 slogans are very similar, likely converged.\n    \"\"\"\n    if len(session.turns) &lt; 3:\n        return False\n\n    last_two = [t.slogan for t in session.turns[-2:]]\n    similarity = calculate_similarity(last_two[0], last_two[1])\n\n    return similarity &gt; 0.95  # 95% similar\n</code></pre>"},{"location":"architecture/workflow/#2-parallel-agent-calls-future","title":"2. Parallel Agent Calls (Future)","text":"<pre><code># Current: Sequential\nslogan = await writer.generate(...)\nreview = await reviewer.evaluate(slogan)\n\n# Future: Parallel generation of multiple candidates\nslogans = await asyncio.gather(\n    writer.generate(input, feedback),\n    writer.generate(input, feedback),  # Different seed\n    writer.generate(input, feedback),\n)\nreviews = await asyncio.gather(*[\n    reviewer.evaluate(s) for s in slogans\n])\n</code></pre>"},{"location":"architecture/workflow/#3-model-caching","title":"3. Model Caching","text":"<pre><code># Cache frequently used models\n_model_cache: dict[str, Agent] = {}\n\nasync def get_cached_agent(\n    role: AgentRole,\n    config: OllamaConfig,\n) -&gt; Agent:\n    \"\"\"Get or create cached agent.\"\"\"\n    cache_key = f\"{role}:{config.model_name}\"\n\n    if cache_key not in _model_cache:\n        if role == AgentRole.WRITER:\n            _model_cache[cache_key] = await create_writer_agent(config)\n        else:\n            _model_cache[cache_key] = await create_reviewer_agent(config)\n\n    return _model_cache[cache_key]\n</code></pre>"},{"location":"architecture/workflow/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/workflow/#logging","title":"Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nasync def run_slogan_generation(\n    user_input: str,\n    model_name: str | None = None,\n    max_turns: int | None = None,\n) -&gt; IterationSession:\n    \"\"\"Main workflow with comprehensive logging.\"\"\"\n\n    logger.info(\n        \"Starting workflow\",\n        extra={\n            \"user_input\": user_input,\n            \"model_name\": model_name or config.model_name,\n            \"max_turns\": max_turns or config.max_turns,\n        }\n    )\n\n    # ... workflow logic ...\n\n    for turn_number in range(1, config.max_turns + 1):\n        logger.debug(f\"Starting turn {turn_number}\")\n\n        # Writer\n        start_time = time.time()\n        slogan = await writer.generate(...)\n        writer_duration = time.time() - start_time\n        logger.debug(\n            f\"Writer generated slogan\",\n            extra={\"duration\": writer_duration, \"slogan\": slogan}\n        )\n\n        # Reviewer\n        start_time = time.time()\n        review = await reviewer.evaluate(slogan)\n        reviewer_duration = time.time() - start_time\n        logger.debug(\n            f\"Reviewer provided feedback\",\n            extra={\"duration\": reviewer_duration, \"approved\": is_approved(review)}\n        )\n\n    logger.info(\n        \"Workflow completed\",\n        extra={\n            \"completion_reason\": session.completion_reason,\n            \"turn_count\": session.turn_count,\n            \"duration\": session.duration.total_seconds(),\n        }\n    )\n\n    return session\n</code></pre>"},{"location":"architecture/workflow/#metrics-future","title":"Metrics (Future)","text":"<pre><code># Prometheus metrics example\nfrom prometheus_client import Counter, Histogram\n\nworkflow_runs = Counter(\n    \"workflow_runs_total\",\n    \"Total workflow runs\",\n    [\"completion_reason\"]\n)\n\nworkflow_duration = Histogram(\n    \"workflow_duration_seconds\",\n    \"Workflow execution time\"\n)\n\nturn_count = Histogram(\n    \"workflow_turns\",\n    \"Number of turns per workflow\"\n)\n</code></pre>"},{"location":"architecture/workflow/#testing-workflow","title":"Testing Workflow","text":""},{"location":"architecture/workflow/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/unit/test_workflow.py\n\nasync def test_is_approved():\n    \"\"\"Test approval detection.\"\"\"\n    assert is_approved(\"SHIP IT!\")\n    assert is_approved(\"Perfect! SHIP IT! Great work.\")\n    assert not is_approved(\"This needs work on shipping\")\n\nasync def test_should_continue():\n    \"\"\"Test continuation logic.\"\"\"\n    assert should_continue_iteration(1, 5, False)  # Turn 1, not approved\n    assert not should_continue_iteration(1, 5, True)  # Approved\n    assert not should_continue_iteration(5, 5, False)  # Max turns\n</code></pre>"},{"location":"architecture/workflow/#integration-tests","title":"Integration Tests","text":"<pre><code># tests/integration/test_workflow_integration.py\n\n@pytest.mark.integration\nasync def test_full_workflow():\n    \"\"\"Test complete workflow with real agents.\"\"\"\n    session = await run_slogan_generation(\n        \"smart home devices\",\n        model_name=\"gemma2:2b\",  # Fast model for testing\n        max_turns=3,\n    )\n\n    assert session.final_slogan\n    assert session.turn_count &gt;= 1\n    assert session.completion_reason in [\n        CompletionReason.APPROVED,\n        CompletionReason.MAX_TURNS,\n    ]\n    assert session.duration\n    assert len(session.turns) == session.turn_count\n</code></pre>"},{"location":"architecture/workflow/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"architecture/workflow/#complete-flow","title":"Complete Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     START WORKFLOW                          \u2502\n\u2502  Input: user_input, model_name?, max_turns?                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  INITIALIZE SESSION                         \u2502\n\u2502  - Load configuration                                       \u2502\n\u2502  - Create IterationSession object                           \u2502\n\u2502  - Apply overrides (model_name, max_turns)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CREATE AGENTS                             \u2502\n\u2502  - Writer agent (creative generation)                       \u2502\n\u2502  - Reviewer agent (quality evaluation)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502     ITERATION LOOP (Turn 1-N)      \u2502\n        \u2502                                    \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502  1. WRITER GENERATES         \u2502 \u2502\n        \u2502  \u2502  - First turn: user_input    \u2502 \u2502\n        \u2502  \u2502  - Later turns: + feedback   \u2502 \u2502\n        \u2502  \u2502  Output: slogan              \u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2502                \u2502                  \u2502\n        \u2502                \u25bc                  \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502  2. REVIEWER EVALUATES       \u2502 \u2502\n        \u2502  \u2502  - Check quality criteria    \u2502 \u2502\n        \u2502  \u2502  - Approve or give feedback  \u2502 \u2502\n        \u2502  \u2502  Output: review              \u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2502                \u2502                  \u2502\n        \u2502                \u25bc                  \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502  3. CHECK APPROVAL           \u2502 \u2502\n        \u2502  \u2502  - Look for \"SHIP IT!\"       \u2502 \u2502\n        \u2502  \u2502  - Set approved flag         \u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2502                \u2502                  \u2502\n        \u2502                \u25bc                  \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502  4. RECORD TURN              \u2502 \u2502\n        \u2502  \u2502  - Create Turn object        \u2502 \u2502\n        \u2502  \u2502  - Add to session.turns      \u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2502                \u2502                  \u2502\n        \u2502                \u25bc                  \u2502\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n        \u2502  \u2502  5. DECIDE NEXT ACTION       \u2502 \u2502\n        \u2502  \u2502  - If approved \u2192 Complete    \u2502 \u2502\n        \u2502  \u2502  - If max turns \u2192 Complete   \u2502 \u2502\n        \u2502  \u2502  - Else \u2192 Continue (feedback)\u2502 \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n        \u2502                                    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                   \u2502\n        Approved \u2502                   \u2502 Max Turns / Not Approved\n                 \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  COMPLETE (APPROVED)       \u2502      \u2502\n\u2502  - Set final_slogan        \u2502      \u2502\n\u2502  - Set completion_reason   \u2502      \u2502\n\u2502  - Set timestamp           \u2502      \u2502\n\u2502  - Return session          \u2502      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                                    \u2502\n                                    \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  COMPLETE (MAX_TURNS)         \u2502\n                    \u2502  - Use last slogan            \u2502\n                    \u2502  - Set completion_reason      \u2502\n                    \u2502  - Set timestamp              \u2502\n                    \u2502  - Return session             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/workflow/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview - System architecture</li> <li>Agent Architecture - Agent design patterns</li> <li>Orchestration API Reference - API docs</li> <li>Development Guide - Developer setup</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Learn how to customize the Slogan Writer-Reviewer Agent System to match your needs.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":"<p>The system can be configured in three ways (in order of precedence):</p> <ol> <li>Command-line flags (highest priority)</li> <li>Environment variables</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"getting-started/configuration/#command-line-configuration","title":"Command-Line Configuration","text":"<p>Override settings for individual commands:</p> <pre><code># Specify model\nslogan-gen generate \"input\" --model mistral\n\n# Set iteration limit\nslogan-gen generate \"input\" --max-turns 7\n\n# Enable verbose output\nslogan-gen generate \"input\" --verbose\n\n# Combine multiple options\nslogan-gen generate \"coffee shop\" \\\n  --model llama3.2 \\\n  --max-turns 5 \\\n  --verbose \\\n  --output result.json\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set default values via environment variables:</p>"},{"location":"getting-started/configuration/#ollama-configuration","title":"Ollama Configuration","text":"<pre><code># Ollama API base URL (default: http://localhost:11434/v1)\nexport OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n\n# Default model to use (default: mistral:latest)\nexport OLLAMA_MODEL_NAME=\"mistral:latest\"\n\n# Maximum iteration rounds (default: 5)\nexport OLLAMA_MAX_TURNS=5\n\n# Temperature for generation (0.0-2.0, default: 0.7)\nexport OLLAMA_TEMPERATURE=0.7\n\n# Maximum tokens per response (default: 500)\nexport OLLAMA_MAX_TOKENS=500\n\n# Request timeout in seconds (default: 30)\nexport OLLAMA_TIMEOUT=30\n</code></pre>"},{"location":"getting-started/configuration/#api-configuration","title":"API Configuration","text":"<p>If using the REST API:</p> <pre><code># Allowed CORS origins (comma-separated)\nexport API_CORS_ORIGINS=\"http://localhost:3000,http://localhost:8080\"\n\n# Maximum generation time in seconds (default: 600)\nexport API_GENERATION_TIMEOUT=600\n\n# Total request timeout in seconds (default: 630)\nexport API_REQUEST_TIMEOUT=630\n\n# Logging level (default: WARNING)\nexport API_LOG_LEVEL=INFO\n\n# Maximum concurrent requests (default: 10)\nexport API_MAX_CONCURRENT_REQUESTS=10\n</code></pre>"},{"location":"getting-started/configuration/#using-a-env-file","title":"Using a .env File","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># .env\nOLLAMA_MODEL_NAME=mistral:latest\nOLLAMA_MAX_TURNS=5\nOLLAMA_TEMPERATURE=0.7\nAPI_LOG_LEVEL=INFO\n</code></pre> <p>The application will automatically load these variables.</p> <p>Environment File Loading</p> <p>For the CLI, you can use <code>direnv</code> or manually source the file. For the API, <code>uvicorn</code> can load <code>.env</code> files with the <code>--env-file</code> flag.</p>"},{"location":"getting-started/configuration/#cli-configuration-commands","title":"CLI Configuration Commands","text":""},{"location":"getting-started/configuration/#show-current-configuration","title":"Show Current Configuration","text":"<p>Display all current settings:</p> <pre><code>slogan-gen config show\n</code></pre> <p>Example Output:</p> <pre><code>Ollama Configuration:\n  Base URL: http://localhost:11434/v1\n  Model: mistral:latest\n  Max Turns: 5\n  Temperature: 0.7\n  Max Tokens: 500\n  Timeout: 30s\n\nAvailable Models:\n  - mistral:latest (default)\n  - gemma2:2b\n  - llama3.2:latest\n</code></pre>"},{"location":"getting-started/configuration/#set-configuration-values","title":"Set Configuration Values","text":"<p>Change default settings:</p> <pre><code># Set default model\nslogan-gen config set model_name mistral\n\n# Set max iterations\nslogan-gen config set max_turns 7\n\n# Set temperature\nslogan-gen config set temperature 0.9\n</code></pre> <p>Configuration Persistence</p> <p>Currently, <code>config set</code> commands update environment variables for the current session only. For permanent changes, add them to your shell profile (<code>~/.zshrc</code>, <code>~/.bashrc</code>) or <code>.env</code> file.</p>"},{"location":"getting-started/configuration/#list-available-models","title":"List Available Models","text":"<p>See what models you have installed:</p> <pre><code>slogan-gen models\n</code></pre> <p>Example Output:</p> <pre><code>Available Ollama Models:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nModel              Size    Modified\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nmistral:latest     7B      2 days ago\ngemma2:2b          2B      1 week ago\nllama3.2:latest    8B      3 days ago\nphi3:mini          3.8B    5 days ago\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nDefault model: mistral:latest\n</code></pre>"},{"location":"getting-started/configuration/#configuration-parameters-reference","title":"Configuration Parameters Reference","text":""},{"location":"getting-started/configuration/#model-selection","title":"Model Selection","text":"Parameter Description Default Valid Values <code>model</code> Ollama model to use <code>mistral:latest</code> Any installed Ollama model <p>Examples:</p> <ul> <li><code>gemma2:2b</code> - Fast, lightweight</li> <li><code>mistral:latest</code> - Balanced quality/speed</li> <li><code>llama3.2:latest</code> - High quality</li> <li><code>phi3:mini</code> - Microsoft's efficient model</li> </ul>"},{"location":"getting-started/configuration/#generation-parameters","title":"Generation Parameters","text":"Parameter Description Default Range <code>max_turns</code> Maximum iteration rounds 5 1-10 <code>temperature</code> Creativity level 0.7 0.0-2.0 <code>max_tokens</code> Max response length 500 50-2000 <code>timeout</code> Request timeout (seconds) 30 5-300 <p>Temperature Guide:</p> <ul> <li>0.0-0.3: Very focused, deterministic (good for consistency)</li> <li>0.4-0.7: Balanced creativity (recommended for slogans)</li> <li>0.8-1.2: More creative and varied</li> <li>1.3-2.0: Highly creative, potentially chaotic</li> </ul>"},{"location":"getting-started/configuration/#output-options","title":"Output Options","text":"Parameter Description Default Valid Values <code>verbose</code> Show iteration details <code>false</code> <code>true</code>, <code>false</code> <code>output</code> Save output to file <code>None</code> Any file path <p>Output Formats:</p> <ul> <li><code>.txt</code> - Plain text (slogan only)</li> <li><code>.json</code> - Full JSON with metadata</li> <li>stdout - Console output (default)</li> </ul>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/configuration/#custom-ollama-instance","title":"Custom Ollama Instance","text":"<p>Connect to a remote or custom Ollama instance:</p> <pre><code>export OLLAMA_BASE_URL=\"http://remote-server:11434/v1\"\nslogan-gen generate \"test\"\n</code></pre>"},{"location":"getting-started/configuration/#performance-tuning","title":"Performance Tuning","text":"<p>Optimize for speed:</p> <pre><code>export OLLAMA_MODEL_NAME=\"gemma2:2b\"  # Fastest model\nexport OLLAMA_MAX_TURNS=3              # Fewer iterations\nexport OLLAMA_MAX_TOKENS=200           # Shorter responses\nexport OLLAMA_TIMEOUT=15               # Shorter timeout\n</code></pre> <p>Optimize for quality:</p> <pre><code>export OLLAMA_MODEL_NAME=\"llama3.2:latest\"  # Best model\nexport OLLAMA_MAX_TURNS=7                    # More iterations\nexport OLLAMA_TEMPERATURE=0.8                # More creative\nexport OLLAMA_MAX_TOKENS=800                 # Longer responses\n</code></pre>"},{"location":"getting-started/configuration/#shell-aliases","title":"Shell Aliases","text":"<p>Create shortcuts for common configurations:</p> <pre><code># Add to ~/.zshrc or ~/.bashrc\n\n# Fast generation\nalias slogan-fast='slogan-gen generate --model gemma2:2b --max-turns 3'\n\n# High quality\nalias slogan-hq='slogan-gen generate --model llama3.2 --max-turns 7 --verbose'\n\n# Save as JSON\nalias slogan-json='slogan-gen generate --output result.json'\n</code></pre> <p>Usage:</p> <pre><code>slogan-fast \"coffee shop\"\nslogan-hq \"luxury hotel\"\n</code></pre>"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#development-setup","title":"Development Setup","text":"<p>Fast iterations for testing:</p> <pre><code># .env.development\nOLLAMA_MODEL_NAME=gemma2:2b\nOLLAMA_MAX_TURNS=3\nOLLAMA_TIMEOUT=15\nAPI_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"getting-started/configuration/#production-setup","title":"Production Setup","text":"<p>Quality and reliability:</p> <pre><code># .env.production\nOLLAMA_MODEL_NAME=mistral:latest\nOLLAMA_MAX_TURNS=5\nOLLAMA_TEMPERATURE=0.7\nOLLAMA_TIMEOUT=30\nAPI_LOG_LEVEL=WARNING\nAPI_MAX_CONCURRENT_REQUESTS=10\n</code></pre>"},{"location":"getting-started/configuration/#load-testing-setup","title":"Load Testing Setup","text":"<p>Handle high volume:</p> <pre><code># .env.loadtest\nOLLAMA_MODEL_NAME=gemma2:2b\nOLLAMA_MAX_TURNS=3\nAPI_MAX_CONCURRENT_REQUESTS=50\nAPI_GENERATION_TIMEOUT=300\n</code></pre>"},{"location":"getting-started/configuration/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":""},{"location":"getting-started/configuration/#check-active-configuration","title":"Check Active Configuration","text":"<p>Verify what settings are active:</p> <pre><code>slogan-gen config show\n</code></pre>"},{"location":"getting-started/configuration/#environment-variable-not-working","title":"Environment Variable Not Working","text":"<p>Ensure proper export:</p> <pre><code># Wrong (no export)\nOLLAMA_MODEL_NAME=mistral\n\n# Correct\nexport OLLAMA_MODEL_NAME=mistral\n</code></pre>"},{"location":"getting-started/configuration/#persistent-configuration","title":"Persistent Configuration","text":"<p>Add to shell profile for permanent changes:</p> <pre><code># Add to ~/.zshrc or ~/.bashrc\nexport OLLAMA_MODEL_NAME=\"mistral:latest\"\nexport OLLAMA_MAX_TURNS=5\n\n# Reload shell\nsource ~/.zshrc  # or source ~/.bashrc\n</code></pre>"},{"location":"getting-started/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Remember the precedence order:</p> <ol> <li>CLI flags override everything</li> <li>Environment variables override defaults</li> <li>Defaults are used if nothing else set</li> </ol> <p>Example:</p> <pre><code># Environment sets default\nexport OLLAMA_MAX_TURNS=5\n\n# CLI flag overrides environment\nslogan-gen generate \"test\" --max-turns 3  # Uses 3, not 5\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Start generating slogans</li> <li>CLI Usage - Complete command reference</li> <li>API Usage - REST API configuration</li> <li>Troubleshooting - Common configuration issues</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will walk you through installing the Slogan Writer-Reviewer Agent System on your machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.11+: Required for modern async/await support</li> <li>Ollama: Local LLM runtime for running AI models</li> <li>uv: Fast Python package manager (recommended)</li> </ul>"},{"location":"getting-started/installation/#step-1-install-ollama","title":"Step 1: Install Ollama","text":"<p>Ollama is required to run local AI models. Install it for your platform:</p>"},{"location":"getting-started/installation/#macos-and-linux","title":"macOS and Linux","text":"<pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>Download and run the installer from ollama.ai</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check that Ollama is installed\nollama --version\n\n# Start Ollama service (if not auto-started)\nollama serve\n</code></pre>"},{"location":"getting-started/installation/#step-2-pull-an-ai-model","title":"Step 2: Pull an AI Model","text":"<p>Download at least one model for slogan generation. We recommend starting with <code>mistral</code> for the best balance of quality and speed:</p>"},{"location":"getting-started/installation/#recommended-models","title":"Recommended Models","text":"<pre><code># Default: Best balance of speed and quality (recommended)\nollama pull mistral\n\n# Fast: Lightweight for quick testing\nollama pull gemma2:2b\n\n# High Quality: Slower but better output\nollama pull llama3.2:latest\n\n# Microsoft: Efficient and capable\nollama pull phi3:mini\n</code></pre>"},{"location":"getting-started/installation/#model-comparison","title":"Model Comparison","text":"Model Size Speed Quality Best For <code>gemma2:2b</code> 2B \u26a1\u26a1\u26a1 \u2b50\u2b50 Quick testing <code>phi3:mini</code> 3.8B \u26a1\u26a1 \u2b50\u2b50\u2b50 Development <code>mistral</code> 7B \u26a1 \u2b50\u2b50\u2b50\u2b50 Production (default) <code>llama3.2</code> 8B \ud83d\udc0c \u2b50\u2b50\u2b50\u2b50\u2b50 High quality output <p>Performance Tip</p> <p>Smaller models (2B-3B parameters) run much faster on CPU but may not follow instructions as well. For production use, stick with <code>mistral</code> or larger.</p>"},{"location":"getting-started/installation/#verify-model-installation","title":"Verify Model Installation","text":"<pre><code># List installed models\nollama list\n\n# Test model generation\nollama run mistral \"Write a short slogan for a coffee shop\"\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-uv-package-manager","title":"Step 3: Install uv Package Manager","text":"<p><code>uv</code> is a fast Python package manager that simplifies dependency management:</p>"},{"location":"getting-started/installation/#install-uv","title":"Install uv","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows (PowerShell)\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Via pip\npip install uv\n</code></pre> <p>For more installation options, see the uv documentation.</p>"},{"location":"getting-started/installation/#step-4-install-the-slogan-generator","title":"Step 4: Install the Slogan Generator","text":"<p>Clone the repository and install the package:</p> <pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd microsoft-agent-framework-with-ollama-1\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the package with dependencies\nuv pip install -e .\n\n# For development (includes testing and linting tools)\nuv pip install -e \".[dev]\"\n\n# For documentation (includes MkDocs)\nuv pip install -e \".[docs]\"\n</code></pre> <p>Editable Installation</p> <p>The <code>-e</code> flag installs the package in \"editable\" mode, meaning changes to the source code take effect immediately without reinstalling.</p>"},{"location":"getting-started/installation/#step-5-verify-installation","title":"Step 5: Verify Installation","text":"<p>Test that everything is working:</p> <pre><code># Test CLI installation\nslogan-gen --version\n\n# Test configuration\nslogan-gen config show\n\n# List available models\nslogan-gen models\n\n# Generate your first slogan\nslogan-gen generate \"eco-friendly water bottle\"\n</code></pre> <p>If the slogan generation works, you're all set! \ud83c\udf89</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#ollama-not-running","title":"Ollama Not Running","text":"<p>Error: <code>\u274c Error: Cannot connect to Ollama at http://localhost:11434</code></p> <p>Solution: Start the Ollama service:</p> <pre><code>ollama serve\n</code></pre>"},{"location":"getting-started/installation/#model-not-found","title":"Model Not Found","text":"<p>Error: <code>\u274c Error: Model 'mistral' not found</code></p> <p>Solution: Pull the model first:</p> <pre><code>ollama pull mistral\n</code></pre>"},{"location":"getting-started/installation/#python-version-too-old","title":"Python Version Too Old","text":"<p>Error: <code>Requires Python &gt;=3.11</code></p> <p>Solution: Upgrade Python:</p> <pre><code># macOS (Homebrew)\nbrew install python@3.11\n\n# Ubuntu/Debian\nsudo apt install python3.11\n\n# Windows: Download from python.org\n</code></pre>"},{"location":"getting-started/installation/#command-not-found-slogan-gen","title":"Command Not Found: slogan-gen","text":"<p>Error: <code>zsh: command not found: slogan-gen</code></p> <p>Solution: Ensure virtual environment is activated:</p> <pre><code>source .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate     # Windows\n</code></pre> <p>Or reinstall the package:</p> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Generate your first slogans</li> <li>Configuration - Customize default settings</li> <li>CLI Usage - Complete command reference</li> <li>API Usage - Use the REST API</li> </ul>"},{"location":"getting-started/installation/#alternative-using-pip","title":"Alternative: Using pip","text":"<p>If you prefer not to use <code>uv</code>, you can use standard <code>pip</code>:</p> <pre><code># Create virtual environment\npython3.11 -m venv .venv\nsource .venv/bin/activate\n\n# Install package\npip install -e .\npip install -e \".[dev]\"  # With development dependencies\n</code></pre> <p>However, <code>uv</code> is significantly faster and provides better dependency resolution.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with the Slogan Writer-Reviewer Agent System in minutes!</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you've completed the Installation Guide before proceeding.</p>"},{"location":"getting-started/quickstart/#your-first-slogan","title":"Your First Slogan","text":"<p>Generate a slogan with a single command:</p> <pre><code>slogan-gen generate \"eco-friendly water bottle\"\n</code></pre> <p>Expected Output:</p> <pre><code>\ud83c\udfaf Generating slogan for: eco-friendly water bottle\n\n\ud83c\udfa8 Writer-Reviewer Collaboration\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2705 Final Slogan (Turn 2/5):\n\ud83d\udca7 Hydrate Sustainably, Live Responsibly\n\n\u2713 Approved by Reviewer\n\u23f1\ufe0f  Total Duration: 8.2 seconds\n</code></pre> <p>Congratulations! You've generated your first slogan! \ud83c\udf89</p>"},{"location":"getting-started/quickstart/#see-the-collaboration-process","title":"See the Collaboration Process","text":"<p>Use <code>--verbose</code> mode to watch the Writer and Reviewer agents collaborate:</p> <pre><code>slogan-gen generate \"tech startup\" --verbose\n</code></pre> <p>Example Output:</p> <pre><code>\ud83c\udfaf Generating slogan for: tech startup\n\n\ud83c\udfa8 Writer-Reviewer Collaboration\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u250c\u2500 Turn 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udcdd Writer:                               \u2502\n\u2502 \"Innovate Today, Transform Tomorrow\"    \u2502\n\u2502                                          \u2502\n\u2502 \ud83d\udcac Reviewer Feedback:                    \u2502\n\u2502 \"Good start, but feels generic. Add     \u2502\n\u2502 more specificity about the tech         \u2502\n\u2502 industry. Consider making it punchier.\" \u2502\n\u2502                                          \u2502\n\u2502 \u23f1\ufe0f  Duration: 4.1s                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 Turn 2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udcdd Writer:                               \u2502\n\u2502 \"\ud83d\ude80 Code the Future, Ship the Possible\" \u2502\n\u2502                                          \u2502\n\u2502 \ud83d\udcac Reviewer Feedback:                    \u2502\n\u2502 \"SHIP IT! Perfect combination of tech   \u2502\n\u2502 imagery and aspirational messaging.\"     \u2502\n\u2502                                          \u2502\n\u2502 \u2705 APPROVED                              \u2502\n\u2502 \u23f1\ufe0f  Duration: 3.8s                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2705 Final Slogan (Turn 2/5):\n\ud83d\ude80 Code the Future, Ship the Possible\n\n\u2713 Approved by Reviewer\n\u23f1\ufe0f  Total Duration: 7.9 seconds\n</code></pre>"},{"location":"getting-started/quickstart/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"getting-started/quickstart/#use-a-different-model","title":"Use a Different Model","text":"<p>Try different AI models for varied styles:</p> <pre><code># Fast and efficient\nslogan-gen generate \"coffee shop\" --model gemma2:2b\n\n# Default balanced model\nslogan-gen generate \"coffee shop\" --model mistral\n\n# High quality output\nslogan-gen generate \"coffee shop\" --model llama3.2\n</code></pre>"},{"location":"getting-started/quickstart/#limit-iterations","title":"Limit Iterations","text":"<p>Control how many revision rounds are allowed:</p> <pre><code># Quick generation (1-3 turns recommended)\nslogan-gen generate \"fitness app\" --max-turns 3\n\n# More iterations for quality (default is 5)\nslogan-gen generate \"luxury hotel\" --max-turns 7\n</code></pre>"},{"location":"getting-started/quickstart/#save-output-to-file","title":"Save Output to File","text":"<p>Save slogans for later use:</p> <pre><code># Save as plain text\nslogan-gen generate \"pizza restaurant\" --output slogan.txt\n\n# Save as JSON for programmatic use\nslogan-gen generate \"pizza restaurant\" --output result.json\n</code></pre> <p>JSON Output Example:</p> <pre><code>{\n  \"input\": \"pizza restaurant\",\n  \"final_slogan\": \"\ud83c\udf55 Slice of Heaven, Every Bite!\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 2,\n  \"max_turns\": 5,\n  \"total_duration_seconds\": 5.8,\n  \"average_duration_per_turn\": 2.9,\n  \"model_used\": \"mistral:latest\",\n  \"turns\": [\n    {\n      \"turn_number\": 1,\n      \"slogan\": \"Pizza Perfection in Every Slice\",\n      \"feedback\": \"Good start, but needs more excitement and memorability.\",\n      \"approved\": false,\n      \"timestamp\": \"2024-01-15T10:30:00Z\",\n      \"duration_seconds\": 3.1\n    },\n    {\n      \"turn_number\": 2,\n      \"slogan\": \"\ud83c\udf55 Slice of Heaven, Every Bite!\",\n      \"feedback\": \"SHIP IT! Perfect combination of emoji and excitement.\",\n      \"approved\": true,\n      \"timestamp\": \"2024-01-15T10:30:05Z\",\n      \"duration_seconds\": 2.7\n    }\n  ]\n}\n</code></pre>"},{"location":"getting-started/quickstart/#tips-for-better-slogans","title":"Tips for Better Slogans","text":""},{"location":"getting-started/quickstart/#be-specific","title":"Be Specific","text":"<p>\u274c Vague: <code>slogan-gen generate \"business\"</code></p> <p>\u2705 Specific: <code>slogan-gen generate \"eco-friendly cleaning products for homes\"</code></p>"},{"location":"getting-started/quickstart/#provide-context","title":"Provide Context","text":"<p>Include details about your target audience or unique value:</p> <pre><code>slogan-gen generate \"mobile app for busy parents to organize family schedules\"\n</code></pre>"},{"location":"getting-started/quickstart/#experiment-with-models","title":"Experiment with Models","text":"<p>Different models have different creative styles:</p> <ul> <li>gemma2:2b: Quick, straightforward slogans</li> <li>mistral: Balanced creativity and professionalism</li> <li>llama3.2: More nuanced, literary style</li> <li>phi3:mini: Technical and precise</li> </ul>"},{"location":"getting-started/quickstart/#use-verbose-mode-for-insights","title":"Use Verbose Mode for Insights","text":"<p>Watch how the agents collaborate to understand what makes a good slogan:</p> <pre><code>slogan-gen generate \"your product\" --verbose\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Check your current settings:</p> <pre><code>slogan-gen config show\n</code></pre> <p>List available models:</p> <pre><code>slogan-gen models\n</code></pre>"},{"location":"getting-started/quickstart/#example-workflows","title":"Example Workflows","text":""},{"location":"getting-started/quickstart/#quick-brainstorming-session","title":"Quick Brainstorming Session","text":"<p>Generate multiple slogans quickly:</p> <pre><code>slogan-gen generate \"coffee shop\" --model gemma2:2b --max-turns 3 --output coffee1.txt\nslogan-gen generate \"artisan coffee with local beans\" --model gemma2:2b --max-turns 3 --output coffee2.txt\nslogan-gen generate \"cozy neighborhood caf\u00e9\" --model gemma2:2b --max-turns 3 --output coffee3.txt\n</code></pre>"},{"location":"getting-started/quickstart/#high-quality-final-slogan","title":"High-Quality Final Slogan","text":"<p>Take time for a polished result:</p> <pre><code>slogan-gen generate \"premium organic coffee roastery\" \\\n  --model llama3.2 \\\n  --max-turns 7 \\\n  --verbose \\\n  --output final-slogan.json\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<p>Process multiple inputs from a file:</p> <pre><code># Create input file\ncat &gt; inputs.txt &lt;&lt; EOF\neco-friendly water bottle\ntech startup\ncoffee shop\nfitness app\nEOF\n\n# Process each line\nwhile read line; do\n  slogan-gen generate \"$line\" --output \"slogan-$(echo $line | tr ' ' '-').txt\"\ndone &lt; inputs.txt\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Guide - Customize default settings</li> <li>CLI Usage - Complete command reference</li> <li>API Usage - Integrate with REST API</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quickstart/#slow-generation","title":"Slow Generation","text":"<p>If generation takes too long, try:</p> <ol> <li>Use a smaller model: <code>--model gemma2:2b</code></li> <li>Reduce iterations: <code>--max-turns 3</code></li> <li>Check system resources (close other apps)</li> </ol> <p>See Troubleshooting for more solutions.</p>"},{"location":"getting-started/quickstart/#quality-issues-with-small-models","title":"Quality Issues with Small Models","text":"<p>If slogans are poor quality:</p> <ol> <li>Switch to a larger model: <code>--model mistral</code></li> <li>Increase iterations: <code>--max-turns 7</code></li> <li>Provide more specific input</li> </ol>"},{"location":"getting-started/quickstart/#connection-errors","title":"Connection Errors","text":"<p>If you get connection errors:</p> <pre><code># Ensure Ollama is running\nollama serve\n\n# Test connection\ncurl http://localhost:11434/api/tags\n</code></pre>"},{"location":"guides/api-clients/","title":"API Client Examples","text":"<p>This guide provides comprehensive examples for consuming the Slogan Writer-Reviewer API from various programming languages and tools.</p>"},{"location":"guides/api-clients/#quick-start","title":"Quick Start","text":""},{"location":"guides/api-clients/#prerequisites","title":"Prerequisites","text":"<ul> <li>API running at <code>http://localhost:8000</code></li> <li>Ollama running with at least one model installed</li> <li>HTTP client library for your language</li> </ul>"},{"location":"guides/api-clients/#basic-request-flow","title":"Basic Request Flow","text":"<ol> <li>Health Check: Verify API is running</li> <li>List Models: Get available models</li> <li>Generate Slogan: Create a slogan with Writer-Reviewer workflow</li> </ol>"},{"location":"guides/api-clients/#python-examples","title":"Python Examples","text":""},{"location":"guides/api-clients/#using-httpx-recommended","title":"Using httpx (Recommended)","text":""},{"location":"guides/api-clients/#installation","title":"Installation","text":"<pre><code>pip install httpx\n</code></pre>"},{"location":"guides/api-clients/#basic-synchronous-example","title":"Basic Synchronous Example","text":"<pre><code>import httpx\n\n# Configure API base URL\nBASE_URL = \"http://localhost:8000\"\n\n# Create client\nclient = httpx.Client(base_url=BASE_URL)\n\n# Health check\nhealth = client.get(\"/api/v1/health\")\nprint(f\"Status: {health.json()['status']}\")\n\n# List models\nmodels = client.get(\"/api/v1/models\")\nprint(f\"Available models: {[m['name'] for m in models.json()['models']]}\")\n\n# Generate slogan\nresponse = client.post(\n    \"/api/v1/slogans/generate\",\n    json={\n        \"input\": \"eco-friendly water bottles\",\n        \"verbose\": False\n    }\n)\n\nresult = response.json()\nprint(f\"Slogan: {result['slogan']}\")\nprint(f\"Turns: {result['turn_count']}\")\nprint(f\"Duration: {result['total_duration_seconds']}s\")\n\n# Close client\nclient.close()\n</code></pre>"},{"location":"guides/api-clients/#async-example","title":"Async Example","text":"<pre><code>import asyncio\nimport httpx\n\nasync def generate_slogan(product: str) -&gt; dict:\n    \"\"\"Generate a slogan asynchronously.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8000/api/v1/slogans/generate\",\n            json={\"input\": product, \"verbose\": True}\n        )\n        response.raise_for_status()\n        return response.json()\n\nasync def main():\n    # Generate multiple slogans concurrently\n    products = [\n        \"smart home devices\",\n        \"organic coffee\",\n        \"fitness tracker\"\n    ]\n\n    tasks = [generate_slogan(p) for p in products]\n    results = await asyncio.gather(*tasks)\n\n    for product, result in zip(products, results):\n        print(f\"\\n{product}:\")\n        print(f\"  Slogan: {result['slogan']}\")\n        print(f\"  Turns: {result['turn_count']}\")\n\n# Run async function\nasyncio.run(main())\n</code></pre>"},{"location":"guides/api-clients/#error-handling","title":"Error Handling","text":"<pre><code>import httpx\n\ndef generate_with_retry(product: str, max_retries: int = 3) -&gt; dict:\n    \"\"\"Generate slogan with retry logic.\"\"\"\n    client = httpx.Client(timeout=620.0)  # Slightly &gt; server timeout\n\n    for attempt in range(max_retries):\n        try:\n            response = client.post(\n                \"http://localhost:8000/api/v1/slogans/generate\",\n                json={\"input\": product}\n            )\n            response.raise_for_status()\n            return response.json()\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 503:\n                print(f\"Attempt {attempt + 1}: Service unavailable\")\n                if attempt &lt; max_retries - 1:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                    continue\n            elif e.response.status_code == 422:\n                print(f\"Validation error: {e.response.json()}\")\n                raise\n            elif e.response.status_code == 504:\n                print(f\"Timeout: Generation took too long\")\n                raise\n            else:\n                print(f\"HTTP error: {e.response.status_code}\")\n                raise\n\n        except httpx.TimeoutException:\n            print(f\"Attempt {attempt + 1}: Request timeout\")\n            if attempt &lt; max_retries - 1:\n                continue\n            raise\n\n        except httpx.RequestError as e:\n            print(f\"Connection error: {e}\")\n            if attempt &lt; max_retries - 1:\n                time.sleep(2 ** attempt)\n                continue\n            raise\n\n    raise Exception(\"Max retries exceeded\")\n</code></pre>"},{"location":"guides/api-clients/#context-manager","title":"Context Manager","text":"<pre><code>import httpx\nfrom contextlib import contextmanager\n\n@contextmanager\ndef slogan_api_client(base_url: str = \"http://localhost:8000\"):\n    \"\"\"Context manager for API client.\"\"\"\n    client = httpx.Client(base_url=base_url, timeout=620.0)\n    try:\n        # Verify health\n        health = client.get(\"/api/v1/health\")\n        if health.json()[\"status\"] != \"healthy\":\n            raise RuntimeError(\"API not healthy\")\n        yield client\n    finally:\n        client.close()\n\n# Usage\nwith slogan_api_client() as client:\n    response = client.post(\n        \"/api/v1/slogans/generate\",\n        json={\"input\": \"cloud storage\"}\n    )\n    print(response.json()[\"slogan\"])\n</code></pre>"},{"location":"guides/api-clients/#using-requests","title":"Using requests","text":""},{"location":"guides/api-clients/#installation_1","title":"Installation","text":"<pre><code>pip install requests\n</code></pre>"},{"location":"guides/api-clients/#basic-example","title":"Basic Example","text":"<pre><code>import requests\n\n# Generate slogan\nresponse = requests.post(\n    \"http://localhost:8000/api/v1/slogans/generate\",\n    json={\n        \"input\": \"sustainable fashion\",\n        \"model\": \"mistral:latest\",\n        \"max_turns\": 5,\n        \"verbose\": True\n    },\n    timeout=620\n)\n\n# Check status\nresponse.raise_for_status()\n\n# Parse response\ndata = response.json()\nprint(f\"Slogan: {data['slogan']}\")\n\n# Print iteration history\nif data['turns']:\n    print(f\"\\nIteration History ({len(data['turns'])} turns):\")\n    for turn in data['turns']:\n        status = \"\u2713\" if turn['approved'] else \"\u2717\"\n        print(f\"  {status} Turn {turn['turn_number']}: {turn['slogan']}\")\n        if turn['feedback']:\n            print(f\"    Feedback: {turn['feedback']}\")\n</code></pre>"},{"location":"guides/api-clients/#session-with-custom-headers","title":"Session with Custom Headers","text":"<pre><code>import requests\nimport uuid\n\n# Create session with custom headers\nsession = requests.Session()\nsession.headers.update({\n    \"X-Request-ID\": str(uuid.uuid4()),\n    \"User-Agent\": \"MyApp/1.0\"\n})\n\n# All requests use these headers\nresponse = session.post(\n    \"http://localhost:8000/api/v1/slogans/generate\",\n    json={\"input\": \"AI assistant\"}\n)\n\nprint(f\"Request ID: {response.headers.get('X-Request-ID')}\")\nprint(f\"Slogan: {response.json()['slogan']}\")\n\nsession.close()\n</code></pre>"},{"location":"guides/api-clients/#javascripttypescript-examples","title":"JavaScript/TypeScript Examples","text":""},{"location":"guides/api-clients/#using-fetch-browsernodejs","title":"Using fetch (Browser/Node.js)","text":""},{"location":"guides/api-clients/#basic-example_1","title":"Basic Example","text":"<pre><code>const BASE_URL = 'http://localhost:8000';\n\nasync function generateSlogan(input, options = {}) {\n  const response = await fetch(`${BASE_URL}/api/v1/slogans/generate`, {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n      input,\n      model: options.model,\n      max_turns: options.maxTurns,\n      verbose: options.verbose || false\n    })\n  });\n\n  if (!response.ok) {\n    const error = await response.json();\n    throw new Error(`API Error: ${JSON.stringify(error)}`);\n  }\n\n  return await response.json();\n}\n\n// Usage\n(async () =&gt; {\n  try {\n    const result = await generateSlogan('smart home devices', {\n      verbose: true\n    });\n\n    console.log(`Slogan: ${result.slogan}`);\n    console.log(`Turns: ${result.turn_count}`);\n    console.log(`Duration: ${result.total_duration_seconds}s`);\n\n    if (result.turns) {\n      console.log('\\nIteration History:');\n      result.turns.forEach(turn =&gt; {\n        console.log(`  ${turn.approved ? '\u2713' : '\u2717'} Turn ${turn.turn_number}: ${turn.slogan}`);\n      });\n    }\n  } catch (error) {\n    console.error('Failed to generate slogan:', error);\n  }\n})();\n</code></pre>"},{"location":"guides/api-clients/#typescript-with-types","title":"TypeScript with Types","text":"<pre><code>interface GenerateRequest {\n  input: string;\n  model?: string;\n  max_turns?: number;\n  verbose?: boolean;\n}\n\ninterface TurnDetail {\n  turn_number: number;\n  slogan: string;\n  feedback: string | null;\n  approved: boolean;\n  timestamp: string;\n}\n\ninterface GenerateResponse {\n  slogan: string;\n  input: string;\n  completion_reason: 'approved' | 'max_turns' | 'error';\n  turn_count: number;\n  model_name: string;\n  total_duration_seconds: number;\n  average_duration_per_turn: number;\n  turns: TurnDetail[] | null;\n  created_at: string;\n  request_id: string | null;\n}\n\nclass SloganAPIClient {\n  private baseURL: string;\n\n  constructor(baseURL: string = 'http://localhost:8000') {\n    this.baseURL = baseURL;\n  }\n\n  async health(): Promise&lt;any&gt; {\n    const response = await fetch(`${this.baseURL}/api/v1/health`);\n    return await response.json();\n  }\n\n  async models(): Promise&lt;any&gt; {\n    const response = await fetch(`${this.baseURL}/api/v1/models`);\n    return await response.json();\n  }\n\n  async generate(request: GenerateRequest): Promise&lt;GenerateResponse&gt; {\n    const response = await fetch(`${this.baseURL}/api/v1/slogans/generate`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(request)\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n    }\n\n    return await response.json();\n  }\n}\n\n// Usage\nconst client = new SloganAPIClient();\n\n(async () =&gt; {\n  // Check health\n  const health = await client.health();\n  console.log('API Status:', health.status);\n\n  // Generate slogan\n  const result = await client.generate({\n    input: 'eco-friendly products',\n    verbose: true\n  });\n\n  console.log('Slogan:', result.slogan);\n})();\n</code></pre>"},{"location":"guides/api-clients/#using-axios","title":"Using axios","text":""},{"location":"guides/api-clients/#installation_2","title":"Installation","text":"<pre><code>npm install axios\n</code></pre>"},{"location":"guides/api-clients/#example-with-interceptors","title":"Example with Interceptors","text":"<pre><code>const axios = require('axios');\n\n// Create axios instance\nconst api = axios.create({\n  baseURL: 'http://localhost:8000',\n  timeout: 620000,  // 620 seconds\n  headers: {\n    'Content-Type': 'application/json'\n  }\n});\n\n// Request interceptor (add request ID)\napi.interceptors.request.use(\n  (config) =&gt; {\n    config.headers['X-Request-ID'] = crypto.randomUUID();\n    console.log(`\u2192 ${config.method.toUpperCase()} ${config.url}`);\n    return config;\n  },\n  (error) =&gt; Promise.reject(error)\n);\n\n// Response interceptor (logging)\napi.interceptors.response.use(\n  (response) =&gt; {\n    console.log(`\u2190 ${response.status} (${response.headers['x-request-id']})`);\n    return response;\n  },\n  (error) =&gt; {\n    if (error.response) {\n      console.error(`\u2717 ${error.response.status}: ${error.response.statusText}`);\n    } else {\n      console.error(`\u2717 Network error: ${error.message}`);\n    }\n    return Promise.reject(error);\n  }\n);\n\n// Generate slogan\nasync function generateSlogan(input, options = {}) {\n  try {\n    const response = await api.post('/api/v1/slogans/generate', {\n      input,\n      ...options\n    });\n    return response.data;\n  } catch (error) {\n    if (error.response?.status === 503) {\n      throw new Error('API service unavailable');\n    } else if (error.response?.status === 422) {\n      throw new Error(`Validation error: ${JSON.stringify(error.response.data)}`);\n    }\n    throw error;\n  }\n}\n\n// Usage\n(async () =&gt; {\n  const result = await generateSlogan('coffee subscription', {\n    model: 'mistral:latest',\n    verbose: true\n  });\n  console.log('Slogan:', result.slogan);\n})();\n</code></pre>"},{"location":"guides/api-clients/#curl-examples","title":"cURL Examples","text":""},{"location":"guides/api-clients/#basic-request","title":"Basic Request","text":"<pre><code># Health check\ncurl http://localhost:8000/api/v1/health\n\n# List models\ncurl http://localhost:8000/api/v1/models\n\n# Generate slogan (basic)\ncurl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"eco-friendly water bottles\"}'\n</code></pre>"},{"location":"guides/api-clients/#verbose-mode","title":"Verbose Mode","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"smart home devices\",\n    \"verbose\": true\n  }' | jq .\n</code></pre>"},{"location":"guides/api-clients/#custom-model-and-turns","title":"Custom Model and Turns","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"organic skincare\",\n    \"model\": \"gemma2:2b\",\n    \"max_turns\": 3\n  }' | jq '{slogan: .slogan, turns: .turn_count}'\n</code></pre>"},{"location":"guides/api-clients/#with-custom-request-id","title":"With Custom Request ID","text":"<pre><code>REQUEST_ID=$(uuidgen)\ncurl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-Request-ID: $REQUEST_ID\" \\\n  -d '{\"input\": \"coffee roastery\"}' \\\n  | jq -r '.slogan'\n\necho \"Request ID: $REQUEST_ID\"\n</code></pre>"},{"location":"guides/api-clients/#save-response-to-file","title":"Save Response to File","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"digital art marketplace\", \"verbose\": true}' \\\n  -o response.json\n\n# Pretty print\njq . response.json\n</code></pre>"},{"location":"guides/api-clients/#error-handling_1","title":"Error Handling","text":"<pre><code># Check exit code\ncurl -f -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"test\"}' \\\n  || echo \"Request failed with exit code $?\"\n\n# Show error details\ncurl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"\"}' \\\n  -w \"\\nHTTP Status: %{http_code}\\n\" \\\n  -s | jq .\n</code></pre>"},{"location":"guides/api-clients/#shell-script-example","title":"Shell Script Example","text":""},{"location":"guides/api-clients/#batch-slogan-generation","title":"Batch Slogan Generation","text":"<pre><code>#!/bin/bash\n\n# batch_generate.sh - Generate slogans for multiple products\n\nAPI_URL=\"http://localhost:8000/api/v1/slogans/generate\"\nOUTPUT_DIR=\"./slogans\"\nMODEL=\"mistral:latest\"\nMAX_TURNS=5\n\n# Create output directory\nmkdir -p \"$OUTPUT_DIR\"\n\n# Products list\nproducts=(\n  \"eco-friendly water bottles\"\n  \"smart home devices\"\n  \"organic coffee\"\n  \"fitness tracker\"\n  \"cloud storage\"\n)\n\n# Check API health\necho \"Checking API health...\"\nhealth_status=$(curl -s http://localhost:8000/api/v1/health | jq -r '.status')\n\nif [ \"$health_status\" != \"healthy\" ]; then\n  echo \"Error: API is not healthy (status: $health_status)\"\n  exit 1\nfi\n\necho \"API is healthy. Starting batch generation...\"\necho\n\n# Generate slogans\nfor product in \"${products[@]}\"; do\n  echo \"Generating slogan for: $product\"\n\n  # Generate filename\n  filename=\"${OUTPUT_DIR}/$(echo \"$product\" | tr ' ' '_').json\"\n\n  # Make request\n  response=$(curl -s -X POST \"$API_URL\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"input\\\": \\\"$product\\\",\n      \\\"model\\\": \\\"$MODEL\\\",\n      \\\"max_turns\\\": $MAX_TURNS,\n      \\\"verbose\\\": true\n    }\")\n\n  # Check if successful\n  if echo \"$response\" | jq -e '.slogan' &gt; /dev/null 2&gt;&amp;1; then\n    # Save to file\n    echo \"$response\" | jq . &gt; \"$filename\"\n\n    # Extract slogan\n    slogan=$(echo \"$response\" | jq -r '.slogan')\n    turns=$(echo \"$response\" | jq -r '.turn_count')\n\n    echo \"  \u2713 Slogan: $slogan\"\n    echo \"  \u2713 Turns: $turns\"\n    echo \"  \u2713 Saved to: $filename\"\n  else\n    echo \"  \u2717 Failed: $response\"\n  fi\n\n  echo\ndone\n\necho \"Batch generation complete!\"\necho \"Results saved in: $OUTPUT_DIR\"\n</code></pre>"},{"location":"guides/api-clients/#go-example","title":"Go Example","text":"<pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"time\"\n)\n\ntype GenerateRequest struct {\n    Input    string `json:\"input\"`\n    Model    string `json:\"model,omitempty\"`\n    MaxTurns int    `json:\"max_turns,omitempty\"`\n    Verbose  bool   `json:\"verbose,omitempty\"`\n}\n\ntype GenerateResponse struct {\n    Slogan                 string  `json:\"slogan\"`\n    Input                  string  `json:\"input\"`\n    CompletionReason       string  `json:\"completion_reason\"`\n    TurnCount              int     `json:\"turn_count\"`\n    ModelName              string  `json:\"model_name\"`\n    TotalDurationSeconds   float64 `json:\"total_duration_seconds\"`\n    AverageDurationPerTurn float64 `json:\"average_duration_per_turn\"`\n    CreatedAt              string  `json:\"created_at\"`\n    RequestID              string  `json:\"request_id\"`\n}\n\ntype SloganClient struct {\n    BaseURL    string\n    HTTPClient *http.Client\n}\n\nfunc NewSloganClient(baseURL string) *SloganClient {\n    return &amp;SloganClient{\n        BaseURL: baseURL,\n        HTTPClient: &amp;http.Client{\n            Timeout: 620 * time.Second,\n        },\n    }\n}\n\nfunc (c *SloganClient) Generate(req GenerateRequest) (*GenerateResponse, error) {\n    // Marshal request\n    body, err := json.Marshal(req)\n    if err != nil {\n        return nil, fmt.Errorf(\"marshal request: %w\", err)\n    }\n\n    // Create HTTP request\n    httpReq, err := http.NewRequest(\n        \"POST\",\n        c.BaseURL+\"/api/v1/slogans/generate\",\n        bytes.NewBuffer(body),\n    )\n    if err != nil {\n        return nil, fmt.Errorf(\"create request: %w\", err)\n    }\n\n    httpReq.Header.Set(\"Content-Type\", \"application/json\")\n\n    // Send request\n    resp, err := c.HTTPClient.Do(httpReq)\n    if err != nil {\n        return nil, fmt.Errorf(\"send request: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    // Read response\n    respBody, err := io.ReadAll(resp.Body)\n    if err != nil {\n        return nil, fmt.Errorf(\"read response: %w\", err)\n    }\n\n    // Check status\n    if resp.StatusCode != http.StatusOK {\n        return nil, fmt.Errorf(\"API error %d: %s\", resp.StatusCode, string(respBody))\n    }\n\n    // Parse response\n    var result GenerateResponse\n    if err := json.Unmarshal(respBody, &amp;result); err != nil {\n        return nil, fmt.Errorf(\"unmarshal response: %w\", err)\n    }\n\n    return &amp;result, nil\n}\n\nfunc main() {\n    client := NewSloganClient(\"http://localhost:8000\")\n\n    result, err := client.Generate(GenerateRequest{\n        Input:   \"eco-friendly water bottles\",\n        Verbose: false,\n    })\n    if err != nil {\n        fmt.Printf(\"Error: %v\\n\", err)\n        return\n    }\n\n    fmt.Printf(\"Slogan: %s\\n\", result.Slogan)\n    fmt.Printf(\"Turns: %d\\n\", result.TurnCount)\n    fmt.Printf(\"Duration: %.2fs\\n\", result.TotalDurationSeconds)\n}\n</code></pre>"},{"location":"guides/api-clients/#best-practices","title":"Best Practices","text":""},{"location":"guides/api-clients/#1-always-check-health-first","title":"1. Always Check Health First","text":"<pre><code># Python\ndef ensure_api_ready(client):\n    health = client.get(\"/api/v1/health\").json()\n    if health[\"status\"] != \"healthy\":\n        raise RuntimeError(\"API not ready\")\n</code></pre> <pre><code># Shell\nif [ \"$(curl -s http://localhost:8000/api/v1/health | jq -r '.status')\" != \"healthy\" ]; then\n  echo \"API not ready\"\n  exit 1\nfi\n</code></pre>"},{"location":"guides/api-clients/#2-use-appropriate-timeouts","title":"2. Use Appropriate Timeouts","text":"<pre><code># Slightly longer than server timeout (600s)\nclient = httpx.Client(timeout=620.0)\n</code></pre>"},{"location":"guides/api-clients/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>try {\n  const result = await generateSlogan(input);\n} catch (error) {\n  if (error.response?.status === 503) {\n    console.error('Service unavailable. Is Ollama running?');\n  } else if (error.response?.status === 422) {\n    console.error('Validation error:', error.response.data);\n  } else {\n    console.error('Unexpected error:', error);\n  }\n}\n</code></pre>"},{"location":"guides/api-clients/#4-use-request-ids-for-tracking","title":"4. Use Request IDs for Tracking","text":"<pre><code>import uuid\n\nrequest_id = str(uuid.uuid4())\nresponse = client.post(\n    url,\n    json={\"input\": text},\n    headers={\"X-Request-ID\": request_id}\n)\n# Log request_id for debugging\n</code></pre>"},{"location":"guides/api-clients/#5-retry-with-backoff","title":"5. Retry with Backoff","text":"<pre><code>import time\n\nfor attempt in range(3):\n    try:\n        result = client.post(url, json=data)\n        break\n    except ConnectionError:\n        if attempt &lt; 2:\n            time.sleep(2 ** attempt)  # 1s, 2s, 4s\n            continue\n        raise\n</code></pre>"},{"location":"guides/api-clients/#see-also","title":"See Also","text":"<ul> <li>REST API Reference - API endpoint documentation</li> <li>OpenAPI Spec - Interactive API documentation</li> <li>API Usage Guide - Complete API usage guide</li> <li>Development Guide - API development setup</li> </ul>"},{"location":"guides/api-usage/","title":"API Usage Guide","text":"<p>Complete reference for using the Slogan Writer-Reviewer REST API for programmatic slogan generation.</p>"},{"location":"guides/api-usage/#overview","title":"Overview","text":"<p>The FastAPI REST API provides HTTP endpoints for integrating slogan generation into web applications, services, and automation workflows. It offers the same Writer-Reviewer collaboration functionality as the CLI, accessible via HTTP requests.</p> <p>Base URL: <code>http://localhost:8000</code> (default local development)</p>"},{"location":"guides/api-usage/#quick-start","title":"Quick Start","text":""},{"location":"guides/api-usage/#starting-the-api-server","title":"Starting the API Server","text":"<p>Development Mode (with auto-reload):</p> <pre><code>uvicorn src.api.main:app --reload\n</code></pre> <p>Production Mode (with multiple workers):</p> <pre><code>uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4\n</code></pre> <p>With Custom Configuration:</p> <pre><code>export API_CORS_ORIGINS=\"https://myapp.com\"\nexport API_LOG_LEVEL=\"INFO\"\nexport API_GENERATION_TIMEOUT=300\n\nuvicorn src.api.main:app --host 0.0.0.0 --port 8000\n</code></pre> <p>The API will be available at:</p> <ul> <li>Local: <code>http://localhost:8000</code></li> <li>Network: <code>http://&lt;your-local-ip&gt;:8000</code></li> </ul>"},{"location":"guides/api-usage/#interactive-documentation","title":"Interactive Documentation","text":"<p>FastAPI automatically generates interactive API documentation:</p>"},{"location":"guides/api-usage/#swagger-ui","title":"Swagger UI","text":"<p>URL: http://localhost:8000/docs</p> <p>Features: - Interactive endpoint testing - Request/response examples - Schema validation - \"Try it out\" functionality</p> <p></p>"},{"location":"guides/api-usage/#redoc","title":"ReDoc","text":"<p>URL: http://localhost:8000/redoc</p> <p>Features: - Clean, readable documentation - Schema explorer - Code samples - Printable format</p>"},{"location":"guides/api-usage/#openapi-specification","title":"OpenAPI Specification","text":"<p>URL: http://localhost:8000/openapi.json</p> <p>The complete OpenAPI 3.0 specification in JSON format.</p> <p>Export for Code Generation:</p> <pre><code># Export OpenAPI spec\ncurl http://localhost:8000/openapi.json &gt; docs/openapi.json\n\n# Use with code generators\nnpx @openapitools/openapi-generator-cli generate \\\n  -i docs/openapi.json \\\n  -g typescript-fetch \\\n  -o ./generated-client\n</code></pre>"},{"location":"guides/api-usage/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/api-usage/#root-endpoint","title":"Root Endpoint","text":"<p>Get API information and available endpoints.</p> <p>Request:</p> <pre><code>curl http://localhost:8000/\n</code></pre> <p>Response:</p> <pre><code>{\n  \"message\": \"AI Slogan Generator API\",\n  \"version\": \"1.0.0\",\n  \"endpoints\": {\n    \"health\": \"/api/v1/health\",\n    \"models\": \"/api/v1/models\",\n    \"generate\": \"/api/v1/slogans/generate\"\n  }\n}\n</code></pre>"},{"location":"guides/api-usage/#health-check","title":"Health Check","text":"<p>Verify API and Ollama connectivity.</p> <p>Endpoint: <code>GET /api/v1/health</code></p> <p>Request:</p> <pre><code>curl http://localhost:8000/api/v1/health\n</code></pre> <p>Response (Healthy):</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2025-10-22T10:30:00Z\",\n  \"ollama\": {\n    \"status\": \"connected\",\n    \"base_url\": \"http://localhost:11434/v1\"\n  }\n}\n</code></pre> <p>Response (Unhealthy):</p> <pre><code>{\n  \"status\": \"unhealthy\",\n  \"timestamp\": \"2025-10-22T10:30:00Z\",\n  \"ollama\": {\n    \"status\": \"disconnected\",\n    \"base_url\": \"http://localhost:11434/v1\",\n    \"error\": \"Connection refused\"\n  }\n}\n</code></pre> <p>Status Codes:</p> <ul> <li><code>200 OK</code>: API and Ollama healthy</li> <li><code>503 Service Unavailable</code>: Ollama unreachable</li> </ul>"},{"location":"guides/api-usage/#list-available-models","title":"List Available Models","text":"<p>Get list of installed Ollama models.</p> <p>Endpoint: <code>GET /api/v1/models</code></p> <p>Request:</p> <pre><code>curl http://localhost:8000/api/v1/models\n</code></pre> <p>Response:</p> <pre><code>{\n  \"models\": [\n    {\n      \"id\": \"gemma2:2b\",\n      \"name\": \"Gemma 2 2B\",\n      \"size\": \"2B\",\n      \"description\": \"Fast and lightweight model\"\n    },\n    {\n      \"id\": \"mistral:latest\",\n      \"name\": \"Mistral 7B\",\n      \"size\": \"7B\",\n      \"description\": \"Fast and capable instruction-following model\"\n    },\n    {\n      \"id\": \"llama3.2:latest\",\n      \"name\": \"Llama 3.2\",\n      \"size\": \"8B\",\n      \"description\": \"Latest Llama model with improved capabilities\"\n    }\n  ],\n  \"total\": 3,\n  \"default_model\": \"mistral:latest\"\n}\n</code></pre> <p>Status Codes:</p> <ul> <li><code>200 OK</code>: Models retrieved successfully</li> <li><code>503 Service Unavailable</code>: Cannot connect to Ollama</li> </ul>"},{"location":"guides/api-usage/#generate-slogan","title":"Generate Slogan","text":"<p>Generate a slogan through Writer-Reviewer collaboration.</p> <p>Endpoint: <code>POST /api/v1/slogans/generate</code></p>"},{"location":"guides/api-usage/#request","title":"Request","text":"<p>Headers:</p> <pre><code>Content-Type: application/json\n</code></pre> <p>Body Schema:</p> <pre><code>{\n  \"input\": \"string (3-200 chars)\",\n  \"model\": \"string (optional)\",\n  \"max_turns\": \"integer (1-10, optional)\",\n  \"verbose\": \"boolean (optional)\"\n}\n</code></pre> <p>Parameters:</p> Field Type Required Default Constraints Description <code>input</code> string \u2705 Yes - 3-200 chars Product/service description <code>model</code> string No <code>mistral:latest</code> Must exist in Ollama Model to use <code>max_turns</code> integer No <code>5</code> 1-10 Max iteration rounds <code>verbose</code> boolean No <code>false</code> - Include turn details <p>Example Request:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"coffee shop\",\n    \"model\": \"mistral:latest\",\n    \"max_turns\": 5,\n    \"verbose\": false\n  }'\n</code></pre>"},{"location":"guides/api-usage/#response","title":"Response","text":"<p>Success (200 OK):</p> <pre><code>{\n  \"request_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"input\": \"coffee shop\",\n  \"final_slogan\": \"\u2615 Brew Happiness, One Cup at a Time\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 2,\n  \"max_turns\": 5,\n  \"total_duration_seconds\": 4.2,\n  \"average_duration_per_turn\": 2.1,\n  \"model_used\": \"mistral:latest\",\n  \"turns\": []\n}\n</code></pre> <p>With Verbose=true:</p> <pre><code>{\n  \"request_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"input\": \"coffee shop\",\n  \"final_slogan\": \"\u2615 Brew Happiness, One Cup at a Time\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 2,\n  \"max_turns\": 5,\n  \"total_duration_seconds\": 4.2,\n  \"average_duration_per_turn\": 2.1,\n  \"model_used\": \"mistral:latest\",\n  \"turns\": [\n    {\n      \"turn_number\": 1,\n      \"slogan\": \"Coffee Perfection in Every Cup\",\n      \"feedback\": \"Good start, but needs more emotional appeal...\",\n      \"approved\": false,\n      \"duration_seconds\": 2.1,\n      \"timestamp\": \"2025-10-22T10:30:00Z\"\n    },\n    {\n      \"turn_number\": 2,\n      \"slogan\": \"\u2615 Brew Happiness, One Cup at a Time\",\n      \"feedback\": \"SHIP IT! Perfect combination of warmth and clarity.\",\n      \"approved\": true,\n      \"duration_seconds\": 2.1,\n      \"timestamp\": \"2025-10-22T10:30:05Z\"\n    }\n  ]\n}\n</code></pre> <p>Response Fields:</p> Field Type Description <code>request_id</code> string Unique request identifier (UUID) <code>input</code> string Original input description <code>final_slogan</code> string Generated slogan (if successful) <code>completion_reason</code> string <code>approved</code>, <code>max_turns_reached</code>, or <code>error</code> <code>turn_count</code> integer Number of iterations performed <code>max_turns</code> integer Maximum turns allowed <code>total_duration_seconds</code> float Total generation time <code>average_duration_per_turn</code> float Average time per iteration <code>model_used</code> string Model identifier used <code>turns</code> array Turn-by-turn details (only if <code>verbose=true</code>) <code>error</code> string Error message (only if failed)"},{"location":"guides/api-usage/#error-responses","title":"Error Responses","text":"<p>Validation Error (422):</p> <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"input\"],\n      \"msg\": \"String should have at least 3 characters\",\n      \"type\": \"string_too_short\",\n      \"input\": \"ab\"\n    }\n  ]\n}\n</code></pre> <p>Model Not Found (400):</p> <pre><code>{\n  \"detail\": \"Model 'unknown' not found in Ollama. Available models: gemma2:2b, mistral:latest\",\n  \"status_code\": 400\n}\n</code></pre> <p>Generation Timeout (504):</p> <pre><code>{\n  \"detail\": \"Generation timeout: exceeded 600 seconds\",\n  \"status_code\": 504\n}\n</code></pre> <p>Internal Server Error (500):</p> <pre><code>{\n  \"detail\": \"An unexpected error occurred during generation\",\n  \"status_code\": 500\n}\n</code></pre> <p>Status Codes:</p> Code Meaning <code>200</code> Success - slogan generated <code>400</code> Bad Request - invalid model or parameters <code>422</code> Validation Error - request body invalid <code>500</code> Internal Server Error - unexpected failure <code>503</code> Service Unavailable - Ollama unreachable <code>504</code> Gateway Timeout - generation exceeded timeout"},{"location":"guides/api-usage/#client-examples","title":"Client Examples","text":""},{"location":"guides/api-usage/#python-httpx","title":"Python (httpx)","text":"<p>Basic Request:</p> <pre><code>import httpx\n\nresponse = httpx.post(\n    \"http://localhost:8000/api/v1/slogans/generate\",\n    json={\n        \"input\": \"coffee shop\",\n        \"model\": \"mistral:latest\",\n        \"max_turns\": 5\n    },\n    timeout=630.0  # 10.5 minutes (matches API timeout)\n)\n\nresult = response.json()\nprint(f\"Slogan: {result['final_slogan']}\")\nprint(f\"Took {result['turn_count']} turns in {result['total_duration_seconds']:.1f}s\")\n</code></pre> <p>With Error Handling:</p> <pre><code>import httpx\n\ndef generate_slogan(input_text: str, model: str = \"mistral:latest\") -&gt; dict:\n    \"\"\"Generate slogan with proper error handling.\"\"\"\n    try:\n        response = httpx.post(\n            \"http://localhost:8000/api/v1/slogans/generate\",\n            json={\n                \"input\": input_text,\n                \"model\": model,\n                \"max_turns\": 5,\n                \"verbose\": True\n            },\n            timeout=630.0\n        )\n        response.raise_for_status()\n        return response.json()\n\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 422:\n            print(f\"Validation error: {e.response.json()}\")\n        elif e.response.status_code == 400:\n            print(f\"Bad request: {e.response.json()['detail']}\")\n        elif e.response.status_code == 503:\n            print(\"Ollama service unavailable\")\n        else:\n            print(f\"HTTP error {e.response.status_code}: {e}\")\n        raise\n\n    except httpx.TimeoutException:\n        print(\"Request timed out\")\n        raise\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        raise\n\n# Usage\ntry:\n    result = generate_slogan(\"eco-friendly water bottle\")\n    print(result[\"final_slogan\"])\nexcept Exception:\n    print(\"Failed to generate slogan\")\n</code></pre> <p>Async Version:</p> <pre><code>import httpx\nimport asyncio\n\nasync def generate_slogan_async(input_text: str) -&gt; dict:\n    \"\"\"Async slogan generation.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8000/api/v1/slogans/generate\",\n            json={\"input\": input_text, \"max_turns\": 5},\n            timeout=630.0\n        )\n        response.raise_for_status()\n        return response.json()\n\n# Usage\nresult = asyncio.run(generate_slogan_async(\"coffee shop\"))\nprint(result[\"final_slogan\"])\n</code></pre>"},{"location":"guides/api-usage/#javascript-fetch-api","title":"JavaScript (Fetch API)","text":"<p>Basic Request:</p> <pre><code>async function generateSlogan(input, model = 'mistral:latest') {\n  const response = await fetch('http://localhost:8000/api/v1/slogans/generate', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n      input: input,\n      model: model,\n      max_turns: 5,\n      verbose: false\n    })\n  });\n\n  if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n  }\n\n  const result = await response.json();\n  return result;\n}\n\n// Usage\ngenerateSlogan('coffee shop')\n  .then(result =&gt; {\n    console.log(`Slogan: ${result.final_slogan}`);\n    console.log(`Took ${result.turn_count} turns`);\n  })\n  .catch(error =&gt; console.error('Error:', error));\n</code></pre> <p>With Error Handling:</p> <pre><code>async function generateSloganSafe(input) {\n  try {\n    const response = await fetch('http://localhost:8000/api/v1/slogans/generate', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ input, max_turns: 5 })\n    });\n\n    if (response.status === 422) {\n      const error = await response.json();\n      console.error('Validation error:', error.detail);\n      return null;\n    }\n\n    if (response.status === 503) {\n      console.error('Ollama service unavailable');\n      return null;\n    }\n\n    if (!response.ok) {\n      throw new Error(`HTTP ${response.status}`);\n    }\n\n    return await response.json();\n  } catch (error) {\n    console.error('Request failed:', error);\n    return null;\n  }\n}\n\n// Usage\nconst result = await generateSloganSafe('eco-friendly water bottle');\nif (result) {\n  console.log(result.final_slogan);\n}\n</code></pre>"},{"location":"guides/api-usage/#curl","title":"cURL","text":"<p>Basic Generation:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"coffee shop\",\n    \"max_turns\": 5\n  }'\n</code></pre> <p>With All Options:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"eco-friendly water bottle\",\n    \"model\": \"mistral:latest\",\n    \"max_turns\": 7,\n    \"verbose\": true\n  }' | jq '.'\n</code></pre> <p>Save Response to File:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"coffee shop\"}' \\\n  -o slogan-result.json\n</code></pre>"},{"location":"guides/api-usage/#configuration","title":"Configuration","text":""},{"location":"guides/api-usage/#environment-variables","title":"Environment Variables","text":"<p>Configure the API using environment variables:</p> Variable Type Default Description <code>API_CORS_ORIGINS</code> string <code>http://localhost:3000,http://localhost:8080</code> Comma-separated allowed origins <code>API_GENERATION_TIMEOUT</code> integer <code>600</code> Max generation time (seconds) <code>API_REQUEST_TIMEOUT</code> integer <code>630</code> Total request timeout (seconds) <code>API_LOG_LEVEL</code> string <code>WARNING</code> Logging level (DEBUG/INFO/WARNING/ERROR) <code>API_MAX_CONCURRENT_REQUESTS</code> integer <code>10</code> Max simultaneous generations"},{"location":"guides/api-usage/#configuration-examples","title":"Configuration Examples","text":"<p>Development (.env file):</p> <pre><code># .env\nAPI_CORS_ORIGINS=http://localhost:3000,http://localhost:5173\nAPI_LOG_LEVEL=DEBUG\nAPI_GENERATION_TIMEOUT=300\nAPI_MAX_CONCURRENT_REQUESTS=5\n</code></pre> <p>Production:</p> <pre><code>export API_CORS_ORIGINS=\"https://myapp.com,https://www.myapp.com\"\nexport API_LOG_LEVEL=INFO\nexport API_GENERATION_TIMEOUT=600\nexport API_REQUEST_TIMEOUT=630\nexport API_MAX_CONCURRENT_REQUESTS=20\n\nuvicorn src.api.main:app --host 0.0.0.0 --port 8000 --workers 4\n</code></pre> <p>Load Configuration:</p> <pre><code># From .env file\nuvicorn src.api.main:app --env-file .env\n\n# From environment\nset -a &amp;&amp; source config.env &amp;&amp; set +a\nuvicorn src.api.main:app\n</code></pre>"},{"location":"guides/api-usage/#cors-configuration","title":"CORS Configuration","text":""},{"location":"guides/api-usage/#understanding-cors","title":"Understanding CORS","text":"<p>Cross-Origin Resource Sharing (CORS) allows web applications on different domains to access your API.</p> <p>Default Allowed Origins:</p> <ul> <li><code>http://localhost:3000</code> (React, Next.js)</li> <li><code>http://localhost:8080</code> (Vue.js)</li> </ul>"},{"location":"guides/api-usage/#configuring-cors","title":"Configuring CORS","text":"<p>Allow Specific Origins:</p> <pre><code>export API_CORS_ORIGINS=\"https://myapp.com,https://staging.myapp.com\"\n</code></pre> <p>Allow All Origins (Development Only!):</p> <pre><code>export API_CORS_ORIGINS=\"*\"\n</code></pre> <p>Security Warning</p> <p>Never use <code>API_CORS_ORIGINS=\"*\"</code> in production. Always specify exact origins.</p>"},{"location":"guides/api-usage/#testing-cors","title":"Testing CORS","text":"<pre><code># Preflight request\ncurl -X OPTIONS http://localhost:8000/api/v1/slogans/generate \\\n  -H \"Origin: https://myapp.com\" \\\n  -H \"Access-Control-Request-Method: POST\" \\\n  -v\n</code></pre>"},{"location":"guides/api-usage/#rate-limiting-concurrency","title":"Rate Limiting &amp; Concurrency","text":"<p>The API limits concurrent requests to prevent resource exhaustion:</p> <ul> <li>Default: 10 concurrent requests</li> <li>Configurable: <code>API_MAX_CONCURRENT_REQUESTS</code></li> </ul> <p>When Limit Reached:</p> <pre><code>{\n  \"detail\": \"Too many concurrent requests. Please try again later.\",\n  \"status_code\": 503\n}\n</code></pre>"},{"location":"guides/api-usage/#request-tracking","title":"Request Tracking","text":"<p>Every request receives a unique <code>X-Request-ID</code> header for debugging and logging:</p> <p>Request:</p> <pre><code>curl -i http://localhost:8000/api/v1/health\n</code></pre> <p>Response Headers:</p> <pre><code>HTTP/1.1 200 OK\nX-Request-ID: 550e8400-e29b-41d4-a716-446655440000\nContent-Type: application/json\n</code></pre> <p>Use this ID when reporting issues or tracking requests in logs.</p>"},{"location":"guides/api-usage/#dev-tunnels-public-access","title":"Dev Tunnels (Public Access)","text":"<p>Expose your local API to the internet for testing and sharing.</p>"},{"location":"guides/api-usage/#setup-steps","title":"Setup Steps","text":"<p>1. Install Dev Tunnels:</p> <pre><code># macOS (Homebrew)\nbrew install devtunnel\n\n# Or download from Microsoft\n</code></pre> <p>2. Authenticate:</p> <pre><code>devtunnel user login\n</code></pre> <p>3. Create Tunnel:</p> <pre><code>devtunnel create -a\n# Note the tunnel name, e.g., \"fancy-fog-6mp2hnq\"\n</code></pre> <p>4. Create Port Mapping:</p> <pre><code>devtunnel port create -p 8000\n</code></pre> <p>5. Start Hosting:</p> <pre><code># In terminal 1\ndevtunnel host fancy-fog-6mp2hnq\n\n# In terminal 2\nuvicorn src.api.main:app --reload\n</code></pre> <p>6. Access Public URL:</p> <p>You'll receive a URL like: <code>https://f61krm0p-8000.euw.devtunnels.ms</code></p> <p>7. Test Public Endpoint:</p> <pre><code>curl https://your-tunnel-url.devtunnels.ms/api/v1/health\n\ncurl -X POST https://your-tunnel-url.devtunnels.ms/api/v1/slogans/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"coffee shop\"}'\n</code></pre> <p>Development Only</p> <p>Dev Tunnels are for development and testing. For production, use proper hosting (Azure, AWS, etc.).</p>"},{"location":"guides/api-usage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/api-usage/#timeouts","title":"Timeouts","text":"<ul> <li>Generation Timeout: 600s (10 minutes) - max time for slogan generation</li> <li>Request Timeout: 630s (10.5 minutes) - total request time including overhead</li> </ul> <p>Adjust for Large Models:</p> <pre><code>export API_GENERATION_TIMEOUT=900  # 15 minutes\nexport API_REQUEST_TIMEOUT=930     # 15.5 minutes\n</code></pre>"},{"location":"guides/api-usage/#concurrent-requests","title":"Concurrent Requests","text":"<ul> <li>Default Limit: 10 concurrent generations</li> <li>Recommendation: 5-10 for CPU, 20+ for GPU</li> </ul> <pre><code>export API_MAX_CONCURRENT_REQUESTS=5  # Conservative\n</code></pre>"},{"location":"guides/api-usage/#model-selection","title":"Model Selection","text":"<p>Choose models based on throughput needs:</p> Model Speed Concurrent Capacity (CPU) Use Case <code>gemma2:2b</code> \u26a1\u26a1\u26a1 10-15 requests High throughput <code>mistral:latest</code> \u26a1 5-10 requests Balanced <code>llama3.2:latest</code> \ud83d\udc0c 2-5 requests Quality focused"},{"location":"guides/api-usage/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"guides/api-usage/#log-levels","title":"Log Levels","text":"<p>Control API logging verbosity:</p> <pre><code>export API_LOG_LEVEL=DEBUG  # All messages\nexport API_LOG_LEVEL=INFO   # General info\nexport API_LOG_LEVEL=WARNING  # Warnings and errors (default)\nexport API_LOG_LEVEL=ERROR  # Errors only\n</code></pre>"},{"location":"guides/api-usage/#log-format","title":"Log Format","text":"<pre><code>2025-10-22 10:30:00 INFO     [550e8400] POST /api/v1/slogans/generate\n2025-10-22 10:30:05 INFO     [550e8400] Generation complete: 2 turns, 4.2s\n</code></pre>"},{"location":"guides/api-usage/#health-monitoring","title":"Health Monitoring","text":"<p>Automated Health Checks:</p> <pre><code># Every 30 seconds\nwhile true; do\n  curl -s http://localhost:8000/api/v1/health | jq '.status'\n  sleep 30\ndone\n</code></pre>"},{"location":"guides/api-usage/#see-also","title":"See Also","text":"<ul> <li>CLI Usage Guide - Command-line alternative</li> <li>Configuration Guide - Environment variables and settings</li> <li>Troubleshooting Guide - Common API issues</li> <li>Development Guide - Contributing and extending the API</li> </ul>"},{"location":"guides/cli-usage/","title":"CLI Usage Guide","text":"<p>Complete reference for using the Slogan Writer-Reviewer Agent System command-line interface.</p>"},{"location":"guides/cli-usage/#overview","title":"Overview","text":"<p>The CLI provides an intuitive interface for generating slogans through multi-agent collaboration. All commands are accessed through the <code>slogan-gen</code> command.</p> <pre><code>slogan-gen --help\n</code></pre>"},{"location":"guides/cli-usage/#command-structure","title":"Command Structure","text":"<pre><code>slogan-gen [COMMAND] [OPTIONS] [ARGUMENTS]\n</code></pre> <p>Available Commands:</p> Command Description <code>generate</code> Generate a slogan (main command) <code>models</code> List available Ollama models <code>config show</code> Display current configuration <code>config set</code> Set configuration values"},{"location":"guides/cli-usage/#generate-command","title":"Generate Command","text":"<p>Generate creative slogans through Writer-Reviewer collaboration.</p>"},{"location":"guides/cli-usage/#basic-syntax","title":"Basic Syntax","text":"<pre><code>slogan-gen generate \"your product or service description\"\n</code></pre>"},{"location":"guides/cli-usage/#options","title":"Options","text":"Option Short Type Default Description <code>--model</code> string <code>mistral:latest</code> Ollama model to use <code>--max-turns</code> integer 5 Maximum iteration turns (1-10) <code>--verbose</code> <code>-v</code> flag false Show detailed iteration history <code>--output</code> <code>-o</code> path Save results to file (.txt or .json)"},{"location":"guides/cli-usage/#examples","title":"Examples","text":""},{"location":"guides/cli-usage/#basic-generation","title":"Basic Generation","text":"<pre><code>slogan-gen generate \"eco-friendly water bottle\"\n</code></pre> <p>Output: <pre><code>\ud83d\ude80 Generating slogan for: eco-friendly water bottle\n   Using model: mistral:latest\n\n\u2705 Final Slogan:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n   \ud83d\udca7 Pure Hydration, Zero Waste!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2713 Approved by Reviewer in 2 turns\n\u23f1\ufe0f  Total duration: 5.8 seconds\n</code></pre></p>"},{"location":"guides/cli-usage/#verbose-mode","title":"Verbose Mode","text":"<p>See the complete Writer-Reviewer collaboration process:</p> <pre><code>slogan-gen generate \"tech startup\" --verbose\n</code></pre> <p>Output: <pre><code>\ud83d\ude80 Generating slogan for: tech startup\n   Using model: mistral:latest\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nTurn 1/5\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\ud83d\udcdd Writer's Slogan:\n   Innovation at the Speed of Tomorrow\n\n\ud83d\udcad Reviewer's Feedback:\n   Good foundation, but it's too generic. Can we make it more\n   specific to what the startup does? Add more impact.\n\nDuration: 2.3s\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nTurn 2/5\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\ud83d\udcdd Writer's Slogan:\n   \ud83d\ude80 Building Tomorrow's Tech, Today!\n\n\ud83d\udcad Reviewer's Feedback:\n   SHIP IT! Perfect combination of emoji, energy, and clarity!\n\nDuration: 2.8s\n\n\u2705 Final Slogan:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n   \ud83d\ude80 Building Tomorrow's Tech, Today!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2713 Approved by Reviewer in 2 turns\n\u23f1\ufe0f  Total duration: 5.1 seconds (avg 2.6s per turn)\n</code></pre></p>"},{"location":"guides/cli-usage/#custom-model","title":"Custom Model","text":"<p>Use a different Ollama model:</p> <pre><code># Fast generation with smaller model\nslogan-gen generate \"coffee shop\" --model gemma2:2b\n\n# High quality with larger model\nslogan-gen generate \"coffee shop\" --model llama3.2:latest\n</code></pre>"},{"location":"guides/cli-usage/#custom-iteration-limit","title":"Custom Iteration Limit","text":"<p>Control how many refinement turns are allowed:</p> <pre><code># Quick generation (3 turns max)\nslogan-gen generate \"fitness app\" --max-turns 3\n\n# Extensive refinement (10 turns max)\nslogan-gen generate \"luxury brand\" --max-turns 10\n</code></pre>"},{"location":"guides/cli-usage/#save-to-file","title":"Save to File","text":"<p>Text Format:</p> <pre><code>slogan-gen generate \"pizza restaurant\" --output result.txt\n</code></pre> <p>Creates <code>result.txt</code> with the formatted output.</p> <p>JSON Format:</p> <pre><code>slogan-gen generate \"pizza restaurant\" --output result.json\n</code></pre> <p>Creates <code>result.json</code>:</p> <pre><code>{\n  \"input\": \"pizza restaurant\",\n  \"final_slogan\": \"\ud83c\udf55 Slice of Heaven, Every Bite!\",\n  \"completion_reason\": \"approved\",\n  \"turn_count\": 2,\n  \"max_turns\": 5,\n  \"total_duration_seconds\": 5.8,\n  \"average_duration_per_turn\": 2.9,\n  \"turns\": [\n    {\n      \"turn_number\": 1,\n      \"slogan\": \"Pizza Perfection in Every Slice\",\n      \"feedback\": \"Good start, but needs more excitement...\",\n      \"approved\": false,\n      \"duration_seconds\": 2.9,\n      \"timestamp\": \"2024-01-15T10:30:00\"\n    },\n    {\n      \"turn_number\": 2,\n      \"slogan\": \"\ud83c\udf55 Slice of Heaven, Every Bite!\",\n      \"feedback\": \"SHIP IT! Perfect combination of emoji and excitement.\",\n      \"approved\": true,\n      \"duration_seconds\": 2.9,\n      \"timestamp\": \"2024-01-15T10:30:05\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/cli-usage/#combined-options","title":"Combined Options","text":"<pre><code>slogan-gen generate \"AI assistant\" \\\n  --model mistral:latest \\\n  --max-turns 7 \\\n  --verbose \\\n  --output results.json\n</code></pre>"},{"location":"guides/cli-usage/#models-command","title":"Models Command","text":"<p>List and manage available Ollama models.</p>"},{"location":"guides/cli-usage/#basic-syntax_1","title":"Basic Syntax","text":"<pre><code>slogan-gen models\n</code></pre>"},{"location":"guides/cli-usage/#options_1","title":"Options","text":"Option Short Description <code>--refresh</code> <code>-r</code> Force refresh the model list"},{"location":"guides/cli-usage/#examples_1","title":"Examples","text":"<p>List Models:</p> <pre><code>slogan-gen models\n</code></pre> <p>Output: <pre><code>\ud83d\udce6 Available Ollama Models:\n\n  1. gemma2:2b\n  2. llama3.2:latest\n  3. mistral:latest (default)\n  4. phi3:mini\n\n\u2713 Total: 4 models\n\n\ud83d\udca1 Use with: slogan-gen generate \"your input\" --model &lt;model-name&gt;\n</code></pre></p> <p>Refresh Model List:</p> <pre><code>slogan-gen models --refresh\n</code></pre>"},{"location":"guides/cli-usage/#config-commands","title":"Config Commands","text":"<p>View and modify configuration settings.</p>"},{"location":"guides/cli-usage/#config-show","title":"Config Show","text":"<p>Display current configuration values.</p> <pre><code>slogan-gen config show\n</code></pre> <p>Output: <pre><code>\u2699\ufe0f  Current Configuration:\n============================================================\nOllama Base URL.............. http://localhost:11434\nDefault Model................ mistral:latest\nTemperature.................. 0.7 (range: 0.0-2.0)\nMax Tokens................... 500 (range: 1-4096)\nTimeout...................... 30s (range: 1-300)\nMax Turns.................... 5 (range: 1-10)\n============================================================\n\n\ud83d\udca1 To modify settings, set environment variables:\n   Example: export OLLAMA_MODEL_NAME=mistral:latest\n   Or create a .env file in your project directory\n</code></pre></p>"},{"location":"guides/cli-usage/#config-set","title":"Config Set","text":"<p>Set configuration values (temporary, for current session).</p>"},{"location":"guides/cli-usage/#syntax","title":"Syntax","text":"<pre><code>slogan-gen config set KEY VALUE\n</code></pre>"},{"location":"guides/cli-usage/#supported-keys","title":"Supported Keys","text":"Key Environment Variable Example Value <code>MODEL_NAME</code> / <code>MODEL</code> <code>OLLAMA_MODEL_NAME</code> <code>mistral:latest</code> <code>BASE_URL</code> / <code>URL</code> <code>OLLAMA_BASE_URL</code> <code>http://localhost:11434</code> <code>TEMPERATURE</code> / <code>TEMP</code> <code>OLLAMA_TEMPERATURE</code> <code>0.8</code> <code>MAX_TOKENS</code> / <code>TOKENS</code> <code>OLLAMA_MAX_TOKENS</code> <code>1000</code> <code>TIMEOUT</code> <code>OLLAMA_TIMEOUT</code> <code>60</code> <code>MAX_TURNS</code> / <code>TURNS</code> <code>OLLAMA_MAX_TURNS</code> <code>7</code>"},{"location":"guides/cli-usage/#examples_2","title":"Examples","text":"<p>Change Model:</p> <pre><code>slogan-gen config set MODEL mistral:latest\n</code></pre> <p>Adjust Temperature:</p> <pre><code>slogan-gen config set TEMPERATURE 0.9\n</code></pre> <p>Increase Timeout:</p> <pre><code>slogan-gen config set TIMEOUT 60\n</code></pre> <p>Temporary Changes</p> <p>Changes made with <code>config set</code> are temporary and only affect the current session. For persistent changes, add to your <code>.env</code> file or shell profile.</p>"},{"location":"guides/cli-usage/#input-guidelines","title":"Input Guidelines","text":""},{"location":"guides/cli-usage/#best-practices","title":"Best Practices","text":"<p>\u2705 Good Inputs: - Specific and descriptive - Include target audience if relevant - Mention key features or benefits</p> <pre><code>slogan-gen generate \"premium organic coffee roastery targeting health-conscious millennials\"\nslogan-gen generate \"eco-friendly cleaning products for environmentally conscious homeowners\"\nslogan-gen generate \"AI-powered project management tool for remote teams\"\n</code></pre> <p>\u274c Avoid: - Single generic words - Extremely long inputs (&gt;200 characters) - Empty or whitespace-only inputs</p> <pre><code># Too vague\nslogan-gen generate \"business\"\n\n# Too long (&gt;200 chars will be rejected)\nslogan-gen generate \"a very long description that goes on and on...\"\n</code></pre>"},{"location":"guides/cli-usage/#input-length","title":"Input Length","text":"<ul> <li>Minimum: 3 characters</li> <li>Maximum: 200 characters</li> <li>Recommended: 10-50 characters for best results</li> </ul>"},{"location":"guides/cli-usage/#output-formats","title":"Output Formats","text":""},{"location":"guides/cli-usage/#text-output-default","title":"Text Output (Default)","text":"<p>Human-readable format with colors and formatting:</p> <pre><code>\u2705 Final Slogan:\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n   Your Generated Slogan Here!\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u2713 Approved by Reviewer in 3 turns\n\u23f1\ufe0f  Total duration: 8.2 seconds\n</code></pre>"},{"location":"guides/cli-usage/#json-output","title":"JSON Output","text":"<p>Machine-readable format for programmatic use:</p> <pre><code>{\n  \"input\": \"string\",\n  \"final_slogan\": \"string\",\n  \"completion_reason\": \"approved|max_turns_reached\",\n  \"turn_count\": 0,\n  \"max_turns\": 5,\n  \"total_duration_seconds\": 0.0,\n  \"average_duration_per_turn\": 0.0,\n  \"turns\": [\n    {\n      \"turn_number\": 1,\n      \"slogan\": \"string\",\n      \"feedback\": \"string\",\n      \"approved\": false,\n      \"duration_seconds\": 0.0,\n      \"timestamp\": \"2024-01-15T10:30:00\"\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/cli-usage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/cli-usage/#model-selection","title":"Model Selection","text":"<p>Choose models based on your needs:</p> Model Size Speed Quality Use Case <code>gemma2:2b</code> 2B \u26a1\u26a1\u26a1 \u2b50\u2b50 Quick testing <code>phi3:mini</code> 3.8B \u26a1\u26a1 \u2b50\u2b50\u2b50 Development <code>mistral:latest</code> 7B \u26a1 \u2b50\u2b50\u2b50\u2b50 Production (default) <code>llama3.2:latest</code> 8B \ud83d\udc0c \u2b50\u2b50\u2b50\u2b50\u2b50 High quality"},{"location":"guides/cli-usage/#timing-expectations","title":"Timing Expectations","text":"<p>Typical generation times (2 turns, CPU-only):</p> <ul> <li>2B models: 5-10 seconds</li> <li>7B models: 15-30 seconds</li> <li>8B+ models: 60-120 seconds</li> </ul> <p>With GPU acceleration, times are significantly faster.</p>"},{"location":"guides/cli-usage/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use smaller models for testing: <code>gemma2:2b</code> or <code>phi3:mini</code></li> <li>Reduce max turns: <code>--max-turns 3</code> for quick results</li> <li>Ensure Ollama uses GPU: Check <code>ollama serve</code> output</li> <li>Close other applications: Free up system resources</li> </ol>"},{"location":"guides/cli-usage/#error-handling","title":"Error Handling","text":""},{"location":"guides/cli-usage/#common-errors","title":"Common Errors","text":"<p>Empty Input: <pre><code>\u274c Error: Input cannot be empty\n</code></pre></p> <p>Model Not Found: <pre><code>\u26a0\ufe0f  Warning: Model 'unknown' not found in available models.\n\nAvailable models: gemma2:2b, mistral:latest, phi3:mini\n\nContinue anyway? [y/N]: n\n\n\ud83d\udca1 To install the model, run: ollama pull unknown\n</code></pre></p> <p>Connection Error: <pre><code>\u274c Connection Error: Cannot connect to Ollama at http://localhost:11434\n\n\ud83d\udca1 Tips:\n   \u2022 Ensure Ollama is running: ollama serve\n   \u2022 Check if the model is available: ollama list\n   \u2022 Pull the model if needed: ollama pull mistral:latest\n</code></pre></p> <p>Validation Error: <pre><code>\u274c Validation Error: 2 validation errors for Turn\nslogan: String should have at most 500 characters\nfeedback: String should have at most 1000 characters\n</code></pre></p> <p>Solution: Use a larger, more capable model (see Troubleshooting Guide)</p>"},{"location":"guides/cli-usage/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 Error (validation, connection, runtime, etc.)"},{"location":"guides/cli-usage/#tips-best-practices","title":"Tips &amp; Best Practices","text":""},{"location":"guides/cli-usage/#1-start-with-verbose-mode","title":"1. Start with Verbose Mode","text":"<p>When learning the system, use <code>--verbose</code> to understand the collaboration process:</p> <pre><code>slogan-gen generate \"your input\" --verbose\n</code></pre>"},{"location":"guides/cli-usage/#2-experiment-with-models","title":"2. Experiment with Models","text":"<p>Try different models to find the best balance of speed and quality for your use case:</p> <pre><code>slogan-gen generate \"test\" --model gemma2:2b     # Fast\nslogan-gen generate \"test\" --model mistral       # Balanced\nslogan-gen generate \"test\" --model llama3.2      # Quality\n</code></pre>"},{"location":"guides/cli-usage/#3-adjust-iterations-based-on-quality","title":"3. Adjust Iterations Based on Quality","text":"<ul> <li>Quick brainstorming: <code>--max-turns 3</code></li> <li>Standard generation: Default (5 turns)</li> <li>High-quality output: <code>--max-turns 7</code> or more</li> </ul>"},{"location":"guides/cli-usage/#4-save-important-results","title":"4. Save Important Results","text":"<p>Always save results you want to keep:</p> <pre><code>slogan-gen generate \"product\" --output results/product-slogan.json\n</code></pre>"},{"location":"guides/cli-usage/#5-use-shell-aliases","title":"5. Use Shell Aliases","text":"<p>Create shortcuts for common tasks:</p> <pre><code># Add to ~/.zshrc or ~/.bashrc\nalias slogan='slogan-gen generate'\nalias slogan-fast='slogan-gen generate --model gemma2:2b --max-turns 3'\nalias slogan-quality='slogan-gen generate --model llama3.2 --max-turns 7 --verbose'\n\n# Usage\nslogan \"coffee shop\"\nslogan-fast \"quick test\"\nslogan-quality \"luxury brand\"\n</code></pre>"},{"location":"guides/cli-usage/#6-batch-processing","title":"6. Batch Processing","text":"<p>Generate multiple slogans using a shell loop:</p> <pre><code># Using a file with inputs\nwhile read -r line; do\n  slogan-gen generate \"$line\" --output \"results/${line// /_}.json\"\ndone &lt; inputs.txt\n</code></pre>"},{"location":"guides/cli-usage/#7-monitor-performance","title":"7. Monitor Performance","text":"<p>Use verbose mode to track timing:</p> <pre><code>slogan-gen generate \"test\" --verbose | grep \"Duration:\"\n</code></pre>"},{"location":"guides/cli-usage/#integration-examples","title":"Integration Examples","text":""},{"location":"guides/cli-usage/#shell-scripts","title":"Shell Scripts","text":"<pre><code>#!/bin/bash\n# generate_slogan.sh\n\nINPUT=\"$1\"\nOUTPUT=\"${2:-slogan.json}\"\nMODEL=\"${3:-mistral:latest}\"\n\nslogan-gen generate \"$INPUT\" \\\n  --model \"$MODEL\" \\\n  --max-turns 5 \\\n  --output \"$OUTPUT\"\n\necho \"Slogan saved to $OUTPUT\"\n</code></pre> <p>Usage: <pre><code>./generate_slogan.sh \"coffee shop\" \"results/coffee.json\" \"mistral:latest\"\n</code></pre></p>"},{"location":"guides/cli-usage/#python-integration","title":"Python Integration","text":"<pre><code>import subprocess\nimport json\n\ndef generate_slogan(input_text: str, model: str = \"mistral:latest\") -&gt; dict:\n    \"\"\"Generate slogan using CLI.\"\"\"\n    result = subprocess.run(\n        [\"slogan-gen\", \"generate\", input_text, \"--model\", model, \"--output\", \"-\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)\n\n# Usage\nslogan_data = generate_slogan(\"eco-friendly water bottle\")\nprint(slogan_data[\"final_slogan\"])\n</code></pre>"},{"location":"guides/cli-usage/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide - First-time usage tutorial</li> <li>Configuration Guide - Detailed configuration options</li> <li>API Usage Guide - REST API alternative</li> <li>Troubleshooting Guide - Common issues and solutions</li> </ul>"},{"location":"guides/development/","title":"Development Guide","text":"<p>Complete guide for developers contributing to the Slogan Writer-Reviewer Agent System.</p>"},{"location":"guides/development/#overview","title":"Overview","text":"<p>This guide covers setting up your development environment, running tests, maintaining code quality, and contributing to the project.</p> <p>Project Stack:</p> <ul> <li>Language: Python 3.11+</li> <li>Package Manager: uv (fast Python package installer)</li> <li>Framework: Microsoft Agent Framework + Ollama</li> <li>API: FastAPI</li> <li>CLI: Click</li> <li>Testing: pytest</li> <li>Code Quality: Ruff (linter + formatter), mypy (type checker)</li> </ul>"},{"location":"guides/development/#development-setup","title":"Development Setup","text":""},{"location":"guides/development/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have these installed:</p> <ul> <li>Python 3.11+: <code>python --version</code></li> <li>uv: Fast Python package manager (install guide)</li> <li>Ollama: Local LLM runtime (install guide)</li> <li>Git: Version control</li> </ul>"},{"location":"guides/development/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/your-org/microsoft-agent-framework-with-ollama.git\ncd microsoft-agent-framework-with-ollama-1\n</code></pre>"},{"location":"guides/development/#create-virtual-environment","title":"Create Virtual Environment","text":"<pre><code># Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # macOS/Linux\n.venv\\Scripts\\activate     # Windows\n</code></pre>"},{"location":"guides/development/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install in editable mode with dev dependencies\nuv pip install -e \".[dev]\"\n\n# Verify installation\nslogan-gen --version\n</code></pre> <p>Dependency Groups:</p> <ul> <li>Default: Core runtime dependencies</li> <li>dev: Development tools (pytest, ruff, mypy, coverage)</li> <li>docs: Documentation tools (mkdocs, mkdocs-material)</li> </ul> <p>Install All Dependencies:</p> <pre><code>uv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"guides/development/#setup-ollama","title":"Setup Ollama","text":"<pre><code># Ensure Ollama is running\nollama serve\n\n# Pull test model\nollama pull mistral:latest\n</code></pre>"},{"location":"guides/development/#verify-setup","title":"Verify Setup","text":"<pre><code># Check configuration\nslogan-gen config show\n\n# Test generation\nslogan-gen generate \"test\" --model mistral:latest\n\n# Run tests\npytest\n</code></pre>"},{"location":"guides/development/#project-structure","title":"Project Structure","text":"<pre><code>microsoft-agent-framework-with-ollama-1/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 agents/           # Writer and Reviewer agent implementations\n\u2502   \u2502   \u251c\u2500\u2500 writer.py     # Writer agent (generates slogans)\n\u2502   \u2502   \u2514\u2500\u2500 reviewer.py   # Reviewer agent (provides feedback)\n\u2502   \u251c\u2500\u2500 orchestration/    # Workflow coordination\n\u2502   \u2502   \u251c\u2500\u2500 workflow.py   # Main workflow logic\n\u2502   \u2502   \u2514\u2500\u2500 models.py     # Data models (Turn, Session)\n\u2502   \u251c\u2500\u2500 cli/              # Command-line interface\n\u2502   \u2502   \u251c\u2500\u2500 main.py       # CLI commands (generate, models, config)\n\u2502   \u2502   \u2514\u2500\u2500 output.py     # Output formatting\n\u2502   \u251c\u2500\u2500 api/              # REST API (FastAPI)\n\u2502   \u2502   \u251c\u2500\u2500 main.py       # FastAPI app and startup\n\u2502   \u2502   \u251c\u2500\u2500 routes/       # API endpoints (health, generate, models)\n\u2502   \u2502   \u251c\u2500\u2500 schemas/      # Request/response models\n\u2502   \u2502   \u251c\u2500\u2500 middleware.py # Request ID, CORS, error handling\n\u2502   \u2502   \u2514\u2500\u2500 exceptions.py # Custom exceptions\n\u2502   \u2514\u2500\u2500 config/           # Configuration management\n\u2502       \u2514\u2500\u2500 settings.py   # Pydantic settings (OllamaConfig)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/             # Unit tests\n\u2502   \u251c\u2500\u2500 integration/      # Integration tests\n\u2502   \u2514\u2500\u2500 api/              # API tests\n\u251c\u2500\u2500 docs/                 # MkDocs documentation\n\u251c\u2500\u2500 pyproject.toml        # Project configuration\n\u2514\u2500\u2500 README.md             # Main documentation\n</code></pre>"},{"location":"guides/development/#key-principles","title":"Key Principles","text":"<p>3-Layer Architecture:</p> <ol> <li>Agent Layer (<code>src/agents/</code>): AI agent implementations</li> <li>Orchestration Layer (<code>src/orchestration/</code>): Workflow coordination</li> <li>Interface Layer (<code>src/cli/</code>, <code>src/api/</code>): User interfaces</li> </ol> <p>Benefits:</p> <ul> <li>Separation of concerns</li> <li>Easy to add new interfaces (e.g., Web UI)</li> <li>Testable components</li> <li>Shared business logic</li> </ul>"},{"location":"guides/development/#development-workflow","title":"Development Workflow","text":""},{"location":"guides/development/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"guides/development/#2-make-changes","title":"2. Make Changes","text":"<p>Edit code, add tests, update documentation.</p>"},{"location":"guides/development/#3-run-code-quality-checks","title":"3. Run Code Quality Checks","text":"<pre><code># Format code\nruff format src/ tests/\n\n# Lint code\nruff check src/ tests/ --fix\n\n# Type check\nmypy src/\n</code></pre>"},{"location":"guides/development/#4-run-tests","title":"4. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# With coverage\npytest --cov=src --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\n</code></pre>"},{"location":"guides/development/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add your feature description\"\n</code></pre> <p>Commit Message Format:</p> <pre><code>&lt;type&gt;: &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types:</p> <ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>test</code>: Adding or updating tests</li> <li><code>refactor</code>: Code refactoring</li> <li><code>chore</code>: Maintenance tasks</li> </ul>"},{"location":"guides/development/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Create a Pull Request on GitHub.</p>"},{"location":"guides/development/#testing","title":"Testing","text":""},{"location":"guides/development/#running-tests","title":"Running Tests","text":"<p>All Tests:</p> <pre><code>pytest\n</code></pre> <p>Specific Test File:</p> <pre><code>pytest tests/unit/test_workflow.py\n</code></pre> <p>Specific Test:</p> <pre><code>pytest tests/unit/test_workflow.py::test_successful_approval\n</code></pre> <p>With Verbose Output:</p> <pre><code>pytest -v\n</code></pre> <p>With Print Statements:</p> <pre><code>pytest -s\n</code></pre>"},{"location":"guides/development/#test-categories","title":"Test Categories","text":"<p>Unit Tests (<code>tests/unit/</code>):</p> <ul> <li>Test individual components in isolation</li> <li>Fast, no external dependencies</li> <li>Mock Ollama responses</li> </ul> <pre><code># Example: tests/unit/test_workflow.py\ndef test_successful_approval(mock_ollama_client):\n    \"\"\"Test workflow completes when reviewer approves.\"\"\"\n    # ...\n</code></pre> <p>Integration Tests (<code>tests/integration/</code>):</p> <ul> <li>Test component interactions</li> <li>Require Ollama running</li> <li>Use real models</li> </ul> <pre><code># Example: tests/integration/test_end_to_end.py\nasync def test_end_to_end_generation():\n    \"\"\"Test complete slogan generation workflow.\"\"\"\n    # ...\n</code></pre> <p>API Tests (<code>tests/api/</code>):</p> <ul> <li>Test FastAPI endpoints</li> <li>Use TestClient</li> <li>Mock Ollama where needed</li> </ul> <pre><code># Example: tests/api/test_generate.py\ndef test_generate_slogan_success(client):\n    \"\"\"Test successful slogan generation via API.\"\"\"\n    response = client.post(\"/api/v1/slogans/generate\", json={...})\n    assert response.status_code == 200\n</code></pre>"},{"location":"guides/development/#coverage","title":"Coverage","text":"<p>Generate Coverage Report:</p> <pre><code>pytest --cov=src --cov-report=html --cov-report=term\n</code></pre> <p>View HTML Report:</p> <pre><code>open htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre> <p>Coverage Goals:</p> <ul> <li>Overall: &gt;80%</li> <li>Critical paths (agents, orchestration): &gt;90%</li> </ul>"},{"location":"guides/development/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"guides/development/#ruff-linter-formatter","title":"Ruff (Linter + Formatter)","text":"<p>Ruff is a fast Python linter and formatter (replaces Black, isort, flake8).</p> <p>Format Code:</p> <pre><code>ruff format src/ tests/\n</code></pre> <p>Lint Code:</p> <pre><code># Check for issues\nruff check src/ tests/\n\n# Auto-fix issues\nruff check src/ tests/ --fix\n</code></pre> <p>Configuration (<code>pyproject.toml</code>):</p> <pre><code>[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\nignore = [\"E501\"]  # Line too long (handled by formatter)\n</code></pre>"},{"location":"guides/development/#mypy-type-checker","title":"Mypy (Type Checker)","text":"<p>Mypy ensures type safety and catches type errors.</p> <p>Run Type Checks:</p> <pre><code>mypy src/\n</code></pre> <p>Configuration (<code>pyproject.toml</code>):</p> <pre><code>[tool.mypy]\npython_version = \"3.11\"\nstrict = true\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n</code></pre> <p>Common Type Issues:</p> <pre><code># \u274c Missing type annotations\ndef process_data(data):\n    return data\n\n# \u2705 With type annotations\ndef process_data(data: str) -&gt; str:\n    return data\n</code></pre>"},{"location":"guides/development/#pre-commit-checklist","title":"Pre-Commit Checklist","text":"<p>Before committing, ensure:</p> <ol> <li>Code is formatted: <code>ruff format src/ tests/</code></li> <li>No lint errors: <code>ruff check src/ tests/</code></li> <li>Types are correct: <code>mypy src/</code></li> <li>Tests pass: <code>pytest</code></li> <li>Coverage is good: <code>pytest --cov=src</code></li> </ol> <p>Automated Pre-Commit Hook (optional):</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Create .pre-commit-config.yaml\ncat &gt; .pre-commit-config.yaml &lt;&lt; EOF\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.0\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.7.0\n    hooks:\n      - id: mypy\n        additional_dependencies: [types-all]\nEOF\n\n# Install hook\npre-commit install\n\n# Now checks run automatically on commit\n</code></pre>"},{"location":"guides/development/#adding-new-features","title":"Adding New Features","text":""},{"location":"guides/development/#example-adding-a-new-agent","title":"Example: Adding a New Agent","text":"<p>1. Define Agent Interface (<code>src/agents/base.py</code>):</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseAgent(ABC):\n    @abstractmethod\n    async def generate(self, prompt: str) -&gt; str:\n        \"\"\"Generate response from agent.\"\"\"\n        pass\n</code></pre> <p>2. Implement Agent (<code>src/agents/editor.py</code>):</p> <pre><code>from src.agents.base import BaseAgent\n\nclass EditorAgent(BaseAgent):\n    \"\"\"Agent that edits and improves slogans.\"\"\"\n\n    async def generate(self, prompt: str) -&gt; str:\n        # Implementation\n        pass\n</code></pre> <p>3. Add Tests (<code>tests/unit/test_editor.py</code>):</p> <pre><code>import pytest\nfrom src.agents.editor import EditorAgent\n\n@pytest.mark.asyncio\nasync def test_editor_improves_slogan():\n    agent = EditorAgent()\n    result = await agent.generate(\"Make this better: Pizza Place\")\n    assert len(result) &gt; 0\n</code></pre> <p>4. Update Workflow (<code>src/orchestration/workflow.py</code>):</p> <p>Integrate new agent into existing workflow.</p> <p>5. Document (update relevant docs):</p> <ul> <li>API reference</li> <li>Architecture docs</li> <li>User guides</li> </ul>"},{"location":"guides/development/#debugging","title":"Debugging","text":""},{"location":"guides/development/#cli-debugging","title":"CLI Debugging","text":"<p>Verbose Mode:</p> <pre><code>slogan-gen generate \"test\" --verbose\n</code></pre> <p>Python Debugger:</p> <pre><code># Add to code\nimport pdb; pdb.set_trace()\n\n# Or use breakpoint() (Python 3.7+)\nbreakpoint()\n</code></pre> <p>Run with Debugger:</p> <pre><code>python -m pdb -m src.cli.main generate \"test\"\n</code></pre>"},{"location":"guides/development/#api-debugging","title":"API Debugging","text":"<p>Debug Logging:</p> <pre><code>export API_LOG_LEVEL=DEBUG\nuvicorn src.api.main:app --reload\n</code></pre> <p>FastAPI Debug Mode:</p> <pre><code># src/api/main.py\napp = FastAPI(debug=True)\n</code></pre> <p>VS Code Debugging (<code>.vscode/launch.json</code>):</p> <pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"FastAPI\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"uvicorn\",\n      \"args\": [\"src.api.main:app\", \"--reload\"],\n      \"jinja\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/development/#common-issues","title":"Common Issues","text":""},{"location":"guides/development/#import-errors","title":"Import Errors","text":"<p>Problem:</p> <pre><code>ModuleNotFoundError: No module named 'agents'\n</code></pre> <p>Solution:</p> <pre><code># Reinstall in editable mode\nuv pip install -e .\n</code></pre>"},{"location":"guides/development/#missing-pytyped-files","title":"Missing py.typed Files","text":"<p>Problem:</p> <pre><code>error: Skipping analyzing \"agents\": module is installed, but missing library stubs\n</code></pre> <p>Solution:</p> <pre><code># Create py.typed markers\ntouch src/py.typed\ntouch src/agents/py.typed\ntouch src/cli/py.typed\ntouch src/config/py.typed\ntouch src/orchestration/py.typed\n</code></pre>"},{"location":"guides/development/#ollama-connection-errors","title":"Ollama Connection Errors","text":"<p>Problem:</p> <pre><code>\u274c Cannot connect to Ollama at http://localhost:11434\n</code></pre> <p>Solutions:</p> <pre><code># 1. Start Ollama\nollama serve\n\n# 2. Check if running\ncurl http://localhost:11434/api/tags\n\n# 3. Verify model installed\nollama list\n</code></pre>"},{"location":"guides/development/#test-failures","title":"Test Failures","text":"<p>Problem: Tests fail with connection errors.</p> <p>Solution:</p> <pre><code># Ensure Ollama is running\nollama serve\n\n# Pull test model\nollama pull mistral:latest\n\n# Run tests\npytest\n</code></pre>"},{"location":"guides/development/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"guides/development/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8: Use ruff for formatting</li> <li>Type Annotations: All functions must have type hints</li> <li>Docstrings: Use Google style docstrings</li> <li>Line Length: 100 characters max</li> </ul> <p>Example:</p> <pre><code>def generate_slogan(\n    input_text: str,\n    model: str = \"mistral:latest\",\n    max_turns: int = 5\n) -&gt; dict[str, Any]:\n    \"\"\"Generate a slogan using Writer-Reviewer collaboration.\n\n    Args:\n        input_text: Product or service description\n        model: Ollama model identifier\n        max_turns: Maximum iteration rounds\n\n    Returns:\n        Dictionary containing final slogan and metadata\n\n    Raises:\n        ValueError: If input_text is empty or invalid\n        ConnectionError: If cannot connect to Ollama\n    \"\"\"\n    # Implementation\n    pass\n</code></pre>"},{"location":"guides/development/#testing-requirements","title":"Testing Requirements","text":"<ul> <li>All new features must have tests</li> <li>Maintain &gt;80% coverage</li> <li>Include unit and integration tests</li> <li>Test edge cases and error handling</li> </ul>"},{"location":"guides/development/#documentation-requirements","title":"Documentation Requirements","text":"<ul> <li>Update relevant docs in <code>docs/</code></li> <li>Add docstrings to all functions/classes</li> <li>Include usage examples</li> <li>Update CHANGELOG.md</li> </ul>"},{"location":"guides/development/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create feature branch: <code>git checkout -b feature/name</code></li> <li>Make changes: Code + tests + docs</li> <li>Run quality checks: ruff, mypy, pytest</li> <li>Commit with clear message: <code>feat: add feature</code></li> <li>Push and create PR</li> <li>Address review feedback</li> <li>Merge after approval</li> </ol>"},{"location":"guides/development/#performance-testing","title":"Performance Testing","text":""},{"location":"guides/development/#benchmarking","title":"Benchmarking","text":"<p>CLI Performance:</p> <pre><code>time slogan-gen generate \"test\" --model gemma2:2b\ntime slogan-gen generate \"test\" --model mistral:latest\ntime slogan-gen generate \"test\" --model llama3.2:latest\n</code></pre> <p>API Performance:</p> <pre><code># Using Apache Bench\nab -n 100 -c 10 -p request.json -T application/json \\\n  http://localhost:8000/api/v1/slogans/generate\n\n# Using wrk\nwrk -t12 -c400 -d30s --latency \\\n  http://localhost:8000/api/v1/health\n</code></pre>"},{"location":"guides/development/#load-testing","title":"Load Testing","text":"<p>See API Usage Guide for load testing details.</p>"},{"location":"guides/development/#release-process","title":"Release Process","text":""},{"location":"guides/development/#version-numbering","title":"Version Numbering","text":"<p>Follow Semantic Versioning:</p> <ul> <li>MAJOR: Breaking changes (2.0.0)</li> <li>MINOR: New features (1.1.0)</li> <li>PATCH: Bug fixes (1.0.1)</li> </ul>"},{"location":"guides/development/#release-steps","title":"Release Steps","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update CHANGELOG.md</li> <li>Run full test suite: <code>pytest</code></li> <li>Build package: <code>python -m build</code></li> <li>Create git tag: <code>git tag v1.0.0</code></li> <li>Push tag: <code>git push origin v1.0.0</code></li> <li>Create GitHub release</li> <li>Publish to PyPI (if applicable)</li> </ol>"},{"location":"guides/development/#resources","title":"Resources","text":""},{"location":"guides/development/#documentation","title":"Documentation","text":"<ul> <li>Quick Start Guide</li> <li>CLI Usage Guide</li> <li>API Usage Guide</li> <li>Configuration Guide</li> <li>Troubleshooting Guide</li> </ul>"},{"location":"guides/development/#external-resources","title":"External Resources","text":"<ul> <li>Microsoft Agent Framework</li> <li>Ollama Documentation</li> <li>FastAPI Documentation</li> <li>Ruff Documentation</li> <li>pytest Documentation</li> </ul>"},{"location":"guides/development/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions and share ideas</li> <li>Pull Requests: Contribute code improvements</li> </ul>"},{"location":"guides/development/#appendix","title":"Appendix","text":""},{"location":"guides/development/#useful-commands","title":"Useful Commands","text":"<pre><code># Development\nuv pip install -e \".[dev]\"          # Install dev dependencies\nsource .venv/bin/activate           # Activate virtual environment\nslogan-gen --version                # Check installed version\n\n# Testing\npytest                              # Run all tests\npytest -v                           # Verbose output\npytest --cov=src                    # With coverage\npytest -k \"test_name\"               # Run specific test\n\n# Code Quality\nruff format src/ tests/             # Format code\nruff check src/ tests/ --fix       # Lint and auto-fix\nmypy src/                           # Type check\n\n# API\nuvicorn src.api.main:app --reload  # Start dev server\ncurl http://localhost:8000/docs    # View API docs\n\n# Documentation\nmkdocs serve                        # Preview docs locally\nmkdocs build                        # Build static site\n\n# Git\ngit checkout -b feature/name        # Create feature branch\ngit add .                           # Stage changes\ngit commit -m \"feat: description\"  # Commit with message\ngit push origin feature/name        # Push branch\n</code></pre>"},{"location":"guides/development/#configuration-files","title":"Configuration Files","text":"<p>pyproject.toml - Project configuration: - Dependencies - Build system - Tool configuration (ruff, mypy, pytest)</p> <p>mkdocs.yml - Documentation configuration: - Site structure - Theme settings - Plugins</p> <p>.env - Environment variables (not committed): - API configuration - Ollama settings - Development overrides</p>"},{"location":"guides/development/#next-steps","title":"Next Steps","text":"<ol> <li>Set up your environment following this guide</li> <li>Read the codebase to understand the architecture</li> <li>Run the tests to verify your setup</li> <li>Pick an issue from GitHub to work on</li> <li>Create a PR with your changes</li> </ol> <p>Welcome to the project! \ud83c\udf89</p>"},{"location":"specs/","title":"Specifications","text":"<p>This section contains detailed specifications for features and implementations using the SpecKit methodology.</p>"},{"location":"specs/#what-is-speckit","title":"What is SpecKit?","text":"<p>SpecKit is a structured approach to feature development that includes:</p> <ul> <li>User stories and acceptance scenarios</li> <li>Functional and non-functional requirements</li> <li>Implementation plan with phases and tasks</li> <li>Testing strategy and acceptance criteria</li> <li>Risk assessment and mitigation plans</li> <li>Contract definitions for interfaces and APIs</li> </ul> <p>Each specification provides complete context for implementing a feature from requirements to deployment.</p>"},{"location":"specs/#available-specifications","title":"Available Specifications","text":""},{"location":"specs/#001-slogan-writer-reviewer-agent-system","title":"001: Slogan Writer-Reviewer Agent System","text":"<p>Status: \u2705 Complete | Location: <code>specs/001-slogan-writer-reviewer/</code></p> <p>The foundational multi-agent system using Microsoft Agent Framework and Ollama for iterative slogan generation.</p> <p>Overview:</p> <ul> <li>Two-agent collaboration (Writer + Reviewer)</li> <li>Iterative refinement loop with feedback</li> <li>Local LLM execution via Ollama</li> <li>CLI interface with Click framework</li> </ul> <p>Key Documents (in project repository):</p> <ul> <li><code>spec.md</code> - Complete specification with requirements, user stories, acceptance criteria</li> <li><code>plan.md</code> - Implementation plan with 3 phases (agent development, workflow orchestration, CLI interface)</li> <li><code>tasks.md</code> - Detailed task breakdown with time estimates</li> <li><code>quickstart.md</code> - Quick start guide for users</li> <li><code>research.md</code> - Research notes on frameworks and approaches</li> <li><code>data-model.md</code> - Core data structures (IterationSession, Turn, CompletionReason)</li> <li><code>contracts/cli-interface.md</code> - CLI contract definition</li> <li><code>checklists/requirements.md</code> - Requirements checklist</li> </ul> <p>\ud83d\udcc1 Location: These files are in the <code>specs/001-slogan-writer-reviewer/</code> directory of the project repository.</p> <p>Related Documentation:</p> <ul> <li>Agent Architecture - Detailed agent design patterns</li> <li>Workflow Architecture - Orchestration system</li> <li>CLI Usage Guide - CLI command reference</li> <li>Agents API Reference - Agent API documentation</li> </ul>"},{"location":"specs/#002-fastapi-rest-api","title":"002: FastAPI REST API","text":"<p>Status: \u2705 Complete | Location: <code>specs/002-fastapi-api/</code></p> <p>REST API implementation enabling programmatic slogan generation with OpenAPI documentation.</p> <p>Overview:</p> <ul> <li>Async FastAPI endpoints</li> <li>OpenAPI/Swagger documentation</li> <li>CORS support for web clients</li> <li>Request/response validation with Pydantic</li> <li>Comprehensive error handling</li> </ul> <p>Key Documents (in project repository):</p> <ul> <li><code>spec.md</code> - API specification with endpoints, request/response schemas, and requirements</li> <li><code>plan.md</code> - Implementation plan with 3 phases (foundation, core endpoints, testing)</li> <li><code>tasks.md</code> - Task breakdown with time estimates</li> <li><code>contracts/api-interface.md</code> - API contract definitions</li> </ul> <p>\ud83d\udcc1 Location: These files are in the <code>specs/002-fastapi-api/</code> directory of the project repository.</p> <p>Endpoints:</p> <ul> <li><code>GET /</code> - Welcome/info message</li> <li><code>GET /api/v1/health</code> - Health check</li> <li><code>GET /api/v1/models</code> - List available Ollama models</li> <li><code>POST /api/v1/slogans/generate</code> - Generate slogan with iterative refinement</li> </ul> <p>Related Documentation:</p> <ul> <li>API Usage Guide - REST API tutorial</li> <li>API Clients - Client examples (Python, JavaScript, cURL, Go)</li> <li>OpenAPI Specification - Interactive API documentation</li> <li>REST API Reference - Complete API documentation</li> </ul>"},{"location":"specs/#003-mkdocs-documentation-system","title":"003: MkDocs Documentation System","text":"<p>Status: \u2705 Phase 6 Complete (6/7) | Location: <code>specs/003-mkdocs-documentation/</code></p> <p>Comprehensive documentation system with Material theme, API reference generation, and GitHub Pages deployment.</p> <p>Overview:</p> <ul> <li>MkDocs 1.6.1 with Material for MkDocs 9.6.22</li> <li>Auto-generated API docs with mkdocstrings</li> <li>Interactive OpenAPI documentation</li> <li>Multi-section organization (Getting Started, Guides, API Reference, Architecture)</li> <li>Search, navigation tabs, dark mode support</li> </ul> <p>Key Documents (in project repository):</p> <ul> <li><code>spec.md</code> - Complete documentation specification with requirements</li> <li><code>plan.md</code> - 7-phase implementation plan</li> <li><code>tasks.md</code> - Detailed task breakdown (39+ tasks)</li> <li><code>README.md</code> - Project overview and quick reference</li> </ul> <p>\ud83d\udcc1 Location: These files are in the <code>specs/003-mkdocs-documentation/</code> directory of the project repository.</p> <p>Implementation Phases:</p> <ol> <li>\u2705 Phase 1: Foundation Setup - Dependencies, mkdocs.yml, directory structure, landing page</li> <li>\u2705 Phase 2: Content Migration - Installation, Quick Start, Configuration, CLI/API guides, Development, Troubleshooting</li> <li>\u2705 Phase 3: API Documentation - API reference pages for agents, orchestration, CLI, config, REST API</li> <li>\u2705 Phase 4: REST API Integration - OpenAPI spec page, API client examples</li> <li>\u2705 Phase 5: Architecture Documentation - Architecture overview, agent/workflow architecture, spec linking</li> <li>\u2705 Phase 6: Theme Customization - Material theme config, custom CSS, logo/favicon</li> <li>\u2b1c Phase 7: Automated Deployment - GitHub Actions, build/deploy pipeline</li> </ol> <p>Progress: 31/39+ tasks complete (~79%)</p> <p>Related Documentation:</p> <ul> <li>Architecture Overview - System architecture</li> <li>Agent Architecture - Agent design patterns</li> <li>Workflow Architecture - Orchestration system</li> <li>Development Guide - How to contribute</li> </ul>"},{"location":"specs/#using-specifications","title":"Using Specifications","text":""},{"location":"specs/#for-developers","title":"For Developers","text":"<p>Specifications provide complete context for feature implementation:</p> <ol> <li>Start with spec.md - Understand requirements, user stories, and acceptance criteria</li> <li>Review plan.md - See the implementation phases and overall approach</li> <li>Follow tasks.md - Execute tasks in order with time estimates</li> <li>Check contracts/ - Understand interface definitions and contracts</li> <li>Validate checklists/ - Ensure all requirements are met</li> </ol>"},{"location":"specs/#for-project-managers","title":"For Project Managers","text":"<p>Specifications enable accurate planning:</p> <ul> <li>Time Estimates: All tasks include estimated duration</li> <li>Dependencies: Tasks are organized with clear dependencies</li> <li>Progress Tracking: Track completion against task lists</li> <li>Risk Assessment: Specifications include identified risks and mitigation plans</li> </ul>"},{"location":"specs/#for-users","title":"For Users","text":"<p>Specifications help understand feature capabilities:</p> <ul> <li>User Stories: Real-world usage scenarios</li> <li>Quick Start Guides: Get started quickly with key features</li> <li>API Contracts: Understand interfaces and data formats</li> </ul>"},{"location":"specs/#speckit-template-structure","title":"SpecKit Template Structure","text":"<p>Each specification follows this structure:</p> <pre><code>specs/NNN-feature-name/\n\u251c\u2500\u2500 spec.md              # Main specification document\n\u251c\u2500\u2500 plan.md              # Implementation plan with phases\n\u251c\u2500\u2500 tasks.md             # Detailed task breakdown\n\u251c\u2500\u2500 README.md            # Quick overview\n\u251c\u2500\u2500 research.md          # (Optional) Research notes\n\u251c\u2500\u2500 data-model.md        # (Optional) Data structures\n\u251c\u2500\u2500 contracts/           # Interface definitions\n\u2502   \u251c\u2500\u2500 api-interface.md\n\u2502   \u2514\u2500\u2500 cli-interface.md\n\u2514\u2500\u2500 checklists/          # Validation checklists\n    \u2514\u2500\u2500 requirements.md\n</code></pre>"},{"location":"specs/#specmd-contents","title":"spec.md Contents","text":"<ul> <li>Overview: Feature description and objectives</li> <li>User Stories: As a [role], I want [feature] so that [benefit]</li> <li>Acceptance Scenarios: Given/When/Then scenarios</li> <li>Functional Requirements: What the system must do</li> <li>Non-Functional Requirements: Performance, security, usability</li> <li>Testing Strategy: Unit, integration, and acceptance tests</li> <li>Risk Assessment: Potential issues and mitigation</li> </ul>"},{"location":"specs/#planmd-contents","title":"plan.md Contents","text":"<ul> <li>Implementation Phases: Logical grouping of work</li> <li>Phase Descriptions: What each phase accomplishes</li> <li>Dependencies: Prerequisites and sequencing</li> <li>Deliverables: Concrete outputs for each phase</li> </ul>"},{"location":"specs/#tasksmd-contents","title":"tasks.md Contents","text":"<ul> <li>Task Lists: Organized by phase</li> <li>Time Estimates: Expected duration for each task</li> <li>Task Descriptions: Clear definition of work</li> <li>Completion Criteria: How to know when done</li> </ul>"},{"location":"specs/#see-also","title":"See Also","text":"<ul> <li>Development Guide - How to contribute to the project</li> <li>Architecture Overview - System architecture and design decisions</li> <li>Testing Strategy - How we test features</li> </ul>"},{"location":"specs/#accessing-specifications","title":"Accessing Specifications","text":"<p>Specifications are maintained in the repository at <code>specs/</code> directory. Each specification follows the SpecKit format with:</p> <ul> <li>spec.md: Complete specification document</li> <li>tasks.md: Granular task breakdown</li> <li>plan.md: Phased implementation plan</li> <li>README.md: Quick reference guide</li> </ul> <p>Note: Full specification documents are available in the project repository. This documentation site focuses on user-facing guides and API references.</p>"}]}